Processing dataset...
Node:  0  not included, including...
Node:  1  not included, including...
Node:  2  not included, including...
Node:  3  not included, including...
Node:  4  not included, including...
Node:  5  not included, including...
Node:  6  not included, including...
Node:  7  not included, including...
Node:  8  not included, including...
Node:  9  not included, including...
Node:  10  not included, including...
Node:  11  not included, including...
Node:  12  not included, including...
Node:  13  not included, including...
Node:  14  not included, including...
Node:  15  not included, including...
Node:  16  not included, including...
Node:  17  not included, including...
Node:  18  not included, including...
Node:  19  not included, including...
Node:  20  not included, including...
Node:  21  not included, including...
Node:  22  not included, including...
Skipping  row_328
Ajustando modelo para bus_fault...
Number of situations:  549
Number of timestamps:  800
Number of situations of the selected type:  86
  0%|          | 0/50 [00:00<?, ?it/s]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: Currently logged in as: maragumar01. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240718_082138-7rpxy49u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-microwave-30
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/7rpxy49u

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5696 | Eval Loss: 0.4593 | Eval R2: -230.0921 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3456 | Eval Loss: 0.3093 | Eval R2: -155.3705 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2243 | Eval Loss: 0.1886 | Eval R2: -84.1872 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1368 | Eval Loss: 0.1119 | Eval R2: -37.8114 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0860 | Eval Loss: 0.0743 | Eval R2: -15.4373 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0633 | Eval Loss: 0.0587 | Eval R2: -7.9674 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0528 | Eval Loss: 0.0501 | Eval R2: -5.9393 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0458 | Eval Loss: 0.0437 | Eval R2: -5.3279 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0404 | Eval Loss: 0.0393 | Eval R2: -5.1066 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0369 | Eval Loss: 0.0362 | Eval R2: -4.7024 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0345 | Eval Loss: 0.0331 | Eval R2: -3.9165 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0325 | Eval Loss: 0.0299 | Eval R2: -2.9671 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0303 | Eval Loss: 0.0279 | Eval R2: -2.5013 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0286 | Eval Loss: 0.0268 | Eval R2: -2.3458 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0272 | Eval Loss: 0.0261 | Eval R2: -2.2632 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0262 | Eval Loss: 0.0256 | Eval R2: -2.2317 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0256 | Eval Loss: 0.0253 | Eval R2: -2.2874 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0251 | Eval Loss: 0.0250 | Eval R2: -2.3657 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0246 | Eval Loss: 0.0247 | Eval R2: -2.3359 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0241 | Eval Loss: 0.0245 | Eval R2: -2.4166 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0237 | Eval Loss: 0.0245 | Eval R2: -2.5273 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0235 | Eval Loss: 0.0245 | Eval R2: -2.5739 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0233 | Eval Loss: 0.0243 | Eval R2: -2.6756 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0231 | Eval Loss: 0.0243 | Eval R2: -2.6621 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0228 | Eval Loss: 0.0240 | Eval R2: -2.6649 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0227 | Eval Loss: 0.0238 | Eval R2: -2.7168 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0224 | Eval Loss: 0.0236 | Eval R2: -2.6749 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020393, test R2 score: -2.525135

==================== GUARDANDO RESULTADOS ===================

         Modelo  ... loss_eval_final
0          LSTM  ...             NaN
1  LSTM_NOBATCH  ...             NaN
2     MPNN_LSTM  ...        0.025072
3   DyGrEncoder  ...             NaN
4         AGCRN  ...             NaN
5         DCRNN  ...             NaN
6         MTGNN  ...             NaN

[7 rows x 12 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02237934060394764, 'r2_eval_final': -2.6748790740966797, 'loss_eval_final': 0.023648934438824654, 'r2_test': -2.5251347797409633, 'loss_test': 0.020392872393131256, 'loss_nodes': [[0.009182129986584187, 0.012411069124937057, 0.013973119668662548, 0.014801002107560635, 0.01888686791062355, 0.015941087156534195, 0.017298586666584015, 0.01839219219982624, 0.020285556092858315, 0.02170218713581562, 0.020185323432087898, 0.01977924257516861, 0.024145567789673805, 0.021424001082777977, 0.022442584857344627, 0.02456161007285118, 0.028257111087441444, 0.026973377913236618, 0.028186846524477005, 0.029027992859482765]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.007 MB of 0.007 MB uploadedwandb: \ 0.007 MB of 0.007 MB uploadedwandb: | 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02238
wandb: loss_eval 0.02365
wandb: loss_test 0.02039
wandb:   r2_eval -2.67488
wandb:   r2_test -2.52513
wandb: 
wandb: ðŸš€ View run zesty-microwave-30 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/7rpxy49u
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_082138-7rpxy49u/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  2%|â–         | 1/50 [11:38:39<570:34:04, 41919.27s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240718_200016-popmrycv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-music-31
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/popmrycv

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8366 | Eval Loss: 0.7296 | Eval R2: -357.1361 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6070 | Eval Loss: 0.6013 | Eval R2: -300.7595 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5007 | Eval Loss: 0.5078 | Eval R2: -255.7133 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4243 | Eval Loss: 0.4327 | Eval R2: -215.4407 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3608 | Eval Loss: 0.3670 | Eval R2: -180.9281 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3043 | Eval Loss: 0.3070 | Eval R2: -149.9868 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2531 | Eval Loss: 0.2513 | Eval R2: -121.1681 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2048 | Eval Loss: 0.1971 | Eval R2: -92.6470 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1574 | Eval Loss: 0.1433 | Eval R2: -63.4361 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1118 | Eval Loss: 0.0962 | Eval R2: -36.4720 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0770 | Eval Loss: 0.0676 | Eval R2: -18.9728 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0590 | Eval Loss: 0.0559 | Eval R2: -12.5239 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0508 | Eval Loss: 0.0499 | Eval R2: -10.3860 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0466 | Eval Loss: 0.0448 | Eval R2: -8.1158 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0434 | Eval Loss: 0.0402 | Eval R2: -6.1892 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0404 | Eval Loss: 0.0366 | Eval R2: -4.8962 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0377 | Eval Loss: 0.0338 | Eval R2: -4.0690 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0354 | Eval Loss: 0.0319 | Eval R2: -3.6045 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0335 | Eval Loss: 0.0303 | Eval R2: -3.2074 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0319 | Eval Loss: 0.0290 | Eval R2: -2.9712 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0308 | Eval Loss: 0.0281 | Eval R2: -2.7918 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0301 | Eval Loss: 0.0274 | Eval R2: -2.7038 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0291 | Eval Loss: 0.0267 | Eval R2: -2.5359 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0284 | Eval Loss: 0.0260 | Eval R2: -2.4106 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0278 | Eval Loss: 0.0255 | Eval R2: -2.3167 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0273 | Eval Loss: 0.0250 | Eval R2: -2.2359 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0265 | Eval Loss: 0.0247 | Eval R2: -2.1971 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.022320, test R2 score: -1.966505
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.026514453813433647, 'r2_eval_final': -2.1970815658569336, 'loss_eval_final': 0.024669673293828964, 'r2_test': -1.966504507336352, 'loss_test': 0.022320197895169258, 'loss_nodes': [[0.008059919811785221, 0.011640083976089954, 0.013617158867418766, 0.01522604189813137, 0.017351381480693817, 0.017881836742162704, 0.017692402005195618, 0.01993168704211712, 0.01971755176782608, 0.01993602328002453, 0.01965300738811493, 0.020306989550590515, 0.02566593699157238, 0.021608205512166023, 0.026197029277682304, 0.046400878578424454, 0.029350433498620987, 0.03259669989347458, 0.030323542654514313, 0.033247191458940506]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02651
wandb: loss_eval 0.02467
wandb: loss_test 0.02232
wandb:   r2_eval -2.19708
wandb:   r2_test -1.9665
wandb: 
wandb: ðŸš€ View run happy-music-31 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/popmrycv
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_200016-popmrycv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  4%|â–         | 2/50 [12:25:47<252:19:39, 18924.58s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240718_204725-ov03iu10
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-paper-32
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ov03iu10

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8192 | Eval Loss: 0.7342 | Eval R2: -374.5409 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6247 | Eval Loss: 0.6679 | Eval R2: -345.8094 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5700 | Eval Loss: 0.6205 | Eval R2: -324.1516 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5321 | Eval Loss: 0.5871 | Eval R2: -308.5658 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5043 | Eval Loss: 0.5608 | Eval R2: -296.4342 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4816 | Eval Loss: 0.5385 | Eval R2: -286.1534 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4620 | Eval Loss: 0.5185 | Eval R2: -276.7031 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4440 | Eval Loss: 0.4993 | Eval R2: -267.3043 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4264 | Eval Loss: 0.4796 | Eval R2: -257.3845 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4082 | Eval Loss: 0.4585 | Eval R2: -246.4866 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3884 | Eval Loss: 0.4352 | Eval R2: -234.2126 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3668 | Eval Loss: 0.4094 | Eval R2: -220.2589 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3432 | Eval Loss: 0.3811 | Eval R2: -204.4937 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3179 | Eval Loss: 0.3509 | Eval R2: -187.0419 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2917 | Eval Loss: 0.3200 | Eval R2: -168.3566 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2660 | Eval Loss: 0.2900 | Eval R2: -149.2367 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2423 | Eval Loss: 0.2625 | Eval R2: -130.7305 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2217 | Eval Loss: 0.2390 | Eval R2: -113.9081 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2050 | Eval Loss: 0.2201 | Eval R2: -99.5829 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1921 | Eval Loss: 0.2055 | Eval R2: -88.1211 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1824 | Eval Loss: 0.1945 | Eval R2: -79.4277 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1752 | Eval Loss: 0.1864 | Eval R2: -73.0869 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1698 | Eval Loss: 0.1803 | Eval R2: -68.5569 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1657 | Eval Loss: 0.1758 | Eval R2: -65.3250 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1626 | Eval Loss: 0.1725 | Eval R2: -62.9873 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1603 | Eval Loss: 0.1699 | Eval R2: -61.2619 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1586 | Eval Loss: 0.1681 | Eval R2: -59.9665 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.124140, test R2 score: -47.070356
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.15855349600315094, 'r2_eval_final': -59.96648406982422, 'loss_eval_final': 0.16805115342140198, 'r2_test': -47.070355824556394, 'loss_test': 0.12413965165615082, 'loss_nodes': [[0.12547335028648376, 0.12585797905921936, 0.12048632651567459, 0.12017330527305603, 0.12677909433841705, 0.11666183173656464, 0.1263538897037506, 0.11827311664819717, 0.1330353021621704, 0.11554308980703354, 0.11997106671333313, 0.12365630269050598, 0.11948968470096588, 0.12285833805799484, 0.12023873627185822, 0.1554553359746933, 0.12220527231693268, 0.12315169721841812, 0.12416932731866837, 0.12295995652675629]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15855
wandb: loss_eval 0.16805
wandb: loss_test 0.12414
wandb:   r2_eval -59.96648
wandb:   r2_test -47.07036
wandb: 
wandb: ðŸš€ View run amber-paper-32 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ov03iu10
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_204725-ov03iu10/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  6%|â–Œ         | 3/50 [13:04:45<148:11:15, 11350.55s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240718_212622-gbihyz45
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-gorge-33
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/gbihyz45

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5608 | Eval Loss: 0.5043 | Eval R2: -263.2942 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4319 | Eval Loss: 0.4552 | Eval R2: -236.3751 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3854 | Eval Loss: 0.4039 | Eval R2: -208.6526 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3393 | Eval Loss: 0.3521 | Eval R2: -178.9646 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2962 | Eval Loss: 0.3022 | Eval R2: -152.7563 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2542 | Eval Loss: 0.2572 | Eval R2: -127.9249 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2147 | Eval Loss: 0.2128 | Eval R2: -103.2773 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1772 | Eval Loss: 0.1721 | Eval R2: -80.5471 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1430 | Eval Loss: 0.1357 | Eval R2: -60.4746 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1136 | Eval Loss: 0.1051 | Eval R2: -43.7433 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0891 | Eval Loss: 0.0804 | Eval R2: -30.3633 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0696 | Eval Loss: 0.0616 | Eval R2: -20.1717 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0547 | Eval Loss: 0.0478 | Eval R2: -12.8705 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0441 | Eval Loss: 0.0381 | Eval R2: -7.8224 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0367 | Eval Loss: 0.0321 | Eval R2: -4.8608 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0322 | Eval Loss: 0.0287 | Eval R2: -3.2882 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0297 | Eval Loss: 0.0269 | Eval R2: -2.5678 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0282 | Eval Loss: 0.0261 | Eval R2: -2.3993 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0274 | Eval Loss: 0.0255 | Eval R2: -2.2602 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0267 | Eval Loss: 0.0250 | Eval R2: -2.1927 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0263 | Eval Loss: 0.0245 | Eval R2: -2.0597 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0259 | Eval Loss: 0.0243 | Eval R2: -2.1045 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0253 | Eval Loss: 0.0238 | Eval R2: -1.9499 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0249 | Eval Loss: 0.0236 | Eval R2: -1.8890 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0245 | Eval Loss: 0.0235 | Eval R2: -1.9161 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0242 | Eval Loss: 0.0234 | Eval R2: -1.8680 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0239 | Eval Loss: 0.0233 | Eval R2: -1.8619 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021683, test R2 score: -1.828693
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.023912088945508003, 'r2_eval_final': -1.8618820905685425, 'loss_eval_final': 0.023346971720457077, 'r2_test': -1.8286932420999595, 'loss_test': 0.021683160215616226, 'loss_nodes': [[0.014060916379094124, 0.01170000247657299, 0.015403435565531254, 0.017205623909831047, 0.015195801854133606, 0.021479591727256775, 0.018477344885468483, 0.02352917194366455, 0.018102724105119705, 0.022225940600037575, 0.02125014178454876, 0.020842645317316055, 0.021231699734926224, 0.019900096580386162, 0.02242244966328144, 0.022561129182577133, 0.041831180453300476, 0.02900829166173935, 0.02962755411863327, 0.027607480064034462]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02391
wandb: loss_eval 0.02335
wandb: loss_test 0.02168
wandb:   r2_eval -1.86188
wandb:   r2_test -1.82869
wandb: 
wandb: ðŸš€ View run soft-gorge-33 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/gbihyz45
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_212622-gbihyz45/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  8%|â–Š         | 4/50 [24:38:08<297:38:38, 23293.88s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_085945-aambeptf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-plant-34
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/aambeptf

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1020 | Eval Loss: 0.8132 | Eval R2: -405.7484 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6161 | Eval Loss: 0.5220 | Eval R2: -257.7320 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3809 | Eval Loss: 0.3045 | Eval R2: -136.6597 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2185 | Eval Loss: 0.1707 | Eval R2: -61.3970 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1320 | Eval Loss: 0.1076 | Eval R2: -26.2130 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0938 | Eval Loss: 0.0825 | Eval R2: -14.4861 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0788 | Eval Loss: 0.0717 | Eval R2: -11.0492 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0710 | Eval Loss: 0.0645 | Eval R2: -9.4934 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0650 | Eval Loss: 0.0588 | Eval R2: -8.4034 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0602 | Eval Loss: 0.0541 | Eval R2: -7.5375 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0564 | Eval Loss: 0.0503 | Eval R2: -6.8134 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0529 | Eval Loss: 0.0471 | Eval R2: -6.1610 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0498 | Eval Loss: 0.0443 | Eval R2: -5.5479 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0472 | Eval Loss: 0.0419 | Eval R2: -5.0546 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0447 | Eval Loss: 0.0398 | Eval R2: -4.6864 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0427 | Eval Loss: 0.0380 | Eval R2: -4.3532 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0407 | Eval Loss: 0.0363 | Eval R2: -4.0460 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0388 | Eval Loss: 0.0347 | Eval R2: -3.8102 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0373 | Eval Loss: 0.0334 | Eval R2: -3.6002 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0321 | Eval R2: -3.3801 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0341 | Eval Loss: 0.0310 | Eval R2: -3.2468 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0332 | Eval Loss: 0.0301 | Eval R2: -3.1390 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0321 | Eval Loss: 0.0291 | Eval R2: -2.9828 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0310 | Eval Loss: 0.0284 | Eval R2: -2.8999 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0304 | Eval Loss: 0.0276 | Eval R2: -2.7613 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0295 | Eval Loss: 0.0270 | Eval R2: -2.7210 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0287 | Eval Loss: 0.0264 | Eval R2: -2.6583 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.024066, test R2 score: -2.124764
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.028741657733917236, 'r2_eval_final': -2.658262014389038, 'loss_eval_final': 0.02642938494682312, 'r2_test': -2.1247635808242618, 'loss_test': 0.02406633459031582, 'loss_nodes': [[0.010959007777273655, 0.02289639785885811, 0.020205408334732056, 0.01825249008834362, 0.020507672801613808, 0.015820806846022606, 0.029317017644643784, 0.02136252447962761, 0.02031732350587845, 0.026136331260204315, 0.025844722986221313, 0.023181220516562462, 0.022753585129976273, 0.029048943892121315, 0.03474719449877739, 0.0255740936845541, 0.027493562549352646, 0.029642254114151, 0.029051749035716057, 0.02821439690887928]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02874
wandb: loss_eval 0.02643
wandb: loss_test 0.02407
wandb:   r2_eval -2.65826
wandb:   r2_test -2.12476
wandb: 
wandb: ðŸš€ View run desert-plant-34 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/aambeptf
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_085945-aambeptf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 10%|â–ˆ         | 5/50 [24:55:00<190:44:24, 15259.22s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_091638-orb6oy9q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-moon-35
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/orb6oy9q

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7637 | Eval Loss: 0.6701 | Eval R2: -317.3430 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5340 | Eval Loss: 0.4803 | Eval R2: -228.9170 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3840 | Eval Loss: 0.3483 | Eval R2: -163.7201 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2713 | Eval Loss: 0.2349 | Eval R2: -104.1325 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1775 | Eval Loss: 0.1416 | Eval R2: -51.4557 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1089 | Eval Loss: 0.0883 | Eval R2: -20.4105 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0755 | Eval Loss: 0.0681 | Eval R2: -10.1863 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0612 | Eval Loss: 0.0565 | Eval R2: -6.8454 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0522 | Eval Loss: 0.0490 | Eval R2: -6.1042 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0465 | Eval Loss: 0.0438 | Eval R2: -5.6676 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0423 | Eval Loss: 0.0400 | Eval R2: -5.2174 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0392 | Eval Loss: 0.0375 | Eval R2: -4.8358 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0369 | Eval Loss: 0.0350 | Eval R2: -4.2403 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0349 | Eval Loss: 0.0328 | Eval R2: -3.6071 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0332 | Eval Loss: 0.0312 | Eval R2: -3.0991 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0316 | Eval Loss: 0.0298 | Eval R2: -2.7922 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0302 | Eval Loss: 0.0290 | Eval R2: -2.6250 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0289 | Eval Loss: 0.0281 | Eval R2: -2.4861 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0281 | Eval Loss: 0.0277 | Eval R2: -2.3891 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0272 | Eval Loss: 0.0272 | Eval R2: -2.3129 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0266 | Eval Loss: 0.0268 | Eval R2: -2.2383 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0262 | Eval Loss: 0.0264 | Eval R2: -2.1568 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0257 | Eval Loss: 0.0260 | Eval R2: -2.0981 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0254 | Eval Loss: 0.0258 | Eval R2: -2.0594 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0250 | Eval Loss: 0.0253 | Eval R2: -2.0007 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0247 | Eval Loss: 0.0249 | Eval R2: -1.9564 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0243 | Eval Loss: 0.0246 | Eval R2: -1.9068 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.022481, test R2 score: -1.838697
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.024334006011486053, 'r2_eval_final': -1.9067635536193848, 'loss_eval_final': 0.024551071226596832, 'r2_test': -1.8386972085446154, 'loss_test': 0.022481068968772888, 'loss_nodes': [[0.011184918694198132, 0.01593499258160591, 0.015584243461489677, 0.017580170184373856, 0.018353881314396858, 0.01770743727684021, 0.02029106579720974, 0.021236738190054893, 0.0215890035033226, 0.025565894320607185, 0.026644032448530197, 0.0224858820438385, 0.025083471089601517, 0.023918600752949715, 0.02571556158363819, 0.02484259009361267, 0.029303256422281265, 0.02864973433315754, 0.03040606528520584, 0.027543844655156136]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02433
wandb: loss_eval 0.02455
wandb: loss_test 0.02248
wandb:   r2_eval -1.90676
wandb:   r2_test -1.8387
wandb: 
wandb: ðŸš€ View run fluent-moon-35 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/orb6oy9q
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_091638-orb6oy9q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 12%|â–ˆâ–        | 6/50 [25:15:38<128:14:07, 10491.98s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_093716-lsge52v4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-field-36
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/lsge52v4

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7714 | Eval Loss: 0.5205 | Eval R2: -254.4348 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4178 | Eval Loss: 0.4084 | Eval R2: -199.8603 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3265 | Eval Loss: 0.3177 | Eval R2: -153.4578 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2499 | Eval Loss: 0.2395 | Eval R2: -110.9164 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1848 | Eval Loss: 0.1718 | Eval R2: -73.5437 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1318 | Eval Loss: 0.1190 | Eval R2: -45.1552 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0916 | Eval Loss: 0.0799 | Eval R2: -25.0720 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0639 | Eval Loss: 0.0560 | Eval R2: -13.1996 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0483 | Eval Loss: 0.0433 | Eval R2: -7.3188 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0406 | Eval Loss: 0.0370 | Eval R2: -4.6713 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0365 | Eval Loss: 0.0335 | Eval R2: -3.5459 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0338 | Eval Loss: 0.0315 | Eval R2: -3.0286 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0318 | Eval Loss: 0.0301 | Eval R2: -2.8447 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0306 | Eval Loss: 0.0291 | Eval R2: -2.7604 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0294 | Eval Loss: 0.0283 | Eval R2: -2.6898 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0284 | Eval Loss: 0.0276 | Eval R2: -2.5861 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0276 | Eval Loss: 0.0270 | Eval R2: -2.5631 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0271 | Eval Loss: 0.0265 | Eval R2: -2.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0264 | Eval Loss: 0.0260 | Eval R2: -2.5134 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0258 | Eval Loss: 0.0255 | Eval R2: -2.4373 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0253 | Eval Loss: 0.0251 | Eval R2: -2.3653 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0249 | Eval Loss: 0.0247 | Eval R2: -2.3199 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0243 | Eval Loss: 0.0244 | Eval R2: -2.2803 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0239 | Eval Loss: 0.0242 | Eval R2: -2.3354 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0236 | Eval Loss: 0.0240 | Eval R2: -2.3648 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0233 | Eval Loss: 0.0238 | Eval R2: -2.3137 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0231 | Eval Loss: 0.0235 | Eval R2: -2.3282 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021735, test R2 score: -2.168550
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02310175821185112, 'r2_eval_final': -2.328239917755127, 'loss_eval_final': 0.02352406457066536, 'r2_test': -2.168550051183605, 'loss_test': 0.02173549123108387, 'loss_nodes': [[0.01034513395279646, 0.014820292592048645, 0.012998241931200027, 0.018518369644880295, 0.018829427659511566, 0.016647713258862495, 0.019268568605184555, 0.021240735426545143, 0.023611489683389664, 0.022105509415268898, 0.021022344008088112, 0.021646859124302864, 0.02162214182317257, 0.02224665880203247, 0.025109993293881416, 0.027386581525206566, 0.028227152302861214, 0.02826693095266819, 0.03202541172504425, 0.02877020463347435]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0231
wandb: loss_eval 0.02352
wandb: loss_test 0.02174
wandb:   r2_eval -2.32824
wandb:   r2_test -2.16855
wandb: 
wandb: ðŸš€ View run vague-field-36 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/lsge52v4
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_093716-lsge52v4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 14%|â–ˆâ–        | 7/50 [36:52:46<247:40:56, 20736.19s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_211423-71eoiz6y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-feather-37
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/71eoiz6y

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.3990 | Eval Loss: 0.3604 | Eval R2: -181.4873 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2826 | Eval Loss: 0.2711 | Eval R2: -134.3394 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2171 | Eval Loss: 0.2146 | Eval R2: -100.5849 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1802 | Eval Loss: 0.1837 | Eval R2: -77.8066 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1630 | Eval Loss: 0.1696 | Eval R2: -64.2035 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1569 | Eval Loss: 0.1645 | Eval R2: -57.5380 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1552 | Eval Loss: 0.1629 | Eval R2: -55.1687 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1546 | Eval Loss: 0.1625 | Eval R2: -54.8663 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1543 | Eval Loss: 0.1625 | Eval R2: -55.2321 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6017 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7840 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8126 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7760 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7363 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7133 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1540 | Eval Loss: 0.1625 | Eval R2: -55.7030 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6975 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6926 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6874 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6823 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6770 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6718 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6667 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6619 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6575 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6534 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6494 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121123, test R2 score: -43.344965
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.15409402549266815, 'r2_eval_final': -55.64940643310547, 'loss_eval_final': 0.16253405809402466, 'r2_test': -43.34496461694144, 'loss_test': 0.12112276256084442, 'loss_nodes': [[0.11829982697963715, 0.12032046914100647, 0.11829836666584015, 0.12113984674215317, 0.12057189643383026, 0.11906023323535919, 0.12270093709230423, 0.11888550966978073, 0.12161321192979813, 0.11873836815357208, 0.12021375447511673, 0.12306991219520569, 0.11966392397880554, 0.12314219027757645, 0.11990226060152054, 0.12138199806213379, 0.12442529201507568, 0.12209079414606094, 0.12596818804740906, 0.12296821922063828]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15409
wandb: loss_eval 0.16253
wandb: loss_test 0.12112
wandb:   r2_eval -55.64941
wandb:   r2_test -43.34496
wandb: 
wandb: ðŸš€ View run twilight-feather-37 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/71eoiz6y
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_211423-71eoiz6y/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 16%|â–ˆâ–Œ        | 8/50 [37:11:43<169:07:53, 14497.00s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_213321-wn172lhn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-flower-38
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/wn172lhn

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.4815 | Eval Loss: 0.4811 | Eval R2: -252.6046 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3961 | Eval Loss: 0.4242 | Eval R2: -225.0882 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3500 | Eval Loss: 0.3741 | Eval R2: -194.9351 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3153 | Eval Loss: 0.3374 | Eval R2: -170.2828 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2881 | Eval Loss: 0.3074 | Eval R2: -152.6624 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2635 | Eval Loss: 0.2789 | Eval R2: -135.9023 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2410 | Eval Loss: 0.2536 | Eval R2: -121.4073 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2202 | Eval Loss: 0.2297 | Eval R2: -108.5136 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2011 | Eval Loss: 0.2085 | Eval R2: -96.9089 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1837 | Eval Loss: 0.1894 | Eval R2: -86.5584 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1680 | Eval Loss: 0.1721 | Eval R2: -77.1952 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1536 | Eval Loss: 0.1564 | Eval R2: -68.6297 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1403 | Eval Loss: 0.1418 | Eval R2: -60.6983 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1279 | Eval Loss: 0.1283 | Eval R2: -53.3199 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1165 | Eval Loss: 0.1158 | Eval R2: -46.4314 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1057 | Eval Loss: 0.1044 | Eval R2: -40.0895 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0960 | Eval Loss: 0.0941 | Eval R2: -34.3413 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0873 | Eval Loss: 0.0847 | Eval R2: -29.0723 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0795 | Eval Loss: 0.0766 | Eval R2: -24.4850 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0725 | Eval Loss: 0.0695 | Eval R2: -20.5034 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0668 | Eval Loss: 0.0636 | Eval R2: -17.1655 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0619 | Eval Loss: 0.0588 | Eval R2: -14.4439 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0580 | Eval Loss: 0.0548 | Eval R2: -12.1957 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0544 | Eval Loss: 0.0515 | Eval R2: -10.4209 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0517 | Eval Loss: 0.0489 | Eval R2: -9.0080 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0495 | Eval Loss: 0.0467 | Eval R2: -7.9263 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0476 | Eval Loss: 0.0450 | Eval R2: -7.0924 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.040512, test R2 score: -5.472537
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04760420694947243, 'r2_eval_final': -7.092442989349365, 'loss_eval_final': 0.044960830360651016, 'r2_test': -5.472536730636471, 'loss_test': 0.04051230102777481, 'loss_nodes': [[0.013047851622104645, 0.018070265650749207, 0.023221027106046677, 0.06525082886219025, 0.03288935869932175, 0.02020418271422386, 0.03260619938373566, 0.035536352545022964, 0.021939115598797798, 0.03132022172212601, 0.08339089900255203, 0.023572735488414764, 0.07175716757774353, 0.02670600824058056, 0.030551351606845856, 0.027739329263567924, 0.03681604564189911, 0.05908837169408798, 0.11071457713842392, 0.045824192464351654]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0476
wandb: loss_eval 0.04496
wandb: loss_test 0.04051
wandb:   r2_eval -7.09244
wandb:   r2_test -5.47254
wandb: 
wandb: ðŸš€ View run dauntless-flower-38 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/wn172lhn
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_213321-wn172lhn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 18%|â–ˆâ–Š        | 9/50 [37:24:04<116:07:39, 10196.59s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_214542-cftnilsm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-salad-39
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/cftnilsm

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.4426 | Eval Loss: 0.2904 | Eval R2: -140.2074 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2224 | Eval Loss: 0.1794 | Eval R2: -73.7693 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.1363 | Eval Loss: 0.1085 | Eval R2: -35.5509 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0877 | Eval Loss: 0.0754 | Eval R2: -15.6763 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0688 | Eval Loss: 0.0636 | Eval R2: -9.0970 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0607 | Eval Loss: 0.0575 | Eval R2: -7.0900 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0546 | Eval Loss: 0.0514 | Eval R2: -6.4681 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0489 | Eval Loss: 0.0458 | Eval R2: -5.8609 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0440 | Eval Loss: 0.0411 | Eval R2: -5.2720 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0401 | Eval Loss: 0.0377 | Eval R2: -4.7836 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0370 | Eval Loss: 0.0357 | Eval R2: -4.4372 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0347 | Eval Loss: 0.0350 | Eval R2: -4.8079 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0330 | Eval Loss: 0.0346 | Eval R2: -5.2236 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0316 | Eval Loss: 0.0345 | Eval R2: -5.6412 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0305 | Eval Loss: 0.0344 | Eval R2: -5.7509 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0297 | Eval Loss: 0.0345 | Eval R2: -5.7571 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0292 | Eval Loss: 0.0337 | Eval R2: -5.0826 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0288 | Eval Loss: 0.0319 | Eval R2: -4.0256 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0282 | Eval Loss: 0.0296 | Eval R2: -2.9325 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0269 | Eval Loss: 0.0278 | Eval R2: -2.3759 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0256 | Eval Loss: 0.0266 | Eval R2: -2.2392 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0245 | Eval Loss: 0.0260 | Eval R2: -2.2364 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0237 | Eval Loss: 0.0256 | Eval R2: -2.1810 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0233 | Eval Loss: 0.0255 | Eval R2: -2.1165 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0229 | Eval Loss: 0.0252 | Eval R2: -2.0544 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0226 | Eval Loss: 0.0252 | Eval R2: -2.0074 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0224 | Eval Loss: 0.0253 | Eval R2: -1.9985 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.023186, test R2 score: -1.977282
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.022421810775995255, 'r2_eval_final': -1.9985336065292358, 'loss_eval_final': 0.025270558893680573, 'r2_test': -1.9772817127524493, 'loss_test': 0.02318604104220867, 'loss_nodes': [[0.012929486110806465, 0.01229389850050211, 0.016530098393559456, 0.016765454784035683, 0.019320206716656685, 0.015845535323023796, 0.018736882135272026, 0.025527607649564743, 0.021486492827534676, 0.026720525696873665, 0.02374999038875103, 0.022919384762644768, 0.026861265301704407, 0.021556925028562546, 0.028532864525914192, 0.03233443573117256, 0.027848223224282265, 0.035395607352256775, 0.0271533764898777, 0.03121252916753292]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02242
wandb: loss_eval 0.02527
wandb: loss_test 0.02319
wandb:   r2_eval -1.99853
wandb:   r2_test -1.97728
wandb: 
wandb: ðŸš€ View run autumn-salad-39 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/cftnilsm
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_214542-cftnilsm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 20%|â–ˆâ–ˆ        | 10/50 [48:30:42<215:31:15, 19396.89s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_085220-fyhwfv9p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-fog-40
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/fyhwfv9p

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7227 | Eval Loss: 0.6804 | Eval R2: -323.2787 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5604 | Eval Loss: 0.5418 | Eval R2: -264.1490 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4471 | Eval Loss: 0.4389 | Eval R2: -215.9712 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3593 | Eval Loss: 0.3478 | Eval R2: -165.2689 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2855 | Eval Loss: 0.2727 | Eval R2: -123.9424 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2229 | Eval Loss: 0.2105 | Eval R2: -91.0042 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1723 | Eval Loss: 0.1604 | Eval R2: -62.2215 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1328 | Eval Loss: 0.1233 | Eval R2: -41.6332 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1047 | Eval Loss: 0.0984 | Eval R2: -27.6248 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0865 | Eval Loss: 0.0829 | Eval R2: -19.2969 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0749 | Eval Loss: 0.0726 | Eval R2: -14.4620 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0670 | Eval Loss: 0.0647 | Eval R2: -11.1466 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0610 | Eval Loss: 0.0587 | Eval R2: -9.0252 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0562 | Eval Loss: 0.0543 | Eval R2: -7.7063 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0523 | Eval Loss: 0.0506 | Eval R2: -6.9013 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0484 | Eval Loss: 0.0460 | Eval R2: -5.7618 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0436 | Eval Loss: 0.0419 | Eval R2: -5.6714 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0403 | Eval Loss: 0.0389 | Eval R2: -4.6014 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0377 | Eval Loss: 0.0367 | Eval R2: -4.2823 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0355 | Eval Loss: 0.0346 | Eval R2: -3.8286 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0337 | Eval Loss: 0.0329 | Eval R2: -3.5361 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0322 | Eval Loss: 0.0315 | Eval R2: -3.2825 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0310 | Eval Loss: 0.0304 | Eval R2: -3.1057 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0299 | Eval Loss: 0.0295 | Eval R2: -2.9955 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0289 | Eval Loss: 0.0286 | Eval R2: -2.8798 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0282 | Eval Loss: 0.0280 | Eval R2: -2.8236 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0275 | Eval Loss: 0.0273 | Eval R2: -2.7753 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.024648, test R2 score: -2.223157
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.027492057532072067, 'r2_eval_final': -2.775306463241577, 'loss_eval_final': 0.027310319244861603, 'r2_test': -2.223157346514366, 'loss_test': 0.024648189544677734, 'loss_nodes': [[0.010078394785523415, 0.022637808695435524, 0.027692966163158417, 0.024502955377101898, 0.017171785235404968, 0.023839110508561134, 0.02418074570596218, 0.019745435565710068, 0.02104807272553444, 0.02320036105811596, 0.020600177347660065, 0.021420828998088837, 0.021476341411471367, 0.034953609108924866, 0.024957992136478424, 0.025513965636491776, 0.026987861841917038, 0.032404229044914246, 0.03108600713312626, 0.03946516290307045]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02749
wandb: loss_eval 0.02731
wandb: loss_test 0.02465
wandb:   r2_eval -2.77531
wandb:   r2_test -2.22316
wandb: 
wandb: ðŸš€ View run charmed-fog-40 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/fyhwfv9p
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_085220-fyhwfv9p/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 22%|â–ˆâ–ˆâ–       | 11/50 [48:53:05<150:16:26, 13871.44s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_091443-ezviesij
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run brisk-smoke-41
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ezviesij

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.4815 | Eval Loss: 0.4811 | Eval R2: -252.6046 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3961 | Eval Loss: 0.4242 | Eval R2: -225.0882 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3500 | Eval Loss: 0.3741 | Eval R2: -194.9351 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3153 | Eval Loss: 0.3374 | Eval R2: -170.2828 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2881 | Eval Loss: 0.3074 | Eval R2: -152.6624 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2635 | Eval Loss: 0.2789 | Eval R2: -135.9023 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2410 | Eval Loss: 0.2536 | Eval R2: -121.4073 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2202 | Eval Loss: 0.2297 | Eval R2: -108.5136 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2011 | Eval Loss: 0.2085 | Eval R2: -96.9089 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1837 | Eval Loss: 0.1894 | Eval R2: -86.5584 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1680 | Eval Loss: 0.1721 | Eval R2: -77.1952 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1536 | Eval Loss: 0.1564 | Eval R2: -68.6297 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1403 | Eval Loss: 0.1418 | Eval R2: -60.6983 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1279 | Eval Loss: 0.1283 | Eval R2: -53.3199 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1165 | Eval Loss: 0.1158 | Eval R2: -46.4314 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1057 | Eval Loss: 0.1044 | Eval R2: -40.0895 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0960 | Eval Loss: 0.0941 | Eval R2: -34.3413 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0873 | Eval Loss: 0.0847 | Eval R2: -29.0723 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0795 | Eval Loss: 0.0766 | Eval R2: -24.4850 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0725 | Eval Loss: 0.0695 | Eval R2: -20.5034 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0668 | Eval Loss: 0.0636 | Eval R2: -17.1655 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0619 | Eval Loss: 0.0588 | Eval R2: -14.4439 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0580 | Eval Loss: 0.0548 | Eval R2: -12.1957 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0544 | Eval Loss: 0.0515 | Eval R2: -10.4209 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0517 | Eval Loss: 0.0489 | Eval R2: -9.0080 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0495 | Eval Loss: 0.0467 | Eval R2: -7.9263 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0476 | Eval Loss: 0.0450 | Eval R2: -7.0924 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.040512, test R2 score: -5.472537
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04760420694947243, 'r2_eval_final': -7.092442989349365, 'loss_eval_final': 0.044960830360651016, 'r2_test': -5.472536730636471, 'loss_test': 0.04051230102777481, 'loss_nodes': [[0.013047851622104645, 0.018070265650749207, 0.023221027106046677, 0.06525082886219025, 0.03288935869932175, 0.02020418271422386, 0.03260619938373566, 0.035536352545022964, 0.021939115598797798, 0.03132022172212601, 0.08339089900255203, 0.023572735488414764, 0.07175716757774353, 0.02670600824058056, 0.030551351606845856, 0.027739329263567924, 0.03681604564189911, 0.05908837169408798, 0.11071457713842392, 0.045824192464351654]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0476
wandb: loss_eval 0.04496
wandb: loss_test 0.04051
wandb:   r2_eval -7.09244
wandb:   r2_test -5.47254
wandb: 
wandb: ðŸš€ View run brisk-smoke-41 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ezviesij
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_091443-ezviesij/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 24%|â–ˆâ–ˆâ–       | 12/50 [49:14:03<105:55:00, 10034.24s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_093540-4rhb3gzv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run classic-thunder-42
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/4rhb3gzv

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6020 | Eval Loss: 0.5009 | Eval R2: -236.0820 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3763 | Eval Loss: 0.3544 | Eval R2: -181.4446 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2802 | Eval Loss: 0.2664 | Eval R2: -124.0412 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2135 | Eval Loss: 0.2019 | Eval R2: -85.0074 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1626 | Eval Loss: 0.1544 | Eval R2: -58.8441 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1265 | Eval Loss: 0.1198 | Eval R2: -37.2872 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0989 | Eval Loss: 0.0953 | Eval R2: -24.4402 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0809 | Eval Loss: 0.0799 | Eval R2: -16.5183 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0696 | Eval Loss: 0.0704 | Eval R2: -11.9218 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0630 | Eval Loss: 0.0643 | Eval R2: -9.7829 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0585 | Eval Loss: 0.0597 | Eval R2: -8.3736 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0550 | Eval Loss: 0.0559 | Eval R2: -7.6146 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0521 | Eval Loss: 0.0524 | Eval R2: -7.0104 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0493 | Eval Loss: 0.0491 | Eval R2: -6.6301 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0467 | Eval Loss: 0.0460 | Eval R2: -6.3225 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0441 | Eval Loss: 0.0431 | Eval R2: -6.1359 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0417 | Eval Loss: 0.0405 | Eval R2: -6.1914 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0394 | Eval Loss: 0.0387 | Eval R2: -6.4642 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0374 | Eval Loss: 0.0381 | Eval R2: -7.1202 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0382 | Eval R2: -7.5570 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0342 | Eval Loss: 0.0383 | Eval R2: -7.7715 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0333 | Eval Loss: 0.0378 | Eval R2: -7.5299 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0322 | Eval Loss: 0.0372 | Eval R2: -6.7688 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0310 | Eval Loss: 0.0364 | Eval R2: -5.9444 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0301 | Eval Loss: 0.0356 | Eval R2: -5.4067 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0291 | Eval Loss: 0.0349 | Eval R2: -5.0882 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0284 | Eval Loss: 0.0346 | Eval R2: -4.9198 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.030864, test R2 score: -3.171587
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.028353003785014153, 'r2_eval_final': -4.919759273529053, 'loss_eval_final': 0.03456522896885872, 'r2_test': -3.1715874036991907, 'loss_test': 0.030864080414175987, 'loss_nodes': [[0.012773529626429081, 0.024868059903383255, 0.015408312901854515, 0.01681605353951454, 0.06806385517120361, 0.02215931937098503, 0.03230952471494675, 0.03479593247175217, 0.021007582545280457, 0.02620639279484749, 0.03227386251091957, 0.035604171454906464, 0.04549826681613922, 0.027170099318027496, 0.029605844989418983, 0.04649478197097778, 0.04021649807691574, 0.02710053138434887, 0.02932910993695259, 0.029579810798168182]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02835
wandb: loss_eval 0.03457
wandb: loss_test 0.03086
wandb:   r2_eval -4.91976
wandb:   r2_test -3.17159
wandb: 
wandb: ðŸš€ View run classic-thunder-42 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/4rhb3gzv
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_093540-4rhb3gzv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 26%|â–ˆâ–ˆâ–Œ       | 13/50 [49:46:00<77:51:26, 7575.30s/it]  Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_100738-tal662ru
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-music-43
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/tal662ru

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7714 | Eval Loss: 0.5205 | Eval R2: -254.4348 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4178 | Eval Loss: 0.4084 | Eval R2: -199.8603 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3265 | Eval Loss: 0.3177 | Eval R2: -153.4578 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2499 | Eval Loss: 0.2395 | Eval R2: -110.9164 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1848 | Eval Loss: 0.1718 | Eval R2: -73.5437 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1318 | Eval Loss: 0.1190 | Eval R2: -45.1552 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0916 | Eval Loss: 0.0799 | Eval R2: -25.0720 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0639 | Eval Loss: 0.0560 | Eval R2: -13.1996 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0483 | Eval Loss: 0.0433 | Eval R2: -7.3188 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0406 | Eval Loss: 0.0370 | Eval R2: -4.6713 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0365 | Eval Loss: 0.0335 | Eval R2: -3.5459 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0338 | Eval Loss: 0.0315 | Eval R2: -3.0286 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0318 | Eval Loss: 0.0301 | Eval R2: -2.8447 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0306 | Eval Loss: 0.0291 | Eval R2: -2.7604 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0294 | Eval Loss: 0.0283 | Eval R2: -2.6898 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0284 | Eval Loss: 0.0276 | Eval R2: -2.5861 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0276 | Eval Loss: 0.0270 | Eval R2: -2.5631 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0271 | Eval Loss: 0.0265 | Eval R2: -2.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0264 | Eval Loss: 0.0260 | Eval R2: -2.5134 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0258 | Eval Loss: 0.0255 | Eval R2: -2.4373 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0253 | Eval Loss: 0.0251 | Eval R2: -2.3653 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0249 | Eval Loss: 0.0247 | Eval R2: -2.3199 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0243 | Eval Loss: 0.0244 | Eval R2: -2.2803 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0239 | Eval Loss: 0.0242 | Eval R2: -2.3354 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0236 | Eval Loss: 0.0240 | Eval R2: -2.3648 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0233 | Eval Loss: 0.0238 | Eval R2: -2.3137 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0231 | Eval Loss: 0.0235 | Eval R2: -2.3282 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021735, test R2 score: -2.168550
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02310175821185112, 'r2_eval_final': -2.328239917755127, 'loss_eval_final': 0.02352406457066536, 'r2_test': -2.168550051183605, 'loss_test': 0.02173549123108387, 'loss_nodes': [[0.01034513395279646, 0.014820292592048645, 0.012998241931200027, 0.018518369644880295, 0.018829427659511566, 0.016647713258862495, 0.019268568605184555, 0.021240735426545143, 0.023611489683389664, 0.022105509415268898, 0.021022344008088112, 0.021646859124302864, 0.02162214182317257, 0.02224665880203247, 0.025109993293881416, 0.027386581525206566, 0.028227152302861214, 0.02826693095266819, 0.03202541172504425, 0.02877020463347435]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0231
wandb: loss_eval 0.02352
wandb: loss_test 0.02174
wandb:   r2_eval -2.32824
wandb:   r2_test -2.16855
wandb: 
wandb: ðŸš€ View run devoted-music-43 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/tal662ru
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_100738-tal662ru/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 28%|â–ˆâ–ˆâ–Š       | 14/50 [66:41:07<236:50:27, 23684.08s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_030245-mko244mc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-wood-44
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/mko244mc

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.4815 | Eval Loss: 0.4811 | Eval R2: -252.6046 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3961 | Eval Loss: 0.4242 | Eval R2: -225.0882 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3500 | Eval Loss: 0.3741 | Eval R2: -194.9351 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3153 | Eval Loss: 0.3374 | Eval R2: -170.2828 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2881 | Eval Loss: 0.3074 | Eval R2: -152.6624 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2635 | Eval Loss: 0.2789 | Eval R2: -135.9023 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2410 | Eval Loss: 0.2536 | Eval R2: -121.4073 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2202 | Eval Loss: 0.2297 | Eval R2: -108.5136 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2011 | Eval Loss: 0.2085 | Eval R2: -96.9089 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1837 | Eval Loss: 0.1894 | Eval R2: -86.5584 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1680 | Eval Loss: 0.1721 | Eval R2: -77.1952 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1536 | Eval Loss: 0.1564 | Eval R2: -68.6297 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1403 | Eval Loss: 0.1418 | Eval R2: -60.6983 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1279 | Eval Loss: 0.1283 | Eval R2: -53.3199 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1165 | Eval Loss: 0.1158 | Eval R2: -46.4314 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1057 | Eval Loss: 0.1044 | Eval R2: -40.0895 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0960 | Eval Loss: 0.0941 | Eval R2: -34.3413 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0873 | Eval Loss: 0.0847 | Eval R2: -29.0723 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0795 | Eval Loss: 0.0766 | Eval R2: -24.4850 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0725 | Eval Loss: 0.0695 | Eval R2: -20.5034 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0668 | Eval Loss: 0.0636 | Eval R2: -17.1655 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0619 | Eval Loss: 0.0588 | Eval R2: -14.4439 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0580 | Eval Loss: 0.0548 | Eval R2: -12.1957 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0544 | Eval Loss: 0.0515 | Eval R2: -10.4209 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0517 | Eval Loss: 0.0489 | Eval R2: -9.0080 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0495 | Eval Loss: 0.0467 | Eval R2: -7.9263 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0476 | Eval Loss: 0.0450 | Eval R2: -7.0924 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.040512, test R2 score: -5.472537
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04760420694947243, 'r2_eval_final': -7.092442989349365, 'loss_eval_final': 0.044960830360651016, 'r2_test': -5.472536730636471, 'loss_test': 0.04051230102777481, 'loss_nodes': [[0.013047851622104645, 0.018070265650749207, 0.023221027106046677, 0.06525082886219025, 0.03288935869932175, 0.02020418271422386, 0.03260619938373566, 0.035536352545022964, 0.021939115598797798, 0.03132022172212601, 0.08339089900255203, 0.023572735488414764, 0.07175716757774353, 0.02670600824058056, 0.030551351606845856, 0.027739329263567924, 0.03681604564189911, 0.05908837169408798, 0.11071457713842392, 0.045824192464351654]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0476
wandb: loss_eval 0.04496
wandb: loss_test 0.04051
wandb:   r2_eval -7.09244
wandb:   r2_test -5.47254
wandb: 
wandb: ðŸš€ View run polar-wood-44 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/mko244mc
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_030245-mko244mc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [67:01:45<164:28:51, 16918.05s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_032322-5syjih0s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-breeze-45
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/5syjih0s

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.3990 | Eval Loss: 0.3604 | Eval R2: -181.4873 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2826 | Eval Loss: 0.2711 | Eval R2: -134.3394 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2171 | Eval Loss: 0.2146 | Eval R2: -100.5849 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1802 | Eval Loss: 0.1837 | Eval R2: -77.8066 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1630 | Eval Loss: 0.1696 | Eval R2: -64.2035 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1569 | Eval Loss: 0.1645 | Eval R2: -57.5380 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1552 | Eval Loss: 0.1629 | Eval R2: -55.1687 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1546 | Eval Loss: 0.1625 | Eval R2: -54.8663 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1543 | Eval Loss: 0.1625 | Eval R2: -55.2321 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6017 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7840 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8126 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7760 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7363 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7133 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1540 | Eval Loss: 0.1625 | Eval R2: -55.7030 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6975 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6926 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6874 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6823 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6770 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6718 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6667 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6619 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6575 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6534 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6494 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121123, test R2 score: -43.344965
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.15409402549266815, 'r2_eval_final': -55.64940643310547, 'loss_eval_final': 0.16253405809402466, 'r2_test': -43.34496461694144, 'loss_test': 0.12112276256084442, 'loss_nodes': [[0.11829982697963715, 0.12032046914100647, 0.11829836666584015, 0.12113984674215317, 0.12057189643383026, 0.11906023323535919, 0.12270093709230423, 0.11888550966978073, 0.12161321192979813, 0.11873836815357208, 0.12021375447511673, 0.12306991219520569, 0.11966392397880554, 0.12314219027757645, 0.11990226060152054, 0.12138199806213379, 0.12442529201507568, 0.12209079414606094, 0.12596818804740906, 0.12296821922063828]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15409
wandb: loss_eval 0.16253
wandb: loss_test 0.12112
wandb:   r2_eval -55.64941
wandb:   r2_test -43.34496
wandb: 
wandb: ðŸš€ View run sweet-breeze-45 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/5syjih0s
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_032322-5syjih0s/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [67:34:18<117:14:20, 12413.56s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_035555-yvsyvtna
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-shadow-46
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/yvsyvtna

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7227 | Eval Loss: 0.6804 | Eval R2: -323.2787 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5604 | Eval Loss: 0.5418 | Eval R2: -264.1490 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4471 | Eval Loss: 0.4389 | Eval R2: -215.9712 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3593 | Eval Loss: 0.3478 | Eval R2: -165.2689 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2855 | Eval Loss: 0.2727 | Eval R2: -123.9424 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2229 | Eval Loss: 0.2105 | Eval R2: -91.0042 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1723 | Eval Loss: 0.1604 | Eval R2: -62.2215 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1328 | Eval Loss: 0.1233 | Eval R2: -41.6332 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1047 | Eval Loss: 0.0984 | Eval R2: -27.6248 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0865 | Eval Loss: 0.0829 | Eval R2: -19.2969 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0749 | Eval Loss: 0.0726 | Eval R2: -14.4620 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0670 | Eval Loss: 0.0647 | Eval R2: -11.1466 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0610 | Eval Loss: 0.0587 | Eval R2: -9.0252 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0562 | Eval Loss: 0.0543 | Eval R2: -7.7063 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0523 | Eval Loss: 0.0506 | Eval R2: -6.9013 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0484 | Eval Loss: 0.0460 | Eval R2: -5.7618 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0436 | Eval Loss: 0.0419 | Eval R2: -5.6714 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0403 | Eval Loss: 0.0389 | Eval R2: -4.6014 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0377 | Eval Loss: 0.0367 | Eval R2: -4.2823 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0355 | Eval Loss: 0.0346 | Eval R2: -3.8286 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0337 | Eval Loss: 0.0329 | Eval R2: -3.5361 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0322 | Eval Loss: 0.0315 | Eval R2: -3.2825 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0310 | Eval Loss: 0.0304 | Eval R2: -3.1057 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0299 | Eval Loss: 0.0295 | Eval R2: -2.9955 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0289 | Eval Loss: 0.0286 | Eval R2: -2.8798 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0282 | Eval Loss: 0.0280 | Eval R2: -2.8236 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0275 | Eval Loss: 0.0273 | Eval R2: -2.7753 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.024648, test R2 score: -2.223157
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.027492057532072067, 'r2_eval_final': -2.775306463241577, 'loss_eval_final': 0.027310319244861603, 'r2_test': -2.223157346514366, 'loss_test': 0.024648189544677734, 'loss_nodes': [[0.010078394785523415, 0.022637808695435524, 0.027692966163158417, 0.024502955377101898, 0.017171785235404968, 0.023839110508561134, 0.02418074570596218, 0.019745435565710068, 0.02104807272553444, 0.02320036105811596, 0.020600177347660065, 0.021420828998088837, 0.021476341411471367, 0.034953609108924866, 0.024957992136478424, 0.025513965636491776, 0.026987861841917038, 0.032404229044914246, 0.03108600713312626, 0.03946516290307045]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02749
wandb: loss_eval 0.02731
wandb: loss_test 0.02465
wandb:   r2_eval -2.77531
wandb:   r2_test -2.22316
wandb: 
wandb: ðŸš€ View run silver-shadow-46 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/yvsyvtna
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_035555-yvsyvtna/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [68:16:04<86:28:55, 9434.41s/it]  Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_043741-6dedr81c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-disco-47
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/6dedr81c

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8456 | Eval Loss: 0.7891 | Eval R2: -391.8618 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6623 | Eval Loss: 0.6863 | Eval R2: -346.7439 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5731 | Eval Loss: 0.5967 | Eval R2: -304.6748 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4948 | Eval Loss: 0.5136 | Eval R2: -262.7293 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4227 | Eval Loss: 0.4355 | Eval R2: -220.5490 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3574 | Eval Loss: 0.3653 | Eval R2: -180.3570 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3011 | Eval Loss: 0.3060 | Eval R2: -144.9742 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2553 | Eval Loss: 0.2589 | Eval R2: -116.3549 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2200 | Eval Loss: 0.2238 | Eval R2: -94.9898 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1944 | Eval Loss: 0.1994 | Eval R2: -80.1069 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1772 | Eval Loss: 0.1836 | Eval R2: -70.2897 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1665 | Eval Loss: 0.1740 | Eval R2: -64.0779 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1604 | Eval Loss: 0.1685 | Eval R2: -60.3010 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1571 | Eval Loss: 0.1655 | Eval R2: -58.1231 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1554 | Eval Loss: 0.1639 | Eval R2: -56.9548 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1545 | Eval Loss: 0.1632 | Eval R2: -56.3781 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1542 | Eval Loss: 0.1629 | Eval R2: -56.1125 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1540 | Eval Loss: 0.1627 | Eval R2: -55.9907 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1540 | Eval Loss: 0.1627 | Eval R2: -55.9278 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1539 | Eval Loss: 0.1626 | Eval R2: -55.8892 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1539 | Eval Loss: 0.1626 | Eval R2: -55.8632 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8456 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8340 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8262 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8202 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8149 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8099 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121037, test R2 score: -43.482063
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.15397050976753235, 'r2_eval_final': -55.809906005859375, 'loss_eval_final': 0.16258804500102997, 'r2_test': -43.48206306364732, 'loss_test': 0.12103690952062607, 'loss_nodes': [[0.118257075548172, 0.12019184231758118, 0.11839974671602249, 0.12108329683542252, 0.12024587392807007, 0.11900562793016434, 0.12283622473478317, 0.11872217059135437, 0.12147502601146698, 0.11863381415605545, 0.11987369507551193, 0.12307041883468628, 0.11947842687368393, 0.12297122180461884, 0.11982066929340363, 0.12157928198575974, 0.12444840371608734, 0.12173856794834137, 0.12587204575538635, 0.12303481251001358]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15397
wandb: loss_eval 0.16259
wandb: loss_test 0.12104
wandb:   r2_eval -55.80991
wandb:   r2_test -43.48206
wandb: 
wandb: ðŸš€ View run true-disco-47 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/6dedr81c
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_043741-6dedr81c/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [68:39:18<62:23:05, 7018.29s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_050055-cr8obs1a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-cosmos-48
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/cr8obs1a

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7661 | Eval Loss: 0.7003 | Eval R2: -349.9211 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5525 | Eval Loss: 0.5353 | Eval R2: -273.0165 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4250 | Eval Loss: 0.4225 | Eval R2: -216.4501 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3376 | Eval Loss: 0.3402 | Eval R2: -171.9990 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2736 | Eval Loss: 0.2768 | Eval R2: -135.4781 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2255 | Eval Loss: 0.2288 | Eval R2: -106.0095 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1914 | Eval Loss: 0.1960 | Eval R2: -84.0105 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1706 | Eval Loss: 0.1770 | Eval R2: -69.4671 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1603 | Eval Loss: 0.1678 | Eval R2: -61.2041 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1560 | Eval Loss: 0.1641 | Eval R2: -57.3077 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1546 | Eval Loss: 0.1629 | Eval R2: -55.9031 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1542 | Eval Loss: 0.1626 | Eval R2: -55.6159 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.6654 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7362 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7653 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7707 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7712 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7711 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7688 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7637 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7571 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7503 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7442 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7387 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7336 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7287 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7239 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121088, test R2 score: -43.408200
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.15404623746871948, 'r2_eval_final': -55.72392272949219, 'loss_eval_final': 0.1625654697418213, 'r2_test': -43.40820035312648, 'loss_test': 0.12108776718378067, 'loss_nodes': [[0.1183430626988411, 0.12026990205049515, 0.11831372231245041, 0.12112147361040115, 0.12032140046358109, 0.11910030990839005, 0.12276194244623184, 0.11860685795545578, 0.12157513946294785, 0.11865835636854172, 0.11987261474132538, 0.12326528131961823, 0.11956842988729477, 0.1232152208685875, 0.1200319230556488, 0.12140730023384094, 0.12456391006708145, 0.12189489603042603, 0.12593021988868713, 0.12293344736099243]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15405
wandb: loss_eval 0.16257
wandb: loss_test 0.12109
wandb:   r2_eval -55.72392
wandb:   r2_test -43.4082
wandb: 
wandb: ðŸš€ View run azure-cosmos-48 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/cr8obs1a
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_050055-cr8obs1a/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [69:00:21<45:33:00, 5289.70s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_052158-blhxuynk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-pond-49
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/blhxuynk

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5901 | Eval Loss: 0.5567 | Eval R2: -275.5182 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4648 | Eval Loss: 0.4630 | Eval R2: -233.4434 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3888 | Eval Loss: 0.3989 | Eval R2: -201.7456 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3359 | Eval Loss: 0.3505 | Eval R2: -176.0914 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2952 | Eval Loss: 0.3093 | Eval R2: -152.5707 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2603 | Eval Loss: 0.2642 | Eval R2: -123.8370 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2247 | Eval Loss: 0.2210 | Eval R2: -96.5661 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1881 | Eval Loss: 0.1805 | Eval R2: -72.1001 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1536 | Eval Loss: 0.1442 | Eval R2: -51.4712 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1237 | Eval Loss: 0.1140 | Eval R2: -34.7799 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0994 | Eval Loss: 0.0913 | Eval R2: -22.9373 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0815 | Eval Loss: 0.0752 | Eval R2: -15.2309 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0689 | Eval Loss: 0.0640 | Eval R2: -10.5640 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0599 | Eval Loss: 0.0557 | Eval R2: -7.7541 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0534 | Eval Loss: 0.0492 | Eval R2: -6.0911 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0481 | Eval Loss: 0.0443 | Eval R2: -5.1267 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0440 | Eval Loss: 0.0407 | Eval R2: -4.5249 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0405 | Eval Loss: 0.0377 | Eval R2: -4.0303 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0381 | Eval Loss: 0.0356 | Eval R2: -3.6707 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0363 | Eval Loss: 0.0343 | Eval R2: -3.4814 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0349 | Eval Loss: 0.0333 | Eval R2: -3.4031 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0339 | Eval Loss: 0.0323 | Eval R2: -3.1653 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0329 | Eval Loss: 0.0317 | Eval R2: -3.0352 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0320 | Eval Loss: 0.0311 | Eval R2: -2.9622 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0311 | Eval Loss: 0.0302 | Eval R2: -2.7872 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0304 | Eval Loss: 0.0299 | Eval R2: -2.7469 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0296 | Eval Loss: 0.0292 | Eval R2: -2.6456 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.026060, test R2 score: -1.985984
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.029585599899291992, 'r2_eval_final': -2.6455605030059814, 'loss_eval_final': 0.02923102304339409, 'r2_test': -1.985983764285082, 'loss_test': 0.02605963684618473, 'loss_nodes': [[0.015324161387979984, 0.020004939287900925, 0.01928715966641903, 0.018079765141010284, 0.022049764171242714, 0.02042791247367859, 0.020780399441719055, 0.026649001985788345, 0.02527480386197567, 0.02290101908147335, 0.027059081941843033, 0.03494635596871376, 0.03042587637901306, 0.024426916614174843, 0.032683659344911575, 0.025832485407590866, 0.038024723529815674, 0.03338044509291649, 0.030360151082277298, 0.03327413648366928]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02959
wandb: loss_eval 0.02923
wandb: loss_test 0.02606
wandb:   r2_eval -2.64556
wandb:   r2_test -1.98598
wandb: 
wandb: ðŸš€ View run lyric-pond-49 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/blhxuynk
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_052158-blhxuynk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [69:14:58<33:02:30, 3965.02s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_053636-pz9l0qr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fallen-pyramid-50
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/pz9l0qr8

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9199 | Eval Loss: 0.6655 | Eval R2: -315.6604 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5516 | Eval Loss: 0.5087 | Eval R2: -247.8306 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4104 | Eval Loss: 0.3863 | Eval R2: -179.0765 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3050 | Eval Loss: 0.2826 | Eval R2: -129.7987 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2213 | Eval Loss: 0.1982 | Eval R2: -76.9929 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1550 | Eval Loss: 0.1368 | Eval R2: -44.6941 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1102 | Eval Loss: 0.0989 | Eval R2: -22.7151 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0839 | Eval Loss: 0.0810 | Eval R2: -13.8152 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0714 | Eval Loss: 0.0709 | Eval R2: -9.9708 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0639 | Eval Loss: 0.0644 | Eval R2: -8.5966 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0585 | Eval Loss: 0.0583 | Eval R2: -7.6825 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0532 | Eval Loss: 0.0523 | Eval R2: -6.8804 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0476 | Eval Loss: 0.0469 | Eval R2: -6.1622 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0431 | Eval Loss: 0.0423 | Eval R2: -5.3624 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0393 | Eval Loss: 0.0385 | Eval R2: -5.4369 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0361 | Eval Loss: 0.0357 | Eval R2: -6.0360 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0336 | Eval Loss: 0.0336 | Eval R2: -6.3059 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0319 | Eval Loss: 0.0319 | Eval R2: -6.1958 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0304 | Eval Loss: 0.0306 | Eval R2: -5.8053 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0292 | Eval Loss: 0.0296 | Eval R2: -5.4967 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0284 | Eval Loss: 0.0286 | Eval R2: -5.0425 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0277 | Eval Loss: 0.0278 | Eval R2: -4.6932 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0270 | Eval Loss: 0.0272 | Eval R2: -4.4474 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0264 | Eval Loss: 0.0268 | Eval R2: -4.4377 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0259 | Eval Loss: 0.0263 | Eval R2: -4.3585 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0254 | Eval Loss: 0.0261 | Eval R2: -4.4841 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0249 | Eval Loss: 0.0260 | Eval R2: -4.6936 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.023343, test R2 score: -4.356578
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.024938644841313362, 'r2_eval_final': -4.693597793579102, 'loss_eval_final': 0.026011090725660324, 'r2_test': -4.356578322981679, 'loss_test': 0.023343238979578018, 'loss_nodes': [[0.013506067916750908, 0.015133285894989967, 0.01765330694615841, 0.024938806891441345, 0.02021697349846363, 0.018739735707640648, 0.020420990884304047, 0.023030132055282593, 0.02236196957528591, 0.02394014224410057, 0.02449779212474823, 0.023477066308259964, 0.02429197169840336, 0.024188939481973648, 0.026932837441563606, 0.027296394109725952, 0.030268127098679543, 0.028231792151927948, 0.028659004718065262, 0.029079433530569077]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02494
wandb: loss_eval 0.02601
wandb: loss_test 0.02334
wandb:   r2_eval -4.6936
wandb:   r2_test -4.35658
wandb: 
wandb: ðŸš€ View run fallen-pyramid-50 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/pz9l0qr8
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_053636-pz9l0qr8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [80:56:55<124:11:29, 15416.87s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_171832-527daz6j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run logical-bush-51
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/527daz6j

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.4815 | Eval Loss: 0.4811 | Eval R2: -252.6046 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3961 | Eval Loss: 0.4242 | Eval R2: -225.0882 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3500 | Eval Loss: 0.3741 | Eval R2: -194.9351 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3153 | Eval Loss: 0.3374 | Eval R2: -170.2828 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2881 | Eval Loss: 0.3074 | Eval R2: -152.6624 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2635 | Eval Loss: 0.2789 | Eval R2: -135.9023 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2410 | Eval Loss: 0.2536 | Eval R2: -121.4073 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2202 | Eval Loss: 0.2297 | Eval R2: -108.5136 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2011 | Eval Loss: 0.2085 | Eval R2: -96.9089 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1837 | Eval Loss: 0.1894 | Eval R2: -86.5584 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1680 | Eval Loss: 0.1721 | Eval R2: -77.1952 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1536 | Eval Loss: 0.1564 | Eval R2: -68.6297 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1403 | Eval Loss: 0.1418 | Eval R2: -60.6983 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1279 | Eval Loss: 0.1283 | Eval R2: -53.3199 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1165 | Eval Loss: 0.1158 | Eval R2: -46.4314 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1057 | Eval Loss: 0.1044 | Eval R2: -40.0895 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0960 | Eval Loss: 0.0941 | Eval R2: -34.3413 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0873 | Eval Loss: 0.0847 | Eval R2: -29.0723 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0795 | Eval Loss: 0.0766 | Eval R2: -24.4850 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0725 | Eval Loss: 0.0695 | Eval R2: -20.5034 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0668 | Eval Loss: 0.0636 | Eval R2: -17.1655 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0619 | Eval Loss: 0.0588 | Eval R2: -14.4439 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0580 | Eval Loss: 0.0548 | Eval R2: -12.1957 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0544 | Eval Loss: 0.0515 | Eval R2: -10.4209 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0517 | Eval Loss: 0.0489 | Eval R2: -9.0080 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0495 | Eval Loss: 0.0467 | Eval R2: -7.9263 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0476 | Eval Loss: 0.0450 | Eval R2: -7.0924 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.040512, test R2 score: -5.472537
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04760420694947243, 'r2_eval_final': -7.092442989349365, 'loss_eval_final': 0.044960830360651016, 'r2_test': -5.472536730636471, 'loss_test': 0.04051230102777481, 'loss_nodes': [[0.013047851622104645, 0.018070265650749207, 0.023221027106046677, 0.06525082886219025, 0.03288935869932175, 0.02020418271422386, 0.03260619938373566, 0.035536352545022964, 0.021939115598797798, 0.03132022172212601, 0.08339089900255203, 0.023572735488414764, 0.07175716757774353, 0.02670600824058056, 0.030551351606845856, 0.027739329263567924, 0.03681604564189911, 0.05908837169408798, 0.11071457713842392, 0.045824192464351654]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0476
wandb: loss_eval 0.04496
wandb: loss_test 0.04051
wandb:   r2_eval -7.09244
wandb:   r2_test -5.47254
wandb: 
wandb: ðŸš€ View run logical-bush-51 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/527daz6j
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_171832-527daz6j/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [81:08:55<85:36:08, 11006.03s/it] Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_173032-7vrqpp4d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-river-52
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/7vrqpp4d

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7661 | Eval Loss: 0.7003 | Eval R2: -349.9211 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5525 | Eval Loss: 0.5353 | Eval R2: -273.0165 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4250 | Eval Loss: 0.4225 | Eval R2: -216.4501 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3376 | Eval Loss: 0.3402 | Eval R2: -171.9990 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2736 | Eval Loss: 0.2768 | Eval R2: -135.4781 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2255 | Eval Loss: 0.2288 | Eval R2: -106.0095 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1914 | Eval Loss: 0.1960 | Eval R2: -84.0105 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1706 | Eval Loss: 0.1770 | Eval R2: -69.4671 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1603 | Eval Loss: 0.1678 | Eval R2: -61.2041 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1560 | Eval Loss: 0.1641 | Eval R2: -57.3077 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1546 | Eval Loss: 0.1629 | Eval R2: -55.9031 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1542 | Eval Loss: 0.1626 | Eval R2: -55.6159 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.6654 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7362 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7653 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7707 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7712 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7711 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7688 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7637 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7571 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7503 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7442 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7387 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7336 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7287 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7239 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121088, test R2 score: -43.408200
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.15404623746871948, 'r2_eval_final': -55.72392272949219, 'loss_eval_final': 0.1625654697418213, 'r2_test': -43.40820035312648, 'loss_test': 0.12108776718378067, 'loss_nodes': [[0.1183430626988411, 0.12026990205049515, 0.11831372231245041, 0.12112147361040115, 0.12032140046358109, 0.11910030990839005, 0.12276194244623184, 0.11860685795545578, 0.12157513946294785, 0.11865835636854172, 0.11987261474132538, 0.12326528131961823, 0.11956842988729477, 0.1232152208685875, 0.1200319230556488, 0.12140730023384094, 0.12456391006708145, 0.12189489603042603, 0.12593021988868713, 0.12293344736099243]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15405
wandb: loss_eval 0.16257
wandb: loss_test 0.12109
wandb:   r2_eval -55.72392
wandb:   r2_test -43.4082
wandb: 
wandb: ðŸš€ View run lively-river-52 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/7vrqpp4d
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_173032-7vrqpp4d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [81:32:11<60:55:05, 8122.44s/it] Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_175349-9l3b4pky
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-aardvark-53
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/9l3b4pky

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7637 | Eval Loss: 0.6701 | Eval R2: -317.3430 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5340 | Eval Loss: 0.4803 | Eval R2: -228.9170 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3840 | Eval Loss: 0.3483 | Eval R2: -163.7201 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2713 | Eval Loss: 0.2349 | Eval R2: -104.1325 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1775 | Eval Loss: 0.1416 | Eval R2: -51.4557 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1089 | Eval Loss: 0.0883 | Eval R2: -20.4105 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0755 | Eval Loss: 0.0681 | Eval R2: -10.1863 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0612 | Eval Loss: 0.0565 | Eval R2: -6.8454 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0522 | Eval Loss: 0.0490 | Eval R2: -6.1042 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0465 | Eval Loss: 0.0438 | Eval R2: -5.6676 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0423 | Eval Loss: 0.0400 | Eval R2: -5.2174 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0392 | Eval Loss: 0.0375 | Eval R2: -4.8358 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0369 | Eval Loss: 0.0350 | Eval R2: -4.2403 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0349 | Eval Loss: 0.0328 | Eval R2: -3.6071 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0332 | Eval Loss: 0.0312 | Eval R2: -3.0991 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0316 | Eval Loss: 0.0298 | Eval R2: -2.7922 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0302 | Eval Loss: 0.0290 | Eval R2: -2.6250 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0289 | Eval Loss: 0.0281 | Eval R2: -2.4861 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0281 | Eval Loss: 0.0277 | Eval R2: -2.3891 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0272 | Eval Loss: 0.0272 | Eval R2: -2.3129 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0266 | Eval Loss: 0.0268 | Eval R2: -2.2383 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0262 | Eval Loss: 0.0264 | Eval R2: -2.1568 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0257 | Eval Loss: 0.0260 | Eval R2: -2.0981 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0254 | Eval Loss: 0.0258 | Eval R2: -2.0594 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0250 | Eval Loss: 0.0253 | Eval R2: -2.0007 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0247 | Eval Loss: 0.0249 | Eval R2: -1.9564 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0243 | Eval Loss: 0.0246 | Eval R2: -1.9068 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.022481, test R2 score: -1.838697
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.024334006011486053, 'r2_eval_final': -1.9067635536193848, 'loss_eval_final': 0.024551071226596832, 'r2_test': -1.8386972085446154, 'loss_test': 0.022481068968772888, 'loss_nodes': [[0.011184918694198132, 0.01593499258160591, 0.015584243461489677, 0.017580170184373856, 0.018353881314396858, 0.01770743727684021, 0.02029106579720974, 0.021236738190054893, 0.0215890035033226, 0.025565894320607185, 0.026644032448530197, 0.0224858820438385, 0.025083471089601517, 0.023918600752949715, 0.02571556158363819, 0.02484259009361267, 0.029303256422281265, 0.02864973433315754, 0.03040606528520584, 0.027543844655156136]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02433
wandb: loss_eval 0.02455
wandb: loss_test 0.02248
wandb:   r2_eval -1.90676
wandb:   r2_test -1.8387
wandb: 
wandb: ðŸš€ View run unique-aardvark-53 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/9l3b4pky
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_175349-9l3b4pky/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [81:52:33<43:42:28, 6051.85s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_181411-qa7zb4t8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-bush-54
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/qa7zb4t8

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.4815 | Eval Loss: 0.4811 | Eval R2: -252.6046 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3961 | Eval Loss: 0.4242 | Eval R2: -225.0882 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3500 | Eval Loss: 0.3741 | Eval R2: -194.9351 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3153 | Eval Loss: 0.3374 | Eval R2: -170.2828 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2881 | Eval Loss: 0.3074 | Eval R2: -152.6624 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2635 | Eval Loss: 0.2789 | Eval R2: -135.9023 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2410 | Eval Loss: 0.2536 | Eval R2: -121.4073 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2202 | Eval Loss: 0.2297 | Eval R2: -108.5136 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2011 | Eval Loss: 0.2085 | Eval R2: -96.9089 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1837 | Eval Loss: 0.1894 | Eval R2: -86.5584 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1680 | Eval Loss: 0.1721 | Eval R2: -77.1952 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1536 | Eval Loss: 0.1564 | Eval R2: -68.6297 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1403 | Eval Loss: 0.1418 | Eval R2: -60.6983 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1279 | Eval Loss: 0.1283 | Eval R2: -53.3199 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1165 | Eval Loss: 0.1158 | Eval R2: -46.4314 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1057 | Eval Loss: 0.1044 | Eval R2: -40.0895 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0960 | Eval Loss: 0.0941 | Eval R2: -34.3413 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0873 | Eval Loss: 0.0847 | Eval R2: -29.0723 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0795 | Eval Loss: 0.0766 | Eval R2: -24.4850 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0725 | Eval Loss: 0.0695 | Eval R2: -20.5034 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0668 | Eval Loss: 0.0636 | Eval R2: -17.1655 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0619 | Eval Loss: 0.0588 | Eval R2: -14.4439 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0580 | Eval Loss: 0.0548 | Eval R2: -12.1957 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0544 | Eval Loss: 0.0515 | Eval R2: -10.4209 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0517 | Eval Loss: 0.0489 | Eval R2: -9.0080 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0495 | Eval Loss: 0.0467 | Eval R2: -7.9263 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0476 | Eval Loss: 0.0450 | Eval R2: -7.0924 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.040512, test R2 score: -5.472537
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04760420694947243, 'r2_eval_final': -7.092442989349365, 'loss_eval_final': 0.044960830360651016, 'r2_test': -5.472536730636471, 'loss_test': 0.04051230102777481, 'loss_nodes': [[0.013047851622104645, 0.018070265650749207, 0.023221027106046677, 0.06525082886219025, 0.03288935869932175, 0.02020418271422386, 0.03260619938373566, 0.035536352545022964, 0.021939115598797798, 0.03132022172212601, 0.08339089900255203, 0.023572735488414764, 0.07175716757774353, 0.02670600824058056, 0.030551351606845856, 0.027739329263567924, 0.03681604564189911, 0.05908837169408798, 0.11071457713842392, 0.045824192464351654]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0476
wandb: loss_eval 0.04496
wandb: loss_test 0.04051
wandb:   r2_eval -7.09244
wandb:   r2_test -5.47254
wandb: 
wandb: ðŸš€ View run dauntless-bush-54 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/qa7zb4t8
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_181411-qa7zb4t8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [82:04:13<30:52:29, 4445.97s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_182550-29aujvpq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-smoke-55
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/29aujvpq

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5608 | Eval Loss: 0.5043 | Eval R2: -263.2942 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4319 | Eval Loss: 0.4552 | Eval R2: -236.3751 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3854 | Eval Loss: 0.4039 | Eval R2: -208.6526 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3393 | Eval Loss: 0.3521 | Eval R2: -178.9646 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2962 | Eval Loss: 0.3022 | Eval R2: -152.7563 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2542 | Eval Loss: 0.2572 | Eval R2: -127.9249 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2147 | Eval Loss: 0.2128 | Eval R2: -103.2773 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1772 | Eval Loss: 0.1721 | Eval R2: -80.5471 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1430 | Eval Loss: 0.1357 | Eval R2: -60.4746 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1136 | Eval Loss: 0.1051 | Eval R2: -43.7433 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0891 | Eval Loss: 0.0804 | Eval R2: -30.3633 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0696 | Eval Loss: 0.0616 | Eval R2: -20.1717 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0547 | Eval Loss: 0.0478 | Eval R2: -12.8705 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0441 | Eval Loss: 0.0381 | Eval R2: -7.8224 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0367 | Eval Loss: 0.0321 | Eval R2: -4.8608 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0322 | Eval Loss: 0.0287 | Eval R2: -3.2882 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0297 | Eval Loss: 0.0269 | Eval R2: -2.5678 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0282 | Eval Loss: 0.0261 | Eval R2: -2.3993 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0274 | Eval Loss: 0.0255 | Eval R2: -2.2602 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0267 | Eval Loss: 0.0250 | Eval R2: -2.1927 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0263 | Eval Loss: 0.0245 | Eval R2: -2.0597 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0259 | Eval Loss: 0.0243 | Eval R2: -2.1045 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0253 | Eval Loss: 0.0238 | Eval R2: -1.9499 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0249 | Eval Loss: 0.0236 | Eval R2: -1.8890 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0245 | Eval Loss: 0.0235 | Eval R2: -1.9161 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0242 | Eval Loss: 0.0234 | Eval R2: -1.8680 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0239 | Eval Loss: 0.0233 | Eval R2: -1.8619 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021683, test R2 score: -1.828693
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.023912088945508003, 'r2_eval_final': -1.8618820905685425, 'loss_eval_final': 0.023346971720457077, 'r2_test': -1.8286932420999595, 'loss_test': 0.021683160215616226, 'loss_nodes': [[0.014060916379094124, 0.01170000247657299, 0.015403435565531254, 0.017205623909831047, 0.015195801854133606, 0.021479591727256775, 0.018477344885468483, 0.02352917194366455, 0.018102724105119705, 0.022225940600037575, 0.02125014178454876, 0.020842645317316055, 0.021231699734926224, 0.019900096580386162, 0.02242244966328144, 0.022561129182577133, 0.041831180453300476, 0.02900829166173935, 0.02962755411863327, 0.027607480064034462]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02391
wandb: loss_eval 0.02335
wandb: loss_test 0.02168
wandb:   r2_eval -1.86188
wandb:   r2_test -1.82869
wandb: 
wandb: ðŸš€ View run charmed-smoke-55 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/29aujvpq
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_182550-29aujvpq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [93:25:06<102:27:37, 15369.08s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240722_054643-6acbzhws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run olive-glade-56
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/6acbzhws

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9011 | Eval Loss: 0.6356 | Eval R2: -327.2025 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5182 | Eval Loss: 0.5516 | Eval R2: -291.4135 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4694 | Eval Loss: 0.5201 | Eval R2: -280.9026 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4488 | Eval Loss: 0.4998 | Eval R2: -270.9287 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4314 | Eval Loss: 0.4821 | Eval R2: -260.5455 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4142 | Eval Loss: 0.4633 | Eval R2: -250.8914 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3971 | Eval Loss: 0.4428 | Eval R2: -238.4463 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3790 | Eval Loss: 0.4212 | Eval R2: -226.4512 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3600 | Eval Loss: 0.3980 | Eval R2: -212.5002 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3397 | Eval Loss: 0.3740 | Eval R2: -198.5145 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3186 | Eval Loss: 0.3490 | Eval R2: -183.7760 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2968 | Eval Loss: 0.3234 | Eval R2: -168.9507 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2747 | Eval Loss: 0.2978 | Eval R2: -154.2344 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.2529 | Eval Loss: 0.2725 | Eval R2: -139.8923 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2317 | Eval Loss: 0.2482 | Eval R2: -126.1379 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2115 | Eval Loss: 0.2250 | Eval R2: -113.0703 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1924 | Eval Loss: 0.2037 | Eval R2: -100.8708 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1748 | Eval Loss: 0.1839 | Eval R2: -89.4955 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1581 | Eval Loss: 0.1657 | Eval R2: -79.0073 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1428 | Eval Loss: 0.1488 | Eval R2: -69.2889 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1286 | Eval Loss: 0.1332 | Eval R2: -60.3590 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1153 | Eval Loss: 0.1188 | Eval R2: -52.1624 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1033 | Eval Loss: 0.1057 | Eval R2: -44.7800 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0924 | Eval Loss: 0.0940 | Eval R2: -38.0463 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0827 | Eval Loss: 0.0834 | Eval R2: -32.1291 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0738 | Eval Loss: 0.0741 | Eval R2: -26.8465 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0664 | Eval Loss: 0.0662 | Eval R2: -22.3667 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.050542, test R2 score: -18.350233
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.06639694422483444, 'r2_eval_final': -22.36670684814453, 'loss_eval_final': 0.06617758423089981, 'r2_test': -18.350232869242564, 'loss_test': 0.05054231733083725, 'loss_nodes': [[0.0575648657977581, 0.01828470267355442, 0.016672324389219284, 0.03035132959485054, 0.027186142280697823, 0.044308844953775406, 0.0321664921939373, 0.15961520373821259, 0.023936066776514053, 0.026012448593974113, 0.128427192568779, 0.04421913996338844, 0.03058609925210476, 0.046831898391246796, 0.12244521081447601, 0.030062580481171608, 0.054867032915353775, 0.05127859488129616, 0.03415274992585182, 0.03187747672200203]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0664
wandb: loss_eval 0.06618
wandb: loss_test 0.05054
wandb:   r2_eval -22.36671
wandb:   r2_test -18.35023
wandb: 
wandb: ðŸš€ View run olive-glade-56 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/6acbzhws
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240722_054643-6acbzhws/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [104:48:55<147:22:37, 23067.73s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240722_171033-a0o9xylm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sage-meadow-57
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/a0o9xylm

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7714 | Eval Loss: 0.5205 | Eval R2: -254.4348 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4178 | Eval Loss: 0.4084 | Eval R2: -199.8603 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3265 | Eval Loss: 0.3177 | Eval R2: -153.4578 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2499 | Eval Loss: 0.2395 | Eval R2: -110.9164 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1848 | Eval Loss: 0.1718 | Eval R2: -73.5437 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1318 | Eval Loss: 0.1190 | Eval R2: -45.1552 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0916 | Eval Loss: 0.0799 | Eval R2: -25.0720 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0639 | Eval Loss: 0.0560 | Eval R2: -13.1996 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0483 | Eval Loss: 0.0433 | Eval R2: -7.3188 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0406 | Eval Loss: 0.0370 | Eval R2: -4.6713 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0365 | Eval Loss: 0.0335 | Eval R2: -3.5459 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0338 | Eval Loss: 0.0315 | Eval R2: -3.0286 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0318 | Eval Loss: 0.0301 | Eval R2: -2.8447 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0306 | Eval Loss: 0.0291 | Eval R2: -2.7604 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0294 | Eval Loss: 0.0283 | Eval R2: -2.6898 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0284 | Eval Loss: 0.0276 | Eval R2: -2.5861 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0276 | Eval Loss: 0.0270 | Eval R2: -2.5631 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0271 | Eval Loss: 0.0265 | Eval R2: -2.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0264 | Eval Loss: 0.0260 | Eval R2: -2.5134 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0258 | Eval Loss: 0.0255 | Eval R2: -2.4373 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0253 | Eval Loss: 0.0251 | Eval R2: -2.3653 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0249 | Eval Loss: 0.0247 | Eval R2: -2.3199 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0243 | Eval Loss: 0.0244 | Eval R2: -2.2803 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0239 | Eval Loss: 0.0242 | Eval R2: -2.3354 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0236 | Eval Loss: 0.0240 | Eval R2: -2.3648 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0233 | Eval Loss: 0.0238 | Eval R2: -2.3137 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0231 | Eval Loss: 0.0235 | Eval R2: -2.3282 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021735, test R2 score: -2.168550
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02310175821185112, 'r2_eval_final': -2.328239917755127, 'loss_eval_final': 0.02352406457066536, 'r2_test': -2.168550051183605, 'loss_test': 0.02173549123108387, 'loss_nodes': [[0.01034513395279646, 0.014820292592048645, 0.012998241931200027, 0.018518369644880295, 0.018829427659511566, 0.016647713258862495, 0.019268568605184555, 0.021240735426545143, 0.023611489683389664, 0.022105509415268898, 0.021022344008088112, 0.021646859124302864, 0.02162214182317257, 0.02224665880203247, 0.025109993293881416, 0.027386581525206566, 0.028227152302861214, 0.02826693095266819, 0.03202541172504425, 0.02877020463347435]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0231
wandb: loss_eval 0.02352
wandb: loss_test 0.02174
wandb:   r2_eval -2.32824
wandb:   r2_test -2.16855
wandb: 
wandb: ðŸš€ View run sage-meadow-57 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/a0o9xylm
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240722_171033-a0o9xylm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [115:59:42<172:27:59, 28221.78s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240723_042120-i3i7ug2p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vibrant-music-58
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/i3i7ug2p

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3926 | Eval Loss: 0.7459 | Eval R2: -359.5201 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5886 | Eval Loss: 0.5779 | Eval R2: -292.2169 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4845 | Eval Loss: 0.5097 | Eval R2: -265.8276 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4212 | Eval Loss: 0.4282 | Eval R2: -215.8962 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3522 | Eval Loss: 0.3560 | Eval R2: -173.1185 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2925 | Eval Loss: 0.2890 | Eval R2: -133.2731 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2353 | Eval Loss: 0.2299 | Eval R2: -100.3704 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1863 | Eval Loss: 0.1793 | Eval R2: -70.1933 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1458 | Eval Loss: 0.1404 | Eval R2: -48.9823 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1157 | Eval Loss: 0.1120 | Eval R2: -34.8322 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0941 | Eval Loss: 0.0910 | Eval R2: -25.1623 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0780 | Eval Loss: 0.0751 | Eval R2: -18.2170 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0657 | Eval Loss: 0.0632 | Eval R2: -13.2290 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0566 | Eval Loss: 0.0546 | Eval R2: -9.8255 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0502 | Eval Loss: 0.0484 | Eval R2: -7.5611 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0457 | Eval Loss: 0.0440 | Eval R2: -6.0846 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0424 | Eval Loss: 0.0409 | Eval R2: -5.1482 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0400 | Eval Loss: 0.0387 | Eval R2: -4.5589 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0384 | Eval Loss: 0.0371 | Eval R2: -4.1521 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0371 | Eval Loss: 0.0358 | Eval R2: -3.8908 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0361 | Eval Loss: 0.0349 | Eval R2: -3.6957 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0354 | Eval Loss: 0.0342 | Eval R2: -3.5863 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0347 | Eval Loss: 0.0336 | Eval R2: -3.4858 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0340 | Eval Loss: 0.0330 | Eval R2: -3.3931 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0336 | Eval Loss: 0.0326 | Eval R2: -3.3255 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0331 | Eval Loss: 0.0322 | Eval R2: -3.2524 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0327 | Eval Loss: 0.0318 | Eval R2: -3.2227 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.028954, test R2 score: -2.531415
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.03268170729279518, 'r2_eval_final': -3.2227351665496826, 'loss_eval_final': 0.03182864189147949, 'r2_test': -2.5314153333965437, 'loss_test': 0.02895420230925083, 'loss_nodes': [[0.02435867115855217, 0.03420835733413696, 0.025863120332360268, 0.02368089184165001, 0.021265225484967232, 0.01966252364218235, 0.02062215842306614, 0.033890027552843094, 0.023503035306930542, 0.03947830572724342, 0.026429645717144012, 0.023150518536567688, 0.02452416718006134, 0.026656322181224823, 0.03009706176817417, 0.0268138125538826, 0.05470292642712593, 0.030928004533052444, 0.03187871351838112, 0.03737059235572815]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03268
wandb: loss_eval 0.03183
wandb: loss_test 0.02895
wandb:   r2_eval -3.22274
wandb:   r2_test -2.53142
wandb: 
wandb: ðŸš€ View run vibrant-music-58 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/i3i7ug2p
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240723_042120-i3i7ug2p/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [127:31:47<187:54:30, 32212.88s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240723_155325-jf5hiz3d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-serenity-59
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/jf5hiz3d

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5485 | Eval Loss: 0.5528 | Eval R2: -286.9584 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4597 | Eval Loss: 0.4780 | Eval R2: -248.5257 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3954 | Eval Loss: 0.4108 | Eval R2: -210.6690 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3425 | Eval Loss: 0.3557 | Eval R2: -177.6449 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2964 | Eval Loss: 0.3049 | Eval R2: -146.3566 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2556 | Eval Loss: 0.2603 | Eval R2: -118.6062 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2182 | Eval Loss: 0.2202 | Eval R2: -94.6600 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1843 | Eval Loss: 0.1847 | Eval R2: -74.2106 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1544 | Eval Loss: 0.1541 | Eval R2: -56.9377 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1287 | Eval Loss: 0.1279 | Eval R2: -42.7057 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1077 | Eval Loss: 0.1064 | Eval R2: -31.4213 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0909 | Eval Loss: 0.0899 | Eval R2: -23.0114 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0783 | Eval Loss: 0.0775 | Eval R2: -17.0957 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0691 | Eval Loss: 0.0685 | Eval R2: -13.2409 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0624 | Eval Loss: 0.0619 | Eval R2: -10.9621 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0570 | Eval Loss: 0.0568 | Eval R2: -9.5738 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0531 | Eval Loss: 0.0526 | Eval R2: -8.5129 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0499 | Eval Loss: 0.0485 | Eval R2: -6.8058 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0472 | Eval Loss: 0.0447 | Eval R2: -5.3899 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0448 | Eval Loss: 0.0420 | Eval R2: -4.6723 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0429 | Eval Loss: 0.0401 | Eval R2: -4.5656 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0410 | Eval Loss: 0.0381 | Eval R2: -4.2720 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0394 | Eval Loss: 0.0358 | Eval R2: -3.6896 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0373 | Eval Loss: 0.0342 | Eval R2: -3.3879 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0353 | Eval Loss: 0.0329 | Eval R2: -3.2774 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0340 | Eval Loss: 0.0316 | Eval R2: -3.1547 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0326 | Eval Loss: 0.0307 | Eval R2: -3.0218 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.026297, test R2 score: -2.202394
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.032602667808532715, 'r2_eval_final': -3.02184796333313, 'loss_eval_final': 0.030711473897099495, 'r2_test': -2.2023937730047543, 'loss_test': 0.02629663422703743, 'loss_nodes': [[0.016708986833691597, 0.024352656677365303, 0.02013040892779827, 0.015479521825909615, 0.039402689784765244, 0.02406451478600502, 0.021318210288882256, 0.021383844316005707, 0.01868954673409462, 0.021498890593647957, 0.02120053581893444, 0.02265515923500061, 0.02188841998577118, 0.02765662968158722, 0.022859204560518265, 0.029041975736618042, 0.048686157912015915, 0.029176628217101097, 0.028956228867173195, 0.0507824532687664]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0326
wandb: loss_eval 0.03071
wandb: loss_test 0.0263
wandb:   r2_eval -3.02185
wandb:   r2_test -2.20239
wandb: 
wandb: ðŸš€ View run young-serenity-59 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/jf5hiz3d
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240723_155325-jf5hiz3d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [127:44:01<126:29:38, 22768.90s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240723_160538-a5rk50r5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run genial-sun-60
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/a5rk50r5

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7637 | Eval Loss: 0.6701 | Eval R2: -317.3430 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5340 | Eval Loss: 0.4803 | Eval R2: -228.9170 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3840 | Eval Loss: 0.3483 | Eval R2: -163.7201 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2713 | Eval Loss: 0.2349 | Eval R2: -104.1325 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1775 | Eval Loss: 0.1416 | Eval R2: -51.4557 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1089 | Eval Loss: 0.0883 | Eval R2: -20.4105 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0755 | Eval Loss: 0.0681 | Eval R2: -10.1863 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0612 | Eval Loss: 0.0565 | Eval R2: -6.8454 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0522 | Eval Loss: 0.0490 | Eval R2: -6.1042 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0465 | Eval Loss: 0.0438 | Eval R2: -5.6676 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0423 | Eval Loss: 0.0400 | Eval R2: -5.2174 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0392 | Eval Loss: 0.0375 | Eval R2: -4.8358 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0369 | Eval Loss: 0.0350 | Eval R2: -4.2403 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0349 | Eval Loss: 0.0328 | Eval R2: -3.6071 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0332 | Eval Loss: 0.0312 | Eval R2: -3.0991 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0316 | Eval Loss: 0.0298 | Eval R2: -2.7922 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0302 | Eval Loss: 0.0290 | Eval R2: -2.6250 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0289 | Eval Loss: 0.0281 | Eval R2: -2.4861 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0281 | Eval Loss: 0.0277 | Eval R2: -2.3891 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0272 | Eval Loss: 0.0272 | Eval R2: -2.3129 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0266 | Eval Loss: 0.0268 | Eval R2: -2.2383 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0262 | Eval Loss: 0.0264 | Eval R2: -2.1568 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0257 | Eval Loss: 0.0260 | Eval R2: -2.0981 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0254 | Eval Loss: 0.0258 | Eval R2: -2.0594 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0250 | Eval Loss: 0.0253 | Eval R2: -2.0007 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0247 | Eval Loss: 0.0249 | Eval R2: -1.9564 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0243 | Eval Loss: 0.0246 | Eval R2: -1.9068 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.022481, test R2 score: -1.838697
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.024334006011486053, 'r2_eval_final': -1.9067635536193848, 'loss_eval_final': 0.024551071226596832, 'r2_test': -1.8386972085446154, 'loss_test': 0.022481068968772888, 'loss_nodes': [[0.011184918694198132, 0.01593499258160591, 0.015584243461489677, 0.017580170184373856, 0.018353881314396858, 0.01770743727684021, 0.02029106579720974, 0.021236738190054893, 0.0215890035033226, 0.025565894320607185, 0.026644032448530197, 0.0224858820438385, 0.025083471089601517, 0.023918600752949715, 0.02571556158363819, 0.02484259009361267, 0.029303256422281265, 0.02864973433315754, 0.03040606528520584, 0.027543844655156136]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02433
wandb: loss_eval 0.02455
wandb: loss_test 0.02248
wandb:   r2_eval -1.90676
wandb:   r2_test -1.8387
wandb: 
wandb: ðŸš€ View run genial-sun-60 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/a5rk50r5
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240723_160538-a5rk50r5/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [128:04:55<86:06:11, 16314.27s/it] Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240723_162632-2eg2bvyu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-sea-61
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/2eg2bvyu

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7059 | Eval Loss: 0.5891 | Eval R2: -315.4023 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4976 | Eval Loss: 0.4773 | Eval R2: -235.6528 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3863 | Eval Loss: 0.3741 | Eval R2: -188.0417 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3104 | Eval Loss: 0.3080 | Eval R2: -147.4864 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2574 | Eval Loss: 0.2558 | Eval R2: -120.5722 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2118 | Eval Loss: 0.2090 | Eval R2: -96.0723 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1722 | Eval Loss: 0.1680 | Eval R2: -73.1200 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1382 | Eval Loss: 0.1326 | Eval R2: -53.0988 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1095 | Eval Loss: 0.1036 | Eval R2: -37.2232 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0869 | Eval Loss: 0.0821 | Eval R2: -25.7889 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0703 | Eval Loss: 0.0659 | Eval R2: -17.3613 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0586 | Eval Loss: 0.0549 | Eval R2: -12.1490 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0508 | Eval Loss: 0.0482 | Eval R2: -9.0513 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0460 | Eval Loss: 0.0439 | Eval R2: -7.3777 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0428 | Eval Loss: 0.0413 | Eval R2: -6.5335 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0405 | Eval Loss: 0.0391 | Eval R2: -5.9556 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0384 | Eval Loss: 0.0376 | Eval R2: -5.7562 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0368 | Eval Loss: 0.0365 | Eval R2: -5.9139 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0357 | Eval Loss: 0.0358 | Eval R2: -5.8175 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0350 | Eval Loss: 0.0358 | Eval R2: -6.1807 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0341 | Eval Loss: 0.0346 | Eval R2: -5.8213 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0332 | Eval Loss: 0.0335 | Eval R2: -5.6505 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0324 | Eval Loss: 0.0319 | Eval R2: -4.8629 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0314 | Eval Loss: 0.0304 | Eval R2: -4.0844 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0304 | Eval Loss: 0.0294 | Eval R2: -3.5392 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0294 | Eval Loss: 0.0288 | Eval R2: -3.3543 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0287 | Eval Loss: 0.0284 | Eval R2: -3.1125 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.025609, test R2 score: -2.822595
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.02866539917886257, 'r2_eval_final': -3.112527847290039, 'loss_eval_final': 0.028383180499076843, 'r2_test': -2.822594833865101, 'loss_test': 0.025608856230974197, 'loss_nodes': [[0.022451216354966164, 0.01659867912530899, 0.02726278454065323, 0.0237167589366436, 0.016089826822280884, 0.02356984093785286, 0.017202937975525856, 0.0214296355843544, 0.028472445905208588, 0.021049590781331062, 0.02208450250327587, 0.030788587406277657, 0.022367138415575027, 0.026109518483281136, 0.0269555002450943, 0.03451486676931381, 0.02583066001534462, 0.03843199834227562, 0.02837926149368286, 0.038871414959430695]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02867
wandb: loss_eval 0.02838
wandb: loss_test 0.02561
wandb:   r2_eval -3.11253
wandb:   r2_test -2.82259
wandb: 
wandb: ðŸš€ View run daily-sea-61 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/2eg2bvyu
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240723_162632-2eg2bvyu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [143:17:49<139:15:43, 27852.39s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_073927-iptcq6hk
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-sound-62
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/iptcq6hk

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5901 | Eval Loss: 0.5567 | Eval R2: -275.5182 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4648 | Eval Loss: 0.4630 | Eval R2: -233.4434 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3888 | Eval Loss: 0.3989 | Eval R2: -201.7456 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3359 | Eval Loss: 0.3505 | Eval R2: -176.0914 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2952 | Eval Loss: 0.3093 | Eval R2: -152.5707 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2603 | Eval Loss: 0.2642 | Eval R2: -123.8370 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2247 | Eval Loss: 0.2210 | Eval R2: -96.5661 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1881 | Eval Loss: 0.1805 | Eval R2: -72.1001 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1536 | Eval Loss: 0.1442 | Eval R2: -51.4712 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1237 | Eval Loss: 0.1140 | Eval R2: -34.7799 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0994 | Eval Loss: 0.0913 | Eval R2: -22.9373 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0815 | Eval Loss: 0.0752 | Eval R2: -15.2309 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0689 | Eval Loss: 0.0640 | Eval R2: -10.5640 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0599 | Eval Loss: 0.0557 | Eval R2: -7.7541 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0534 | Eval Loss: 0.0492 | Eval R2: -6.0911 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0481 | Eval Loss: 0.0443 | Eval R2: -5.1267 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0440 | Eval Loss: 0.0407 | Eval R2: -4.5249 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0405 | Eval Loss: 0.0377 | Eval R2: -4.0303 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0381 | Eval Loss: 0.0356 | Eval R2: -3.6707 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0363 | Eval Loss: 0.0343 | Eval R2: -3.4814 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0349 | Eval Loss: 0.0333 | Eval R2: -3.4031 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0339 | Eval Loss: 0.0323 | Eval R2: -3.1653 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0329 | Eval Loss: 0.0317 | Eval R2: -3.0352 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0320 | Eval Loss: 0.0311 | Eval R2: -2.9622 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0311 | Eval Loss: 0.0302 | Eval R2: -2.7872 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0304 | Eval Loss: 0.0299 | Eval R2: -2.7469 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0296 | Eval Loss: 0.0292 | Eval R2: -2.6456 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.026060, test R2 score: -1.985984
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.029585599899291992, 'r2_eval_final': -2.6455605030059814, 'loss_eval_final': 0.02923102304339409, 'r2_test': -1.985983764285082, 'loss_test': 0.02605963684618473, 'loss_nodes': [[0.015324161387979984, 0.020004939287900925, 0.01928715966641903, 0.018079765141010284, 0.022049764171242714, 0.02042791247367859, 0.020780399441719055, 0.026649001985788345, 0.02527480386197567, 0.02290101908147335, 0.027059081941843033, 0.03494635596871376, 0.03042587637901306, 0.024426916614174843, 0.032683659344911575, 0.025832485407590866, 0.038024723529815674, 0.03338044509291649, 0.030360151082277298, 0.03327413648366928]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02959
wandb: loss_eval 0.02923
wandb: loss_test 0.02606
wandb:   r2_eval -2.64556
wandb:   r2_test -1.98598
wandb: 
wandb: ðŸš€ View run peach-sound-62 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/iptcq6hk
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_073927-iptcq6hk/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [143:34:06<93:27:03, 19789.62s/it] Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_075543-xtwggbv0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run whole-capybara-63
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/xtwggbv0

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6031 | Eval Loss: 0.5248 | Eval R2: -262.7061 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3964 | Eval Loss: 0.3398 | Eval R2: -166.4747 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2556 | Eval Loss: 0.2104 | Eval R2: -95.0095 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1579 | Eval Loss: 0.1247 | Eval R2: -45.0466 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0988 | Eval Loss: 0.0809 | Eval R2: -18.9586 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0722 | Eval Loss: 0.0632 | Eval R2: -8.8289 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0614 | Eval Loss: 0.0542 | Eval R2: -6.0299 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0543 | Eval Loss: 0.0475 | Eval R2: -5.4051 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0479 | Eval Loss: 0.0419 | Eval R2: -4.7956 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0429 | Eval Loss: 0.0375 | Eval R2: -4.1420 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0391 | Eval Loss: 0.0342 | Eval R2: -3.5770 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0360 | Eval Loss: 0.0317 | Eval R2: -3.1300 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0333 | Eval Loss: 0.0297 | Eval R2: -2.8100 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0311 | Eval Loss: 0.0279 | Eval R2: -2.5908 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0294 | Eval Loss: 0.0268 | Eval R2: -2.4074 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0281 | Eval Loss: 0.0258 | Eval R2: -2.2301 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0269 | Eval Loss: 0.0250 | Eval R2: -2.1382 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0261 | Eval Loss: 0.0245 | Eval R2: -2.0654 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0253 | Eval Loss: 0.0241 | Eval R2: -2.0078 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0246 | Eval Loss: 0.0236 | Eval R2: -1.9624 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0238 | Eval Loss: 0.0232 | Eval R2: -1.9118 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0235 | Eval Loss: 0.0230 | Eval R2: -1.8955 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0230 | Eval Loss: 0.0230 | Eval R2: -1.9545 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0227 | Eval Loss: 0.0231 | Eval R2: -2.0325 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0223 | Eval Loss: 0.0230 | Eval R2: -2.0708 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0221 | Eval Loss: 0.0231 | Eval R2: -2.2206 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0218 | Eval Loss: 0.0234 | Eval R2: -2.2504 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020715, test R2 score: -2.256804
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02177548222243786, 'r2_eval_final': -2.250430107116699, 'loss_eval_final': 0.023352203890681267, 'r2_test': -2.2568037993845094, 'loss_test': 0.020714759826660156, 'loss_nodes': [[0.009541656821966171, 0.011226315051317215, 0.015874097123742104, 0.014788800850510597, 0.016316022723913193, 0.01591096818447113, 0.017741171643137932, 0.020566513761878014, 0.01965503953397274, 0.020439624786376953, 0.02010190859436989, 0.020600471645593643, 0.02258363738656044, 0.022902488708496094, 0.026161497458815575, 0.025985222309827805, 0.02740887552499771, 0.028695570304989815, 0.028110843151807785, 0.02968449890613556]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02178
wandb: loss_eval 0.02335
wandb: loss_test 0.02071
wandb:   r2_eval -2.25043
wandb:   r2_test -2.2568
wandb: 
wandb: ðŸš€ View run whole-capybara-63 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/xtwggbv0
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_075543-xtwggbv0/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [143:59:26<63:35:41, 14308.84s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_082104-k1wl0294
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vital-blaze-64
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/k1wl0294

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.3990 | Eval Loss: 0.3604 | Eval R2: -181.4873 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2826 | Eval Loss: 0.2711 | Eval R2: -134.3394 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2171 | Eval Loss: 0.2146 | Eval R2: -100.5849 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1802 | Eval Loss: 0.1837 | Eval R2: -77.8066 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1630 | Eval Loss: 0.1696 | Eval R2: -64.2035 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1569 | Eval Loss: 0.1645 | Eval R2: -57.5380 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1552 | Eval Loss: 0.1629 | Eval R2: -55.1687 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1546 | Eval Loss: 0.1625 | Eval R2: -54.8663 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1543 | Eval Loss: 0.1625 | Eval R2: -55.2321 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6017 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7840 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8126 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7760 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7363 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7133 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1540 | Eval Loss: 0.1625 | Eval R2: -55.7030 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6975 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6926 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6874 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6823 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6770 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6718 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6667 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6619 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6575 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6534 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6494 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121123, test R2 score: -43.344965
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.15409402549266815, 'r2_eval_final': -55.64940643310547, 'loss_eval_final': 0.16253405809402466, 'r2_test': -43.34496461694144, 'loss_test': 0.12112276256084442, 'loss_nodes': [[0.11829982697963715, 0.12032046914100647, 0.11829836666584015, 0.12113984674215317, 0.12057189643383026, 0.11906023323535919, 0.12270093709230423, 0.11888550966978073, 0.12161321192979813, 0.11873836815357208, 0.12021375447511673, 0.12306991219520569, 0.11966392397880554, 0.12314219027757645, 0.11990226060152054, 0.12138199806213379, 0.12442529201507568, 0.12209079414606094, 0.12596818804740906, 0.12296821922063828]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15409
wandb: loss_eval 0.16253
wandb: loss_test 0.12112
wandb:   r2_eval -55.64941
wandb:   r2_test -43.34496
wandb: 
wandb: ðŸš€ View run vital-blaze-64 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/k1wl0294
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_082104-k1wl0294/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [144:17:49<43:06:46, 10347.12s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_083927-uggic6rb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-valley-65
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/uggic6rb

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6020 | Eval Loss: 0.5009 | Eval R2: -236.0820 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3763 | Eval Loss: 0.3544 | Eval R2: -181.4446 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2802 | Eval Loss: 0.2664 | Eval R2: -124.0412 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2135 | Eval Loss: 0.2019 | Eval R2: -85.0074 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1626 | Eval Loss: 0.1544 | Eval R2: -58.8441 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1265 | Eval Loss: 0.1198 | Eval R2: -37.2872 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0989 | Eval Loss: 0.0953 | Eval R2: -24.4402 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0809 | Eval Loss: 0.0799 | Eval R2: -16.5183 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0696 | Eval Loss: 0.0704 | Eval R2: -11.9218 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0630 | Eval Loss: 0.0643 | Eval R2: -9.7829 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0585 | Eval Loss: 0.0597 | Eval R2: -8.3736 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0550 | Eval Loss: 0.0559 | Eval R2: -7.6146 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0521 | Eval Loss: 0.0524 | Eval R2: -7.0104 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0493 | Eval Loss: 0.0491 | Eval R2: -6.6301 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0467 | Eval Loss: 0.0460 | Eval R2: -6.3225 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0441 | Eval Loss: 0.0431 | Eval R2: -6.1359 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0417 | Eval Loss: 0.0405 | Eval R2: -6.1914 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0394 | Eval Loss: 0.0387 | Eval R2: -6.4642 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0374 | Eval Loss: 0.0381 | Eval R2: -7.1202 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0382 | Eval R2: -7.5570 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0342 | Eval Loss: 0.0383 | Eval R2: -7.7715 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0333 | Eval Loss: 0.0378 | Eval R2: -7.5299 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0322 | Eval Loss: 0.0372 | Eval R2: -6.7688 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0310 | Eval Loss: 0.0364 | Eval R2: -5.9444 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0301 | Eval Loss: 0.0356 | Eval R2: -5.4067 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0291 | Eval Loss: 0.0349 | Eval R2: -5.0882 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0284 | Eval Loss: 0.0346 | Eval R2: -4.9198 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.030864, test R2 score: -3.171587
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.028353003785014153, 'r2_eval_final': -4.919759273529053, 'loss_eval_final': 0.03456522896885872, 'r2_test': -3.1715874036991907, 'loss_test': 0.030864080414175987, 'loss_nodes': [[0.012773529626429081, 0.024868059903383255, 0.015408312901854515, 0.01681605353951454, 0.06806385517120361, 0.02215931937098503, 0.03230952471494675, 0.03479593247175217, 0.021007582545280457, 0.02620639279484749, 0.03227386251091957, 0.035604171454906464, 0.04549826681613922, 0.027170099318027496, 0.029605844989418983, 0.04649478197097778, 0.04021649807691574, 0.02710053138434887, 0.02932910993695259, 0.029579810798168182]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02835
wandb: loss_eval 0.03457
wandb: loss_test 0.03086
wandb:   r2_eval -4.91976
wandb:   r2_test -3.17159
wandb: 
wandb: ðŸš€ View run earnest-valley-65 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/uggic6rb
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_083927-uggic6rb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [144:37:36<29:33:07, 7599.07s/it] Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_085914-22uyk0cb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run confused-jazz-66
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/22uyk0cb

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6031 | Eval Loss: 0.5248 | Eval R2: -262.7061 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3964 | Eval Loss: 0.3398 | Eval R2: -166.4747 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2556 | Eval Loss: 0.2104 | Eval R2: -95.0095 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1579 | Eval Loss: 0.1247 | Eval R2: -45.0466 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0988 | Eval Loss: 0.0809 | Eval R2: -18.9586 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0722 | Eval Loss: 0.0632 | Eval R2: -8.8289 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0614 | Eval Loss: 0.0542 | Eval R2: -6.0299 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0543 | Eval Loss: 0.0475 | Eval R2: -5.4051 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0479 | Eval Loss: 0.0419 | Eval R2: -4.7956 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0429 | Eval Loss: 0.0375 | Eval R2: -4.1420 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0391 | Eval Loss: 0.0342 | Eval R2: -3.5770 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0360 | Eval Loss: 0.0317 | Eval R2: -3.1300 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0333 | Eval Loss: 0.0297 | Eval R2: -2.8100 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0311 | Eval Loss: 0.0279 | Eval R2: -2.5908 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0294 | Eval Loss: 0.0268 | Eval R2: -2.4074 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0281 | Eval Loss: 0.0258 | Eval R2: -2.2301 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0269 | Eval Loss: 0.0250 | Eval R2: -2.1382 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0261 | Eval Loss: 0.0245 | Eval R2: -2.0654 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0253 | Eval Loss: 0.0241 | Eval R2: -2.0078 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0246 | Eval Loss: 0.0236 | Eval R2: -1.9624 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0238 | Eval Loss: 0.0232 | Eval R2: -1.9118 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0235 | Eval Loss: 0.0230 | Eval R2: -1.8955 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0230 | Eval Loss: 0.0230 | Eval R2: -1.9545 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0227 | Eval Loss: 0.0231 | Eval R2: -2.0325 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0223 | Eval Loss: 0.0230 | Eval R2: -2.0708 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0221 | Eval Loss: 0.0231 | Eval R2: -2.2206 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0218 | Eval Loss: 0.0234 | Eval R2: -2.2504 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020715, test R2 score: -2.256804
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02177548222243786, 'r2_eval_final': -2.250430107116699, 'loss_eval_final': 0.023352203890681267, 'r2_test': -2.2568037993845094, 'loss_test': 0.020714759826660156, 'loss_nodes': [[0.009541656821966171, 0.011226315051317215, 0.015874097123742104, 0.014788800850510597, 0.016316022723913193, 0.01591096818447113, 0.017741171643137932, 0.020566513761878014, 0.01965503953397274, 0.020439624786376953, 0.02010190859436989, 0.020600471645593643, 0.02258363738656044, 0.022902488708496094, 0.026161497458815575, 0.025985222309827805, 0.02740887552499771, 0.028695570304989815, 0.028110843151807785, 0.02968449890613556]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.002 MB of 0.006 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02178
wandb: loss_eval 0.02335
wandb: loss_test 0.02071
wandb:   r2_eval -2.25043
wandb:   r2_test -2.2568
wandb: 
wandb: ðŸš€ View run confused-jazz-66 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/22uyk0cb
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_085914-22uyk0cb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [145:02:58<20:51:27, 5775.93s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_092436-5nkiei54
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-wildflower-67
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/5nkiei54

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8456 | Eval Loss: 0.7891 | Eval R2: -391.8618 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6623 | Eval Loss: 0.6863 | Eval R2: -346.7439 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5731 | Eval Loss: 0.5967 | Eval R2: -304.6748 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4948 | Eval Loss: 0.5136 | Eval R2: -262.7293 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4227 | Eval Loss: 0.4355 | Eval R2: -220.5490 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3574 | Eval Loss: 0.3653 | Eval R2: -180.3570 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3011 | Eval Loss: 0.3060 | Eval R2: -144.9742 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2553 | Eval Loss: 0.2589 | Eval R2: -116.3549 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2200 | Eval Loss: 0.2238 | Eval R2: -94.9898 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1944 | Eval Loss: 0.1994 | Eval R2: -80.1069 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1772 | Eval Loss: 0.1836 | Eval R2: -70.2897 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1665 | Eval Loss: 0.1740 | Eval R2: -64.0779 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1604 | Eval Loss: 0.1685 | Eval R2: -60.3010 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1571 | Eval Loss: 0.1655 | Eval R2: -58.1231 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1554 | Eval Loss: 0.1639 | Eval R2: -56.9548 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1545 | Eval Loss: 0.1632 | Eval R2: -56.3781 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1542 | Eval Loss: 0.1629 | Eval R2: -56.1125 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1540 | Eval Loss: 0.1627 | Eval R2: -55.9907 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1540 | Eval Loss: 0.1627 | Eval R2: -55.9278 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1539 | Eval Loss: 0.1626 | Eval R2: -55.8892 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1539 | Eval Loss: 0.1626 | Eval R2: -55.8632 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8456 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8340 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8262 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8202 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8149 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8099 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121037, test R2 score: -43.482063
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.15397050976753235, 'r2_eval_final': -55.809906005859375, 'loss_eval_final': 0.16258804500102997, 'r2_test': -43.48206306364732, 'loss_test': 0.12103690952062607, 'loss_nodes': [[0.118257075548172, 0.12019184231758118, 0.11839974671602249, 0.12108329683542252, 0.12024587392807007, 0.11900562793016434, 0.12283622473478317, 0.11872217059135437, 0.12147502601146698, 0.11863381415605545, 0.11987369507551193, 0.12307041883468628, 0.11947842687368393, 0.12297122180461884, 0.11982066929340363, 0.12157928198575974, 0.12444840371608734, 0.12173856794834137, 0.12587204575538635, 0.12303481251001358]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15397
wandb: loss_eval 0.16259
wandb: loss_test 0.12104
wandb:   r2_eval -55.80991
wandb:   r2_test -43.48206
wandb: 
wandb: ðŸš€ View run zesty-wildflower-67 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/5nkiei54
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_092436-5nkiei54/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [145:21:18<14:34:36, 4373.07s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_094256-ahijoelm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-star-68
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ahijoelm

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8456 | Eval Loss: 0.7891 | Eval R2: -391.8618 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6623 | Eval Loss: 0.6863 | Eval R2: -346.7439 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5731 | Eval Loss: 0.5967 | Eval R2: -304.6748 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4948 | Eval Loss: 0.5136 | Eval R2: -262.7293 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4227 | Eval Loss: 0.4355 | Eval R2: -220.5490 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3574 | Eval Loss: 0.3653 | Eval R2: -180.3570 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3011 | Eval Loss: 0.3060 | Eval R2: -144.9742 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2553 | Eval Loss: 0.2589 | Eval R2: -116.3549 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2200 | Eval Loss: 0.2238 | Eval R2: -94.9898 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1944 | Eval Loss: 0.1994 | Eval R2: -80.1069 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1772 | Eval Loss: 0.1836 | Eval R2: -70.2897 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1665 | Eval Loss: 0.1740 | Eval R2: -64.0779 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1604 | Eval Loss: 0.1685 | Eval R2: -60.3010 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1571 | Eval Loss: 0.1655 | Eval R2: -58.1231 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1554 | Eval Loss: 0.1639 | Eval R2: -56.9548 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1545 | Eval Loss: 0.1632 | Eval R2: -56.3781 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1542 | Eval Loss: 0.1629 | Eval R2: -56.1125 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1540 | Eval Loss: 0.1627 | Eval R2: -55.9907 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1540 | Eval Loss: 0.1627 | Eval R2: -55.9278 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1539 | Eval Loss: 0.1626 | Eval R2: -55.8892 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1539 | Eval Loss: 0.1626 | Eval R2: -55.8632 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8456 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8340 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8262 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8202 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8149 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8099 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121037, test R2 score: -43.482063
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.15397050976753235, 'r2_eval_final': -55.809906005859375, 'loss_eval_final': 0.16258804500102997, 'r2_test': -43.48206306364732, 'loss_test': 0.12103690952062607, 'loss_nodes': [[0.118257075548172, 0.12019184231758118, 0.11839974671602249, 0.12108329683542252, 0.12024587392807007, 0.11900562793016434, 0.12283622473478317, 0.11872217059135437, 0.12147502601146698, 0.11863381415605545, 0.11987369507551193, 0.12307041883468628, 0.11947842687368393, 0.12297122180461884, 0.11982066929340363, 0.12157928198575974, 0.12444840371608734, 0.12173856794834137, 0.12587204575538635, 0.12303481251001358]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15397
wandb: loss_eval 0.16259
wandb: loss_test 0.12104
wandb:   r2_eval -55.80991
wandb:   r2_test -43.48206
wandb: 
wandb: ðŸš€ View run zesty-star-68 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ahijoelm
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_094256-ahijoelm/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [145:39:42<10:21:55, 3392.27s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_100119-58qni0zb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run atomic-shape-69
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/58qni0zb

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.3990 | Eval Loss: 0.3604 | Eval R2: -181.4873 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2826 | Eval Loss: 0.2711 | Eval R2: -134.3394 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2171 | Eval Loss: 0.2146 | Eval R2: -100.5849 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1802 | Eval Loss: 0.1837 | Eval R2: -77.8066 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1630 | Eval Loss: 0.1696 | Eval R2: -64.2035 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1569 | Eval Loss: 0.1645 | Eval R2: -57.5380 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1552 | Eval Loss: 0.1629 | Eval R2: -55.1687 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1546 | Eval Loss: 0.1625 | Eval R2: -54.8663 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1543 | Eval Loss: 0.1625 | Eval R2: -55.2321 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6017 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7840 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.8126 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7760 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7363 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1540 | Eval Loss: 0.1626 | Eval R2: -55.7133 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1540 | Eval Loss: 0.1625 | Eval R2: -55.7030 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6975 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6926 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6874 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6823 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6770 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6718 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6667 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6619 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6575 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6534 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1541 | Eval Loss: 0.1625 | Eval R2: -55.6494 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.121123, test R2 score: -43.344965
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.15409402549266815, 'r2_eval_final': -55.64940643310547, 'loss_eval_final': 0.16253405809402466, 'r2_test': -43.34496461694144, 'loss_test': 0.12112276256084442, 'loss_nodes': [[0.11829982697963715, 0.12032046914100647, 0.11829836666584015, 0.12113984674215317, 0.12057189643383026, 0.11906023323535919, 0.12270093709230423, 0.11888550966978073, 0.12161321192979813, 0.11873836815357208, 0.12021375447511673, 0.12306991219520569, 0.11966392397880554, 0.12314219027757645, 0.11990226060152054, 0.12138199806213379, 0.12442529201507568, 0.12209079414606094, 0.12596818804740906, 0.12296821922063828]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15409
wandb: loss_eval 0.16253
wandb: loss_test 0.12112
wandb:   r2_eval -55.64941
wandb:   r2_test -43.34496
wandb: 
wandb: ðŸš€ View run atomic-shape-69 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/58qni0zb
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_100119-58qni0zb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [145:58:00<7:30:39, 2703.94s/it] Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_101937-xqn1ewwj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run effortless-wave-70
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/xqn1ewwj

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9011 | Eval Loss: 0.6356 | Eval R2: -327.2025 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5182 | Eval Loss: 0.5516 | Eval R2: -291.4135 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4694 | Eval Loss: 0.5201 | Eval R2: -280.9026 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4488 | Eval Loss: 0.4998 | Eval R2: -270.9287 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4314 | Eval Loss: 0.4821 | Eval R2: -260.5455 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4142 | Eval Loss: 0.4633 | Eval R2: -250.8914 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3971 | Eval Loss: 0.4428 | Eval R2: -238.4463 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3790 | Eval Loss: 0.4212 | Eval R2: -226.4512 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3600 | Eval Loss: 0.3980 | Eval R2: -212.5002 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3397 | Eval Loss: 0.3740 | Eval R2: -198.5145 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3186 | Eval Loss: 0.3490 | Eval R2: -183.7760 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2968 | Eval Loss: 0.3234 | Eval R2: -168.9507 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2747 | Eval Loss: 0.2978 | Eval R2: -154.2344 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.2529 | Eval Loss: 0.2725 | Eval R2: -139.8923 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2317 | Eval Loss: 0.2482 | Eval R2: -126.1379 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2115 | Eval Loss: 0.2250 | Eval R2: -113.0703 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1924 | Eval Loss: 0.2037 | Eval R2: -100.8708 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1748 | Eval Loss: 0.1839 | Eval R2: -89.4955 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1581 | Eval Loss: 0.1657 | Eval R2: -79.0073 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1428 | Eval Loss: 0.1488 | Eval R2: -69.2889 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1286 | Eval Loss: 0.1332 | Eval R2: -60.3590 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1153 | Eval Loss: 0.1188 | Eval R2: -52.1624 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1033 | Eval Loss: 0.1057 | Eval R2: -44.7800 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0924 | Eval Loss: 0.0940 | Eval R2: -38.0463 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0827 | Eval Loss: 0.0834 | Eval R2: -32.1291 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0738 | Eval Loss: 0.0741 | Eval R2: -26.8465 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0664 | Eval Loss: 0.0662 | Eval R2: -22.3667 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.050542, test R2 score: -18.350233
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.06639694422483444, 'r2_eval_final': -22.36670684814453, 'loss_eval_final': 0.06617758423089981, 'r2_test': -18.350232869242564, 'loss_test': 0.05054231733083725, 'loss_nodes': [[0.0575648657977581, 0.01828470267355442, 0.016672324389219284, 0.03035132959485054, 0.027186142280697823, 0.044308844953775406, 0.0321664921939373, 0.15961520373821259, 0.023936066776514053, 0.026012448593974113, 0.128427192568779, 0.04421913996338844, 0.03058609925210476, 0.046831898391246796, 0.12244521081447601, 0.030062580481171608, 0.054867032915353775, 0.05127859488129616, 0.03415274992585182, 0.03187747672200203]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0664
wandb: loss_eval 0.06618
wandb: loss_test 0.05054
wandb:   r2_eval -22.36671
wandb:   r2_test -18.35023
wandb: 
wandb: ðŸš€ View run effortless-wave-70 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/xqn1ewwj
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_101937-xqn1ewwj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [157:31:09<35:55:25, 14369.54s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_215246-ktjxt421
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run unique-wildflower-71
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ktjxt421

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8192 | Eval Loss: 0.7342 | Eval R2: -374.5409 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6247 | Eval Loss: 0.6679 | Eval R2: -345.8094 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5700 | Eval Loss: 0.6205 | Eval R2: -324.1516 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5321 | Eval Loss: 0.5871 | Eval R2: -308.5658 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5043 | Eval Loss: 0.5608 | Eval R2: -296.4342 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4816 | Eval Loss: 0.5385 | Eval R2: -286.1534 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4620 | Eval Loss: 0.5185 | Eval R2: -276.7031 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4440 | Eval Loss: 0.4993 | Eval R2: -267.3043 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4264 | Eval Loss: 0.4796 | Eval R2: -257.3845 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4082 | Eval Loss: 0.4585 | Eval R2: -246.4866 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3884 | Eval Loss: 0.4352 | Eval R2: -234.2126 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3668 | Eval Loss: 0.4094 | Eval R2: -220.2589 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3432 | Eval Loss: 0.3811 | Eval R2: -204.4937 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3179 | Eval Loss: 0.3509 | Eval R2: -187.0419 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2917 | Eval Loss: 0.3200 | Eval R2: -168.3566 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2660 | Eval Loss: 0.2900 | Eval R2: -149.2367 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2423 | Eval Loss: 0.2625 | Eval R2: -130.7305 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2217 | Eval Loss: 0.2390 | Eval R2: -113.9081 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2050 | Eval Loss: 0.2201 | Eval R2: -99.5829 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1921 | Eval Loss: 0.2055 | Eval R2: -88.1211 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1824 | Eval Loss: 0.1945 | Eval R2: -79.4277 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1752 | Eval Loss: 0.1864 | Eval R2: -73.0869 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1698 | Eval Loss: 0.1803 | Eval R2: -68.5569 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1657 | Eval Loss: 0.1758 | Eval R2: -65.3250 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1626 | Eval Loss: 0.1725 | Eval R2: -62.9873 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1603 | Eval Loss: 0.1699 | Eval R2: -61.2619 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1586 | Eval Loss: 0.1681 | Eval R2: -59.9665 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.124140, test R2 score: -47.070356
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.15855349600315094, 'r2_eval_final': -59.96648406982422, 'loss_eval_final': 0.16805115342140198, 'r2_test': -47.070355824556394, 'loss_test': 0.12413965165615082, 'loss_nodes': [[0.12547335028648376, 0.12585797905921936, 0.12048632651567459, 0.12017330527305603, 0.12677909433841705, 0.11666183173656464, 0.1263538897037506, 0.11827311664819717, 0.1330353021621704, 0.11554308980703354, 0.11997106671333313, 0.12365630269050598, 0.11948968470096588, 0.12285833805799484, 0.12023873627185822, 0.1554553359746933, 0.12220527231693268, 0.12315169721841812, 0.12416932731866837, 0.12295995652675629]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.002 MB of 0.003 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15855
wandb: loss_eval 0.16805
wandb: loss_test 0.12414
wandb:   r2_eval -59.96648
wandb:   r2_test -47.07036
wandb: 
wandb: ðŸš€ View run unique-wildflower-71 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/ktjxt421
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_215246-ktjxt421/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [157:51:09<23:09:09, 10418.63s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_221246-8avtwt2w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-sky-72
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/8avtwt2w

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8192 | Eval Loss: 0.7342 | Eval R2: -374.5409 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6247 | Eval Loss: 0.6679 | Eval R2: -345.8094 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5700 | Eval Loss: 0.6205 | Eval R2: -324.1516 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5321 | Eval Loss: 0.5871 | Eval R2: -308.5658 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5043 | Eval Loss: 0.5608 | Eval R2: -296.4342 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4816 | Eval Loss: 0.5385 | Eval R2: -286.1534 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4620 | Eval Loss: 0.5185 | Eval R2: -276.7031 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4440 | Eval Loss: 0.4993 | Eval R2: -267.3043 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4264 | Eval Loss: 0.4796 | Eval R2: -257.3845 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4082 | Eval Loss: 0.4585 | Eval R2: -246.4866 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3884 | Eval Loss: 0.4352 | Eval R2: -234.2126 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3668 | Eval Loss: 0.4094 | Eval R2: -220.2589 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3432 | Eval Loss: 0.3811 | Eval R2: -204.4937 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3179 | Eval Loss: 0.3509 | Eval R2: -187.0419 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2917 | Eval Loss: 0.3200 | Eval R2: -168.3566 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2660 | Eval Loss: 0.2900 | Eval R2: -149.2367 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2423 | Eval Loss: 0.2625 | Eval R2: -130.7305 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2217 | Eval Loss: 0.2390 | Eval R2: -113.9081 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2050 | Eval Loss: 0.2201 | Eval R2: -99.5829 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1921 | Eval Loss: 0.2055 | Eval R2: -88.1211 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1824 | Eval Loss: 0.1945 | Eval R2: -79.4277 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1752 | Eval Loss: 0.1864 | Eval R2: -73.0869 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1698 | Eval Loss: 0.1803 | Eval R2: -68.5569 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1657 | Eval Loss: 0.1758 | Eval R2: -65.3250 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1626 | Eval Loss: 0.1725 | Eval R2: -62.9873 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1603 | Eval Loss: 0.1699 | Eval R2: -61.2619 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1586 | Eval Loss: 0.1681 | Eval R2: -59.9665 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.124140, test R2 score: -47.070356
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.15855349600315094, 'r2_eval_final': -59.96648406982422, 'loss_eval_final': 0.16805115342140198, 'r2_test': -47.070355824556394, 'loss_test': 0.12413965165615082, 'loss_nodes': [[0.12547335028648376, 0.12585797905921936, 0.12048632651567459, 0.12017330527305603, 0.12677909433841705, 0.11666183173656464, 0.1263538897037506, 0.11827311664819717, 0.1330353021621704, 0.11554308980703354, 0.11997106671333313, 0.12365630269050598, 0.11948968470096588, 0.12285833805799484, 0.12023873627185822, 0.1554553359746933, 0.12220527231693268, 0.12315169721841812, 0.12416932731866837, 0.12295995652675629]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.15855
wandb: loss_eval 0.16805
wandb: loss_test 0.12414
wandb:   r2_eval -59.96648
wandb:   r2_test -47.07036
wandb: 
wandb: ðŸš€ View run toasty-sky-72 at: https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/8avtwt2w
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_221246-8avtwt2w/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [158:11:33<14:53:41, 7660.19s/it] Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_223310-54n9anxa
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vivid-glitter-73
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_bus_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_bus_fault/runs/54n9anxa

==================== DATASET INFO ===================

Train dataset: 2025
Validation dataset: 435
Test dataset: 430

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3926 | Eval Loss: 0.7459 | Eval R2: -359.5201 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5886 | Eval Loss: 0.5779 | Eval R2: -292.2169 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4845 | Eval Loss: 0.5097 | Eval R2: -265.8276 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4212 | Eval Loss: 0.4282 | Eval R2: -215.8962 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3522 | Eval Loss: 0.3560 | Eval R2: -173.1185 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2925 | Eval Loss: 0.2890 | Eval R2: -133.2731 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2353 | Eval Loss: 0.2299 | Eval R2: -100.3704 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1863 | Eval Loss: 0.1793 | Eval R2: -70.1933 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1458 | Eval Loss: 0.1404 | Eval R2: -48.9823 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1157 | Eval Loss: 0.1120 | Eval R2: -34.8322 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0941 | Eval Loss: 0.0910 | Eval R2: -25.1623 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0780 | Eval Loss: 0.0751 | Eval R2: -18.2170 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0657 | Eval Loss: 0.0632 | Eval R2: -13.2290 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0566 | Eval Loss: 0.0546 | Eval R2: -9.8255 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0502 | Eval Loss: 0.0484 | Eval R2: -7.5611 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0457 | Eval Loss: 0.0440 | Eval R2: -6.0846 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0424 | Eval Loss: 0.0409 | Eval R2: -5.1482 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0400 | Eval Loss: 0.0387 | Eval R2: -4.5589 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0384 | Eval Loss: 0.0371 | Eval R2: -4.1521 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0371 | Eval Loss: 0.0358 | Eval R2: -3.8908 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0361 | Eval Loss: 0.0349 | Eval R2: -3.6957 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0354 | Eval Loss: 0.0342 | Eval R2: -3.5863 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0347 | Eval Loss: 0.0336 | Eval R2: -3.4858 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0340 | Eval Loss: 0.0330 | Eval R2: -3.3931 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0336 | Eval Loss: 0.0326 | Eval R2: -3.3255 | LR: 0.0010 | 
