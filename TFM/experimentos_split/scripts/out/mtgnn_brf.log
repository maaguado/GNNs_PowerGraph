Processing dataset...
Node:  0  not included, including...
Node:  1  not included, including...
Node:  2  not included, including...
Node:  3  not included, including...
Node:  4  not included, including...
Node:  5  not included, including...
Node:  6  not included, including...
Node:  7  not included, including...
Node:  8  not included, including...
Node:  9  not included, including...
Node:  10  not included, including...
Node:  11  not included, including...
Node:  12  not included, including...
Node:  13  not included, including...
Node:  14  not included, including...
Node:  15  not included, including...
Node:  16  not included, including...
Node:  17  not included, including...
Node:  18  not included, including...
Node:  19  not included, including...
Node:  20  not included, including...
Node:  21  not included, including...
Node:  22  not included, including...
Skipping  row_328
Ajustando modelo para branch_fault...
Number of situations:  549
Number of timestamps:  800
Number of situations of the selected type:  92
  0%|          | 0/50 [00:00<?, ?it/s]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: Currently logged in as: maragumar01. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_090244-p7pqqiow
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run frosty-plant-27
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/p7pqqiow

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1195 | Eval Loss: 0.8741 | Eval R2: -86.2239 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7157 | Eval Loss: 0.7889 | Eval R2: -79.3022 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6421 | Eval Loss: 0.7178 | Eval R2: -73.5326 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5815 | Eval Loss: 0.6597 | Eval R2: -68.7048 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5319 | Eval Loss: 0.6108 | Eval R2: -64.5177 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4902 | Eval Loss: 0.5686 | Eval R2: -60.7601 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4544 | Eval Loss: 0.5315 | Eval R2: -57.3233 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4231 | Eval Loss: 0.4986 | Eval R2: -54.1592 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3958 | Eval Loss: 0.4692 | Eval R2: -51.2494 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3716 | Eval Loss: 0.4429 | Eval R2: -48.5805 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3499 | Eval Loss: 0.4190 | Eval R2: -46.1356 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3300 | Eval Loss: 0.3966 | Eval R2: -43.8857 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3110 | Eval Loss: 0.3749 | Eval R2: -41.8272 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.2925 | Eval Loss: 0.3538 | Eval R2: -39.8315 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2750 | Eval Loss: 0.3329 | Eval R2: -37.5560 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2582 | Eval Loss: 0.3130 | Eval R2: -35.4450 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2423 | Eval Loss: 0.2941 | Eval R2: -33.4532 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2273 | Eval Loss: 0.2758 | Eval R2: -31.4570 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2130 | Eval Loss: 0.2585 | Eval R2: -29.5186 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1995 | Eval Loss: 0.2420 | Eval R2: -27.6557 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1866 | Eval Loss: 0.2266 | Eval R2: -25.8814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1747 | Eval Loss: 0.2120 | Eval R2: -24.2060 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1637 | Eval Loss: 0.1984 | Eval R2: -22.5999 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1532 | Eval Loss: 0.1858 | Eval R2: -21.1079 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1436 | Eval Loss: 0.1741 | Eval R2: -19.7307 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1347 | Eval Loss: 0.1630 | Eval R2: -18.4039 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1264 | Eval Loss: 0.1528 | Eval R2: -17.1652 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.091562, test R2 score: -28.667080

==================== GUARDANDO RESULTADOS ===================

         Modelo  ... Loss_final
0          LSTM  ...   0.020435
1  LSTM_NOBATCH  ...   0.020590
2   DyGrEncoder  ...   0.018094
3     MPNN_LSTM  ...   0.020219
4        MSTGCN  ...   0.022348
5     EvolveGCN  ...   0.129529
6        ASTGCN  ...   0.023180
7         AGCRN  ...   0.031952
8         DCRNN  ...   0.019569
9         MTGNN  ...   0.126450

[10 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.12644968926906586, 'r2_eval_final': -17.165163040161133, 'loss_eval_final': 0.1528257131576538, 'r2_test': -28.667079882499493, 'loss_test': 0.0915619507431984, 'loss_nodes': [[0.22884787619113922, 0.06959261000156403, 0.047054894268512726, 0.221352681517601, 0.33882686495780945, 0.03058018907904625, 0.04920437932014465, 0.08924147486686707, 0.06271810829639435, 0.038055527955293655, 0.1031111478805542, 0.1879163384437561, 0.05752849578857422, 0.04111957177519798, 0.03627130761742592, 0.03433100879192352, 0.046053074300289154, 0.07136855274438858, 0.03728175163269043, 0.04078315570950508]]}
wandb: - 0.007 MB of 0.011 MB uploadedwandb: \ 0.014 MB of 0.015 MB uploadedwandb: | 0.014 MB of 0.015 MB uploadedwandb: / 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.12645
wandb: loss_eval 0.15283
wandb: loss_test 0.09156
wandb:   r2_eval -17.16516
wandb:   r2_test -28.66708
wandb: 
wandb: ðŸš€ View run frosty-plant-27 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/p7pqqiow
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_090244-p7pqqiow/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  2%|â–         | 1/50 [17:17<14:07:32, 1037.80s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_092001-ljjczhtr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lively-brook-28
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ljjczhtr

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9406 | Eval Loss: 0.7177 | Eval R2: -69.0260 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5665 | Eval Loss: 0.6025 | Eval R2: -61.0530 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4780 | Eval Loss: 0.5325 | Eval R2: -56.1126 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4211 | Eval Loss: 0.4851 | Eval R2: -52.5185 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3819 | Eval Loss: 0.4495 | Eval R2: -49.5841 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3509 | Eval Loss: 0.4173 | Eval R2: -46.5507 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3231 | Eval Loss: 0.3845 | Eval R2: -43.1084 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2960 | Eval Loss: 0.3521 | Eval R2: -39.4927 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2692 | Eval Loss: 0.3195 | Eval R2: -35.7007 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2427 | Eval Loss: 0.2867 | Eval R2: -31.8565 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2167 | Eval Loss: 0.2537 | Eval R2: -27.9908 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1911 | Eval Loss: 0.2212 | Eval R2: -24.2244 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1661 | Eval Loss: 0.1900 | Eval R2: -20.6477 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1430 | Eval Loss: 0.1614 | Eval R2: -17.4101 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1227 | Eval Loss: 0.1366 | Eval R2: -14.6335 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1054 | Eval Loss: 0.1157 | Eval R2: -12.3548 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0910 | Eval Loss: 0.0986 | Eval R2: -10.5112 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0795 | Eval Loss: 0.0851 | Eval R2: -9.0514 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0705 | Eval Loss: 0.0746 | Eval R2: -7.9315 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0636 | Eval Loss: 0.0665 | Eval R2: -7.0611 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0582 | Eval Loss: 0.0601 | Eval R2: -6.4148 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0537 | Eval Loss: 0.0550 | Eval R2: -5.9043 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0504 | Eval Loss: 0.0509 | Eval R2: -5.5040 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0475 | Eval Loss: 0.0474 | Eval R2: -5.1943 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0445 | Eval R2: -4.9677 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0431 | Eval Loss: 0.0423 | Eval R2: -4.7716 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0412 | Eval Loss: 0.0404 | Eval R2: -4.6258 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.027672, test R2 score: -1.808903

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.020435
1   LSTM_NOBATCH  ...   0.020590
2    DyGrEncoder  ...   0.018094
3      MPNN_LSTM  ...   0.020219
4         MSTGCN  ...   0.022348
5      EvolveGCN  ...   0.129529
6         ASTGCN  ...   0.023180
7          AGCRN  ...   0.031952
8          DCRNN  ...   0.019569
9          MTGNN  ...   0.126450
10         MTGNN  ...   0.041193

[11 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.041193388402462006, 'r2_eval_final': -4.625755310058594, 'loss_eval_final': 0.04035433754324913, 'r2_test': -1.808903404679062, 'loss_test': 0.02767224609851837, 'loss_nodes': [[0.013202655129134655, 0.01554956380277872, 0.03150223195552826, 0.034704551100730896, 0.019489217549562454, 0.03005487471818924, 0.018383193761110306, 0.025080516934394836, 0.019748760387301445, 0.021197233349084854, 0.019077656790614128, 0.022572167217731476, 0.04447653517127037, 0.04018567502498627, 0.022073620930314064, 0.02564123459160328, 0.027177829295396805, 0.02321942150592804, 0.06678865849971771, 0.033319320529699326]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04119
wandb: loss_eval 0.04035
wandb: loss_test 0.02767
wandb:   r2_eval -4.62576
wandb:   r2_test -1.8089
wandb: 
wandb: ðŸš€ View run lively-brook-28 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ljjczhtr
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_092001-ljjczhtr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  4%|â–         | 2/50 [34:23<13:44:36, 1030.76s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_093707-rdzp7iys
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-thunder-29
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/rdzp7iys

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6861 | Eval Loss: 0.6309 | Eval R2: -64.1731 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4428 | Eval Loss: 0.4352 | Eval R2: -47.3339 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3037 | Eval Loss: 0.3134 | Eval R2: -34.6715 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2184 | Eval Loss: 0.2349 | Eval R2: -26.1195 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1667 | Eval Loss: 0.1843 | Eval R2: -19.6434 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1366 | Eval Loss: 0.1524 | Eval R2: -16.1076 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1145 | Eval Loss: 0.1268 | Eval R2: -13.1481 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0977 | Eval Loss: 0.1071 | Eval R2: -11.1422 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0839 | Eval Loss: 0.0910 | Eval R2: -9.5152 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0732 | Eval Loss: 0.0782 | Eval R2: -8.3502 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0640 | Eval Loss: 0.0676 | Eval R2: -7.4014 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0567 | Eval Loss: 0.0589 | Eval R2: -6.5987 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0509 | Eval Loss: 0.0521 | Eval R2: -5.9504 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0465 | Eval Loss: 0.0472 | Eval R2: -5.4698 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0434 | Eval Loss: 0.0440 | Eval R2: -5.1518 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0414 | Eval Loss: 0.0419 | Eval R2: -4.9762 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0401 | Eval Loss: 0.0403 | Eval R2: -4.8069 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0392 | Eval Loss: 0.0389 | Eval R2: -4.6977 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0381 | Eval Loss: 0.0378 | Eval R2: -4.5674 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0371 | Eval Loss: 0.0367 | Eval R2: -4.4823 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0363 | Eval Loss: 0.0357 | Eval R2: -4.3959 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0354 | Eval Loss: 0.0349 | Eval R2: -4.3051 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0348 | Eval Loss: 0.0340 | Eval R2: -4.2082 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0340 | Eval Loss: 0.0332 | Eval R2: -4.1400 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0334 | Eval Loss: 0.0325 | Eval R2: -4.0458 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0328 | Eval Loss: 0.0318 | Eval R2: -3.9814 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0322 | Eval Loss: 0.0312 | Eval R2: -3.9179 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021093, test R2 score: -0.372129

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.020435
1   LSTM_NOBATCH  ...   0.020590
2    DyGrEncoder  ...   0.018094
3      MPNN_LSTM  ...   0.020219
4         MSTGCN  ...   0.022348
5      EvolveGCN  ...   0.129529
6         ASTGCN  ...   0.023180
7          AGCRN  ...   0.031952
8          DCRNN  ...   0.019569
9          MTGNN  ...   0.126450
10         MTGNN  ...   0.041193
11         MTGNN  ...   0.032206

[12 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.03220556676387787, 'r2_eval_final': -3.917853355407715, 'loss_eval_final': 0.031176086515188217, 'r2_test': -0.37212928242891574, 'loss_test': 0.021093221381306648, 'loss_nodes': [[0.01075640320777893, 0.017333436757326126, 0.01910487376153469, 0.016976632177829742, 0.018728718161582947, 0.019068777561187744, 0.02052992209792137, 0.019608285278081894, 0.024387065321207047, 0.01987742818892002, 0.022385532036423683, 0.022065799683332443, 0.023566624149680138, 0.023589322343468666, 0.02104058861732483, 0.018391195684671402, 0.02679825760424137, 0.023205308243632317, 0.028735902160406113, 0.025714339688420296]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.010 MB uploadedwandb: - 0.009 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03221
wandb: loss_eval 0.03118
wandb: loss_test 0.02109
wandb:   r2_eval -3.91785
wandb:   r2_test -0.37213
wandb: 
wandb: ðŸš€ View run flowing-thunder-29 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/rdzp7iys
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_093707-rdzp7iys/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  6%|â–Œ         | 3/50 [45:26<11:15:44, 862.64s/it] Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_094810-ttvgad48
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sandy-shape-30
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ttvgad48

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9979 | Eval Loss: 0.8213 | Eval R2: -80.6152 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6678 | Eval Loss: 0.7452 | Eval R2: -75.0460 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6065 | Eval Loss: 0.6908 | Eval R2: -70.6658 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5597 | Eval Loss: 0.6441 | Eval R2: -66.7606 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5198 | Eval Loss: 0.6033 | Eval R2: -63.2337 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4852 | Eval Loss: 0.5674 | Eval R2: -60.0161 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4548 | Eval Loss: 0.5352 | Eval R2: -57.0445 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4277 | Eval Loss: 0.5058 | Eval R2: -54.2660 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4030 | Eval Loss: 0.4785 | Eval R2: -51.6184 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3800 | Eval Loss: 0.4528 | Eval R2: -49.0846 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3585 | Eval Loss: 0.4286 | Eval R2: -46.6588 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3382 | Eval Loss: 0.4056 | Eval R2: -44.3397 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3192 | Eval Loss: 0.3839 | Eval R2: -42.1296 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3013 | Eval Loss: 0.3635 | Eval R2: -40.0339 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2845 | Eval Loss: 0.3445 | Eval R2: -38.0584 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2690 | Eval Loss: 0.3268 | Eval R2: -36.2070 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2547 | Eval Loss: 0.3105 | Eval R2: -34.4806 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2415 | Eval Loss: 0.2956 | Eval R2: -32.8777 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2295 | Eval Loss: 0.2819 | Eval R2: -31.3952 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.2187 | Eval Loss: 0.2695 | Eval R2: -30.0292 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.2089 | Eval Loss: 0.2582 | Eval R2: -28.7763 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.2001 | Eval Loss: 0.2481 | Eval R2: -27.6329 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1922 | Eval Loss: 0.2391 | Eval R2: -26.5959 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1853 | Eval Loss: 0.2311 | Eval R2: -25.6622 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1793 | Eval Loss: 0.2240 | Eval R2: -24.8283 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1740 | Eval Loss: 0.2179 | Eval R2: -24.0901 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1694 | Eval Loss: 0.2126 | Eval R2: -23.4428 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.129693, test R2 score: -48.333738
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.16944706439971924, 'r2_eval_final': -23.44275665283203, 'loss_eval_final': 0.2126348912715912, 'r2_test': -48.333738278847584, 'loss_test': 0.12969258427619934, 'loss_nodes': [[0.11533419787883759, 0.10998618602752686, 0.11684517562389374, 0.12167207151651382, 0.11069349944591522, 0.1272405982017517, 0.11780054122209549, 0.12352266907691956, 0.11911483854055405, 0.1126246377825737, 0.12741659581661224, 0.1447647362947464, 0.32901352643966675, 0.11253992468118668, 0.1309782713651657, 0.11909196525812149, 0.11139724403619766, 0.11656883358955383, 0.11263705044984818, 0.11460893601179123]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.002 MB of 0.006 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.16945
wandb: loss_eval 0.21263
wandb: loss_test 0.12969
wandb:   r2_eval -23.44276
wandb:   r2_test -48.33374
wandb: 
wandb: ðŸš€ View run sandy-shape-30 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ttvgad48
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_094810-ttvgad48/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  8%|â–Š         | 4/50 [1:01:16<11:27:44, 897.06s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_100359-vicomotf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misunderstood-moon-31
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/vicomotf

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5601 | Eval Loss: 0.5446 | Eval R2: -55.6836 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4060 | Eval Loss: 0.4260 | Eval R2: -45.5388 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3225 | Eval Loss: 0.3513 | Eval R2: -38.1084 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2631 | Eval Loss: 0.2874 | Eval R2: -31.4527 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2151 | Eval Loss: 0.2319 | Eval R2: -25.1911 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1737 | Eval Loss: 0.1841 | Eval R2: -19.9922 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1375 | Eval Loss: 0.1434 | Eval R2: -15.5676 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1074 | Eval Loss: 0.1098 | Eval R2: -11.9490 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0834 | Eval Loss: 0.0838 | Eval R2: -9.1917 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0656 | Eval Loss: 0.0648 | Eval R2: -7.2133 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0526 | Eval Loss: 0.0521 | Eval R2: -5.8853 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0445 | Eval Loss: 0.0444 | Eval R2: -5.0748 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0399 | Eval Loss: 0.0400 | Eval R2: -4.6278 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0371 | Eval Loss: 0.0376 | Eval R2: -4.3609 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0358 | Eval Loss: 0.0360 | Eval R2: -4.1908 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0347 | Eval Loss: 0.0348 | Eval R2: -4.0466 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0338 | Eval R2: -3.9507 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0331 | Eval Loss: 0.0330 | Eval R2: -3.8532 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0325 | Eval Loss: 0.0322 | Eval R2: -3.8090 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0320 | Eval Loss: 0.0316 | Eval R2: -3.7702 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0313 | Eval R2: -3.7362 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0310 | Eval R2: -3.7163 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0305 | Eval Loss: 0.0305 | Eval R2: -3.6961 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0299 | Eval Loss: 0.0303 | Eval R2: -3.6937 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0295 | Eval Loss: 0.0301 | Eval R2: -3.6855 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0291 | Eval Loss: 0.0300 | Eval R2: -3.6777 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0288 | Eval Loss: 0.0297 | Eval R2: -3.6746 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021431, test R2 score: -0.484070
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.028842782601714134, 'r2_eval_final': -3.6746442317962646, 'loss_eval_final': 0.029669931158423424, 'r2_test': -0.4840701399815036, 'loss_test': 0.021431460976600647, 'loss_nodes': [[0.008580573834478855, 0.020281916484236717, 0.022828765213489532, 0.016233496367931366, 0.017392266541719437, 0.02297174744307995, 0.02195732481777668, 0.020490504801273346, 0.019842499867081642, 0.01909630373120308, 0.017229199409484863, 0.02214187942445278, 0.03264974057674408, 0.024735823273658752, 0.025740263983607292, 0.01529536210000515, 0.026793451979756355, 0.02421008236706257, 0.02342572622001171, 0.02673228457570076]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02884
wandb: loss_eval 0.02967
wandb: loss_test 0.02143
wandb:   r2_eval -3.67464
wandb:   r2_test -0.48407
wandb: 
wandb: ðŸš€ View run misunderstood-moon-31 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/vicomotf
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_100359-vicomotf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 10%|â–ˆ         | 5/50 [2:07:04<24:58:00, 1997.35s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_110948-uf0cnm2h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run stoic-haze-32
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/uf0cnm2h

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.2568 | Eval Loss: 1.1295 | Eval R2: -101.8356 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8972 | Eval Loss: 0.8594 | Eval R2: -81.9560 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6748 | Eval Loss: 0.6802 | Eval R2: -68.1097 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5284 | Eval Loss: 0.5582 | Eval R2: -58.0949 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4322 | Eval Loss: 0.4764 | Eval R2: -50.8342 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3653 | Eval Loss: 0.4136 | Eval R2: -44.9040 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3137 | Eval Loss: 0.3606 | Eval R2: -39.6444 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2693 | Eval Loss: 0.3079 | Eval R2: -34.0456 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2275 | Eval Loss: 0.2573 | Eval R2: -28.4109 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1887 | Eval Loss: 0.2114 | Eval R2: -23.0894 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1557 | Eval Loss: 0.1728 | Eval R2: -18.5912 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1294 | Eval Loss: 0.1426 | Eval R2: -15.0105 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1102 | Eval Loss: 0.1203 | Eval R2: -12.3987 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0961 | Eval Loss: 0.1040 | Eval R2: -10.6342 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0852 | Eval Loss: 0.0920 | Eval R2: -9.3715 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0772 | Eval Loss: 0.0827 | Eval R2: -8.3829 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0710 | Eval Loss: 0.0754 | Eval R2: -7.6480 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0659 | Eval Loss: 0.0696 | Eval R2: -7.0950 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0616 | Eval Loss: 0.0648 | Eval R2: -6.6677 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0583 | Eval Loss: 0.0609 | Eval R2: -6.3038 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0553 | Eval Loss: 0.0575 | Eval R2: -6.0243 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0525 | Eval Loss: 0.0545 | Eval R2: -5.7920 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0500 | Eval Loss: 0.0517 | Eval R2: -5.5722 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0476 | Eval Loss: 0.0491 | Eval R2: -5.3585 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0457 | Eval Loss: 0.0467 | Eval R2: -5.1638 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0437 | Eval Loss: 0.0449 | Eval R2: -5.0513 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0421 | Eval Loss: 0.0432 | Eval R2: -4.9379 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.029993, test R2 score: -1.799020
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.04206268489360809, 'r2_eval_final': -4.937933444976807, 'loss_eval_final': 0.04319256916642189, 'r2_test': -1.799020162339317, 'loss_test': 0.02999282442033291, 'loss_nodes': [[0.05149027705192566, 0.015877176076173782, 0.02220309153199196, 0.016386831179261208, 0.017888352274894714, 0.035866495221853256, 0.03802168741822243, 0.08438753336668015, 0.021375661715865135, 0.022349746897816658, 0.020658519119024277, 0.025144334882497787, 0.024777615442872047, 0.02197689190506935, 0.02293766662478447, 0.020766161382198334, 0.055561017245054245, 0.029263479635119438, 0.023395398631691933, 0.029528465121984482]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04206
wandb: loss_eval 0.04319
wandb: loss_test 0.02999
wandb:   r2_eval -4.93793
wandb:   r2_test -1.79902
wandb: 
wandb: ðŸš€ View run stoic-haze-32 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/uf0cnm2h
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_110948-uf0cnm2h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 12%|â–ˆâ–        | 6/50 [2:29:37<21:44:00, 1778.19s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_113220-3hrbx0he
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run good-bird-33
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/3hrbx0he

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6067 | Eval Loss: 0.5669 | Eval R2: -58.0743 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4334 | Eval Loss: 0.4612 | Eval R2: -50.6842 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3577 | Eval Loss: 0.3994 | Eval R2: -44.7530 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3075 | Eval Loss: 0.3515 | Eval R2: -39.5366 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2690 | Eval Loss: 0.3077 | Eval R2: -34.9604 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2337 | Eval Loss: 0.2640 | Eval R2: -29.7876 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2000 | Eval Loss: 0.2213 | Eval R2: -25.0021 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1668 | Eval Loss: 0.1813 | Eval R2: -20.3333 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1377 | Eval Loss: 0.1467 | Eval R2: -16.2444 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1134 | Eval Loss: 0.1182 | Eval R2: -12.9542 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0939 | Eval Loss: 0.0960 | Eval R2: -10.4614 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0790 | Eval Loss: 0.0795 | Eval R2: -8.6400 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0683 | Eval Loss: 0.0680 | Eval R2: -7.3659 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0607 | Eval Loss: 0.0600 | Eval R2: -6.5598 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0556 | Eval Loss: 0.0547 | Eval R2: -6.0174 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0518 | Eval Loss: 0.0508 | Eval R2: -5.6661 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0491 | Eval Loss: 0.0479 | Eval R2: -5.4151 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0466 | Eval Loss: 0.0454 | Eval R2: -5.2136 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0445 | Eval Loss: 0.0434 | Eval R2: -5.0678 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0427 | Eval Loss: 0.0415 | Eval R2: -4.9287 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0410 | Eval Loss: 0.0398 | Eval R2: -4.8269 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0394 | Eval Loss: 0.0382 | Eval R2: -4.7125 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0378 | Eval Loss: 0.0368 | Eval R2: -4.6638 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0364 | Eval Loss: 0.0357 | Eval R2: -4.5684 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0354 | Eval Loss: 0.0346 | Eval R2: -4.4774 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0343 | Eval Loss: 0.0337 | Eval R2: -4.3974 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0335 | Eval Loss: 0.0328 | Eval R2: -4.3126 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021774, test R2 score: -0.442618
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.03352585434913635, 'r2_eval_final': -4.31256628036499, 'loss_eval_final': 0.03283679857850075, 'r2_test': -0.4426180461621023, 'loss_test': 0.021773681044578552, 'loss_nodes': [[0.012117646634578705, 0.015503104776144028, 0.01995454542338848, 0.019054394215345383, 0.022403335198760033, 0.014490786008536816, 0.020095743238925934, 0.02228090539574623, 0.018837561830878258, 0.023077813908457756, 0.01721315272152424, 0.022331954911351204, 0.024609414860606194, 0.03543638065457344, 0.01976127177476883, 0.018085338175296783, 0.02568795531988144, 0.024752117693424225, 0.031954891979694366, 0.027825303375720978]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.006 MB uploadedwandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03353
wandb: loss_eval 0.03284
wandb: loss_test 0.02177
wandb:   r2_eval -4.31257
wandb:   r2_test -0.44262
wandb: 
wandb: ðŸš€ View run good-bird-33 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/3hrbx0he
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_113220-3hrbx0he/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 14%|â–ˆâ–        | 7/50 [2:50:00<19:04:27, 1596.92s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_115244-x2tqzf1q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-energy-34
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/x2tqzf1q

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5301 | Eval Loss: 0.5246 | Eval R2: -56.6961 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3775 | Eval Loss: 0.3985 | Eval R2: -44.5523 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2775 | Eval Loss: 0.2930 | Eval R2: -32.9210 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1954 | Eval Loss: 0.1999 | Eval R2: -22.2212 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1323 | Eval Loss: 0.1349 | Eval R2: -13.8524 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0989 | Eval Loss: 0.1010 | Eval R2: -9.5382 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0814 | Eval Loss: 0.0809 | Eval R2: -7.7944 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0682 | Eval Loss: 0.0670 | Eval R2: -6.7268 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0582 | Eval Loss: 0.0572 | Eval R2: -5.8345 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0513 | Eval Loss: 0.0506 | Eval R2: -5.2568 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0464 | Eval Loss: 0.0457 | Eval R2: -4.8528 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0429 | Eval Loss: 0.0418 | Eval R2: -4.5865 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0401 | Eval Loss: 0.0388 | Eval R2: -4.4244 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0379 | Eval Loss: 0.0366 | Eval R2: -4.3185 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0362 | Eval Loss: 0.0350 | Eval R2: -4.2302 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0349 | Eval Loss: 0.0338 | Eval R2: -4.1476 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0329 | Eval R2: -4.0610 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0328 | Eval Loss: 0.0321 | Eval R2: -3.9698 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0323 | Eval Loss: 0.0315 | Eval R2: -3.8840 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0318 | Eval Loss: 0.0310 | Eval R2: -3.8128 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0305 | Eval R2: -3.7516 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0301 | Eval R2: -3.7023 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0304 | Eval Loss: 0.0297 | Eval R2: -3.6609 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0300 | Eval Loss: 0.0293 | Eval R2: -3.6304 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0296 | Eval Loss: 0.0289 | Eval R2: -3.6087 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0293 | Eval Loss: 0.0286 | Eval R2: -3.5724 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0290 | Eval Loss: 0.0283 | Eval R2: -3.5481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019603, test R2 score: -0.144526

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.020435
1   LSTM_NOBATCH  ...   0.020590
2    DyGrEncoder  ...   0.018094
3      MPNN_LSTM  ...   0.020219
4         MSTGCN  ...   0.022348
5      EvolveGCN  ...   0.129529
6         ASTGCN  ...   0.023180
7          AGCRN  ...   0.031952
8          DCRNN  ...   0.019569
9          MTGNN  ...   0.126450
10         MTGNN  ...   0.041193
11         MTGNN  ...   0.032206
12         MTGNN  ...   0.028956

[13 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02895587868988514, 'r2_eval_final': -3.548116683959961, 'loss_eval_final': 0.028283270075917244, 'r2_test': -0.14452561994347413, 'loss_test': 0.01960347220301628, 'loss_nodes': [[0.009625226259231567, 0.015390723012387753, 0.017777705565094948, 0.016343189403414726, 0.017771588638424873, 0.013662372715771198, 0.01801620051264763, 0.020987290889024734, 0.020280852913856506, 0.020015131682157516, 0.017248516902327538, 0.02101333811879158, 0.023830709978938103, 0.023288622498512268, 0.021366503089666367, 0.018385767936706543, 0.02307620644569397, 0.02448929101228714, 0.023289477452635765, 0.026210714131593704]]}
wandb: - 0.002 MB of 0.010 MB uploadedwandb: \ 0.009 MB of 0.010 MB uploadedwandb: | 0.009 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02896
wandb: loss_eval 0.02828
wandb: loss_test 0.0196
wandb:   r2_eval -3.54812
wandb:   r2_test -0.14453
wandb: 
wandb: ðŸš€ View run fluent-energy-34 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/x2tqzf1q
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_115244-x2tqzf1q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 16%|â–ˆâ–Œ        | 8/50 [3:09:39<17:04:40, 1463.82s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_121223-bxlrztjd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-firefly-35
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/bxlrztjd

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6861 | Eval Loss: 0.6309 | Eval R2: -64.1731 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4428 | Eval Loss: 0.4352 | Eval R2: -47.3339 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3037 | Eval Loss: 0.3134 | Eval R2: -34.6715 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2184 | Eval Loss: 0.2349 | Eval R2: -26.1195 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1667 | Eval Loss: 0.1843 | Eval R2: -19.6434 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1366 | Eval Loss: 0.1524 | Eval R2: -16.1076 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1145 | Eval Loss: 0.1268 | Eval R2: -13.1481 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0977 | Eval Loss: 0.1071 | Eval R2: -11.1422 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0839 | Eval Loss: 0.0910 | Eval R2: -9.5152 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0732 | Eval Loss: 0.0782 | Eval R2: -8.3502 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0640 | Eval Loss: 0.0676 | Eval R2: -7.4014 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0567 | Eval Loss: 0.0589 | Eval R2: -6.5987 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0509 | Eval Loss: 0.0521 | Eval R2: -5.9504 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0465 | Eval Loss: 0.0472 | Eval R2: -5.4698 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0434 | Eval Loss: 0.0440 | Eval R2: -5.1518 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0414 | Eval Loss: 0.0419 | Eval R2: -4.9762 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0401 | Eval Loss: 0.0403 | Eval R2: -4.8069 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0392 | Eval Loss: 0.0389 | Eval R2: -4.6977 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0381 | Eval Loss: 0.0378 | Eval R2: -4.5674 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0371 | Eval Loss: 0.0367 | Eval R2: -4.4823 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0363 | Eval Loss: 0.0357 | Eval R2: -4.3959 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0354 | Eval Loss: 0.0349 | Eval R2: -4.3051 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0348 | Eval Loss: 0.0340 | Eval R2: -4.2082 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0340 | Eval Loss: 0.0332 | Eval R2: -4.1400 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0334 | Eval Loss: 0.0325 | Eval R2: -4.0458 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0328 | Eval Loss: 0.0318 | Eval R2: -3.9814 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0322 | Eval Loss: 0.0312 | Eval R2: -3.9179 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021093, test R2 score: -0.372129
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.03220556676387787, 'r2_eval_final': -3.917853355407715, 'loss_eval_final': 0.031176086515188217, 'r2_test': -0.37212928242891574, 'loss_test': 0.021093221381306648, 'loss_nodes': [[0.01075640320777893, 0.017333436757326126, 0.01910487376153469, 0.016976632177829742, 0.018728718161582947, 0.019068777561187744, 0.02052992209792137, 0.019608285278081894, 0.024387065321207047, 0.01987742818892002, 0.022385532036423683, 0.022065799683332443, 0.023566624149680138, 0.023589322343468666, 0.02104058861732483, 0.018391195684671402, 0.02679825760424137, 0.023205308243632317, 0.028735902160406113, 0.025714339688420296]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03221
wandb: loss_eval 0.03118
wandb: loss_test 0.02109
wandb:   r2_eval -3.91785
wandb:   r2_test -0.37213
wandb: 
wandb: ðŸš€ View run cosmic-firefly-35 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/bxlrztjd
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_121223-bxlrztjd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 18%|â–ˆâ–Š        | 9/50 [3:21:17<13:56:35, 1224.29s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_122401-8xnzzc5v
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-shape-36
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/8xnzzc5v

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8469 | Eval Loss: 0.6166 | Eval R2: -62.4670 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4232 | Eval Loss: 0.4041 | Eval R2: -43.2557 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2733 | Eval Loss: 0.2715 | Eval R2: -30.1345 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1835 | Eval Loss: 0.1888 | Eval R2: -20.0040 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1364 | Eval Loss: 0.1439 | Eval R2: -13.9813 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1135 | Eval Loss: 0.1219 | Eval R2: -11.6725 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1000 | Eval Loss: 0.1085 | Eval R2: -10.6552 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0898 | Eval Loss: 0.0963 | Eval R2: -9.5197 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0805 | Eval Loss: 0.0844 | Eval R2: -8.4025 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0714 | Eval Loss: 0.0732 | Eval R2: -7.3920 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0629 | Eval Loss: 0.0632 | Eval R2: -6.4813 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0553 | Eval Loss: 0.0546 | Eval R2: -5.7147 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0488 | Eval Loss: 0.0478 | Eval R2: -5.1240 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0438 | Eval Loss: 0.0428 | Eval R2: -4.7210 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0402 | Eval Loss: 0.0394 | Eval R2: -4.4457 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0377 | Eval Loss: 0.0371 | Eval R2: -4.2397 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0360 | Eval Loss: 0.0354 | Eval R2: -4.1048 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0345 | Eval Loss: 0.0341 | Eval R2: -3.9942 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0334 | Eval Loss: 0.0329 | Eval R2: -3.9200 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0325 | Eval Loss: 0.0319 | Eval R2: -3.8683 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0317 | Eval Loss: 0.0310 | Eval R2: -3.8074 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0301 | Eval R2: -3.7676 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0303 | Eval Loss: 0.0293 | Eval R2: -3.7310 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0297 | Eval Loss: 0.0286 | Eval R2: -3.7064 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0291 | Eval Loss: 0.0280 | Eval R2: -3.6970 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0285 | Eval Loss: 0.0275 | Eval R2: -3.6785 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0279 | Eval Loss: 0.0271 | Eval R2: -3.6893 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.017566, test R2 score: -0.042249

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.020435
1   LSTM_NOBATCH  ...   0.020590
2    DyGrEncoder  ...   0.018094
3      MPNN_LSTM  ...   0.020219
4         MSTGCN  ...   0.022348
5      EvolveGCN  ...   0.129529
6         ASTGCN  ...   0.023180
7          AGCRN  ...   0.031952
8          DCRNN  ...   0.019569
9          MTGNN  ...   0.126450
10         MTGNN  ...   0.041193
11         MTGNN  ...   0.032206
12         MTGNN  ...   0.028956
13         MTGNN  ...   0.027949

[14 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.027949195355176926, 'r2_eval_final': -3.689293622970581, 'loss_eval_final': 0.027116579934954643, 'r2_test': -0.0422485305574804, 'loss_test': 0.017566492781043053, 'loss_nodes': [[0.01061359141021967, 0.013762978836894035, 0.016460943967103958, 0.013742550276219845, 0.015148378908634186, 0.011305090971291065, 0.015580961480736732, 0.018607521429657936, 0.016537388786673546, 0.016799455508589745, 0.015795554965734482, 0.02146291732788086, 0.02122339978814125, 0.01979907788336277, 0.018871840089559555, 0.016365034505724907, 0.02240491844713688, 0.02184617705643177, 0.021845700219273567, 0.02315639890730381]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.010 MB uploadedwandb: | 0.002 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02795
wandb: loss_eval 0.02712
wandb: loss_test 0.01757
wandb:   r2_eval -3.68929
wandb:   r2_test -0.04225
wandb: 
wandb: ðŸš€ View run ethereal-shape-36 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/8xnzzc5v
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_122401-8xnzzc5v/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 20%|â–ˆâ–ˆ        | 10/50 [4:23:51<22:16:54, 2005.36s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_132635-j07zdfjw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-waterfall-37
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/j07zdfjw

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9406 | Eval Loss: 0.7177 | Eval R2: -69.0260 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5665 | Eval Loss: 0.6025 | Eval R2: -61.0530 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4780 | Eval Loss: 0.5325 | Eval R2: -56.1126 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4211 | Eval Loss: 0.4851 | Eval R2: -52.5185 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3819 | Eval Loss: 0.4495 | Eval R2: -49.5841 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3509 | Eval Loss: 0.4173 | Eval R2: -46.5507 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3231 | Eval Loss: 0.3845 | Eval R2: -43.1084 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2960 | Eval Loss: 0.3521 | Eval R2: -39.4927 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2692 | Eval Loss: 0.3195 | Eval R2: -35.7007 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2427 | Eval Loss: 0.2867 | Eval R2: -31.8565 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2167 | Eval Loss: 0.2537 | Eval R2: -27.9908 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1911 | Eval Loss: 0.2212 | Eval R2: -24.2244 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1661 | Eval Loss: 0.1900 | Eval R2: -20.6477 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1430 | Eval Loss: 0.1614 | Eval R2: -17.4101 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1227 | Eval Loss: 0.1366 | Eval R2: -14.6335 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1054 | Eval Loss: 0.1157 | Eval R2: -12.3548 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0910 | Eval Loss: 0.0986 | Eval R2: -10.5112 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0795 | Eval Loss: 0.0851 | Eval R2: -9.0514 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0705 | Eval Loss: 0.0746 | Eval R2: -7.9315 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0636 | Eval Loss: 0.0665 | Eval R2: -7.0611 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0582 | Eval Loss: 0.0601 | Eval R2: -6.4148 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0537 | Eval Loss: 0.0550 | Eval R2: -5.9043 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0504 | Eval Loss: 0.0509 | Eval R2: -5.5040 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0475 | Eval Loss: 0.0474 | Eval R2: -5.1943 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0445 | Eval R2: -4.9677 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0431 | Eval Loss: 0.0423 | Eval R2: -4.7716 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0412 | Eval Loss: 0.0404 | Eval R2: -4.6258 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.027672, test R2 score: -1.808903
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.041193388402462006, 'r2_eval_final': -4.625755310058594, 'loss_eval_final': 0.04035433754324913, 'r2_test': -1.808903404679062, 'loss_test': 0.02767224609851837, 'loss_nodes': [[0.013202655129134655, 0.01554956380277872, 0.03150223195552826, 0.034704551100730896, 0.019489217549562454, 0.03005487471818924, 0.018383193761110306, 0.025080516934394836, 0.019748760387301445, 0.021197233349084854, 0.019077656790614128, 0.022572167217731476, 0.04447653517127037, 0.04018567502498627, 0.022073620930314064, 0.02564123459160328, 0.027177829295396805, 0.02321942150592804, 0.06678865849971771, 0.033319320529699326]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.002 MB of 0.006 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04119
wandb: loss_eval 0.04035
wandb: loss_test 0.02767
wandb:   r2_eval -4.62576
wandb:   r2_test -1.8089
wandb: 
wandb: ðŸš€ View run wild-waterfall-37 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/j07zdfjw
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_132635-j07zdfjw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 22%|â–ˆâ–ˆâ–       | 11/50 [4:42:42<18:49:26, 1737.61s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_134525-35j24gub
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run absurd-cloud-38
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/35j24gub

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6763 | Eval Loss: 0.6243 | Eval R2: -59.9298 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5073 | Eval Loss: 0.5522 | Eval R2: -54.4522 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4438 | Eval Loss: 0.4915 | Eval R2: -49.5612 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3911 | Eval Loss: 0.4390 | Eval R2: -45.0494 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3460 | Eval Loss: 0.3932 | Eval R2: -40.8861 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3077 | Eval Loss: 0.3536 | Eval R2: -37.1289 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2753 | Eval Loss: 0.3198 | Eval R2: -33.8068 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2483 | Eval Loss: 0.2914 | Eval R2: -30.9293 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2259 | Eval Loss: 0.2679 | Eval R2: -28.4905 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2078 | Eval Loss: 0.2488 | Eval R2: -26.4732 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1932 | Eval Loss: 0.2334 | Eval R2: -24.8467 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1817 | Eval Loss: 0.2214 | Eval R2: -23.5660 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1728 | Eval Loss: 0.2122 | Eval R2: -22.5798 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1660 | Eval Loss: 0.2051 | Eval R2: -21.8380 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1608 | Eval Loss: 0.1999 | Eval R2: -21.2946 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1570 | Eval Loss: 0.1960 | Eval R2: -20.9076 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1542 | Eval Loss: 0.1933 | Eval R2: -20.6390 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1521 | Eval Loss: 0.1913 | Eval R2: -20.4567 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1507 | Eval Loss: 0.1900 | Eval R2: -20.3355 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1497 | Eval Loss: 0.1891 | Eval R2: -20.2564 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1490 | Eval Loss: 0.1885 | Eval R2: -20.2059 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1486 | Eval Loss: 0.1881 | Eval R2: -20.1745 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1483 | Eval Loss: 0.1878 | Eval R2: -20.1556 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1481 | Eval Loss: 0.1877 | Eval R2: -20.1448 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1480 | Eval Loss: 0.1876 | Eval R2: -20.1391 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1479 | Eval Loss: 0.1875 | Eval R2: -20.1365 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1478 | Eval Loss: 0.1875 | Eval R2: -20.1357 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.111854, test R2 score: -38.445755
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.14784418046474457, 'r2_eval_final': -20.135665893554688, 'loss_eval_final': 0.1874518245458603, 'r2_test': -38.44575455570788, 'loss_test': 0.11185360699892044, 'loss_nodes': [[0.10646530240774155, 0.1090187132358551, 0.1168610230088234, 0.11113505065441132, 0.10904613882303238, 0.10864491015672684, 0.11052116006612778, 0.11730612814426422, 0.11200553923845291, 0.11206074804067612, 0.10926749557256699, 0.1111818477511406, 0.11995711922645569, 0.11203429102897644, 0.10961897671222687, 0.10937025398015976, 0.11143827438354492, 0.1178245097398758, 0.11203797906637192, 0.11127680540084839]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.14784
wandb: loss_eval 0.18745
wandb: loss_test 0.11185
wandb:   r2_eval -20.13567
wandb:   r2_test -38.44575
wandb: 
wandb: ðŸš€ View run absurd-cloud-38 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/35j24gub
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_134525-35j24gub/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 24%|â–ˆâ–ˆâ–       | 12/50 [4:54:33<15:02:52, 1425.58s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_135717-z1p2451c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run divine-sea-39
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/z1p2451c

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6573 | Eval Loss: 0.5538 | Eval R2: -55.6784 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4328 | Eval Loss: 0.4495 | Eval R2: -45.5657 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3522 | Eval Loss: 0.3719 | Eval R2: -39.1185 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2902 | Eval Loss: 0.3107 | Eval R2: -33.0903 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2420 | Eval Loss: 0.2594 | Eval R2: -27.7261 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2021 | Eval Loss: 0.2153 | Eval R2: -23.0883 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1681 | Eval Loss: 0.1777 | Eval R2: -18.9870 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1399 | Eval Loss: 0.1463 | Eval R2: -15.5743 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1167 | Eval Loss: 0.1207 | Eval R2: -12.8849 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0979 | Eval Loss: 0.1002 | Eval R2: -10.7940 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0826 | Eval Loss: 0.0842 | Eval R2: -9.1934 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0709 | Eval Loss: 0.0719 | Eval R2: -7.9978 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0619 | Eval Loss: 0.0625 | Eval R2: -7.0461 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0547 | Eval Loss: 0.0552 | Eval R2: -6.3060 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0492 | Eval Loss: 0.0495 | Eval R2: -5.7373 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0451 | Eval Loss: 0.0450 | Eval R2: -5.3583 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0418 | Eval Loss: 0.0415 | Eval R2: -5.0419 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0391 | Eval Loss: 0.0389 | Eval R2: -4.8112 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0371 | Eval Loss: 0.0369 | Eval R2: -4.6227 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0357 | Eval Loss: 0.0353 | Eval R2: -4.4842 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0344 | Eval Loss: 0.0340 | Eval R2: -4.3481 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0334 | Eval Loss: 0.0329 | Eval R2: -4.2187 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0325 | Eval Loss: 0.0320 | Eval R2: -4.1514 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0318 | Eval Loss: 0.0312 | Eval R2: -4.0536 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0310 | Eval Loss: 0.0305 | Eval R2: -3.9915 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0305 | Eval Loss: 0.0298 | Eval R2: -3.9036 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0300 | Eval Loss: 0.0292 | Eval R2: -3.8488 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019885, test R2 score: -0.234907
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.029994983226060867, 'r2_eval_final': -3.848764657974243, 'loss_eval_final': 0.02921174094080925, 'r2_test': -0.23490676674224115, 'loss_test': 0.01988532394170761, 'loss_nodes': [[0.009583338163793087, 0.014968340285122395, 0.01885943114757538, 0.019679903984069824, 0.016741234809160233, 0.012855453416705132, 0.01953095570206642, 0.02006017602980137, 0.02120284177362919, 0.01921192742884159, 0.016463950276374817, 0.021908311173319817, 0.022603482007980347, 0.022363794967532158, 0.022911371663212776, 0.016040848568081856, 0.025596145540475845, 0.024490447714924812, 0.023276524618268013, 0.02935803309082985]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02999
wandb: loss_eval 0.02921
wandb: loss_test 0.01989
wandb:   r2_eval -3.84876
wandb:   r2_test -0.23491
wandb: 
wandb: ðŸš€ View run divine-sea-39 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/z1p2451c
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_135717-z1p2451c/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 26%|â–ˆâ–ˆâ–Œ       | 13/50 [5:55:02<21:30:43, 2093.06s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_145746-v2tltnix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-capybara-40
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/v2tltnix

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5301 | Eval Loss: 0.5246 | Eval R2: -56.6961 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3775 | Eval Loss: 0.3985 | Eval R2: -44.5523 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2775 | Eval Loss: 0.2930 | Eval R2: -32.9210 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1954 | Eval Loss: 0.1999 | Eval R2: -22.2212 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1323 | Eval Loss: 0.1349 | Eval R2: -13.8524 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0989 | Eval Loss: 0.1010 | Eval R2: -9.5382 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0814 | Eval Loss: 0.0809 | Eval R2: -7.7944 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0682 | Eval Loss: 0.0670 | Eval R2: -6.7268 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0582 | Eval Loss: 0.0572 | Eval R2: -5.8345 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0513 | Eval Loss: 0.0506 | Eval R2: -5.2568 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0464 | Eval Loss: 0.0457 | Eval R2: -4.8528 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0429 | Eval Loss: 0.0418 | Eval R2: -4.5865 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0401 | Eval Loss: 0.0388 | Eval R2: -4.4244 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0379 | Eval Loss: 0.0366 | Eval R2: -4.3185 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0362 | Eval Loss: 0.0350 | Eval R2: -4.2302 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0349 | Eval Loss: 0.0338 | Eval R2: -4.1476 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0329 | Eval R2: -4.0610 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0328 | Eval Loss: 0.0321 | Eval R2: -3.9698 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0323 | Eval Loss: 0.0315 | Eval R2: -3.8840 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0318 | Eval Loss: 0.0310 | Eval R2: -3.8128 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0305 | Eval R2: -3.7516 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0301 | Eval R2: -3.7023 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0304 | Eval Loss: 0.0297 | Eval R2: -3.6609 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0300 | Eval Loss: 0.0293 | Eval R2: -3.6304 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0296 | Eval Loss: 0.0289 | Eval R2: -3.6087 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0293 | Eval Loss: 0.0286 | Eval R2: -3.5724 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0290 | Eval Loss: 0.0283 | Eval R2: -3.5481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019603, test R2 score: -0.144526
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02895587868988514, 'r2_eval_final': -3.548116683959961, 'loss_eval_final': 0.028283270075917244, 'r2_test': -0.14452561994347413, 'loss_test': 0.01960347220301628, 'loss_nodes': [[0.009625226259231567, 0.015390723012387753, 0.017777705565094948, 0.016343189403414726, 0.017771588638424873, 0.013662372715771198, 0.01801620051264763, 0.020987290889024734, 0.020280852913856506, 0.020015131682157516, 0.017248516902327538, 0.02101333811879158, 0.023830709978938103, 0.023288622498512268, 0.021366503089666367, 0.018385767936706543, 0.02307620644569397, 0.02448929101228714, 0.023289477452635765, 0.026210714131593704]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02896
wandb: loss_eval 0.02828
wandb: loss_test 0.0196
wandb:   r2_eval -3.54812
wandb:   r2_test -0.14453
wandb: 
wandb: ðŸš€ View run blooming-capybara-40 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/v2tltnix
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_145746-v2tltnix/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 28%|â–ˆâ–ˆâ–Š       | 14/50 [6:14:03<18:03:16, 1805.45s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_151647-gwspkcn8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-resonance-41
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/gwspkcn8

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6322 | Eval Loss: 1.4431 | Eval R2: -121.8103 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2103 | Eval Loss: 1.2183 | Eval R2: -106.5182 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.0115 | Eval Loss: 1.0375 | Eval R2: -93.5331 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.8585 | Eval Loss: 0.9000 | Eval R2: -83.3674 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.7424 | Eval Loss: 0.7942 | Eval R2: -75.3603 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.6526 | Eval Loss: 0.7106 | Eval R2: -68.9137 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5815 | Eval Loss: 0.6433 | Eval R2: -63.6178 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.5240 | Eval Loss: 0.5879 | Eval R2: -59.1703 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4766 | Eval Loss: 0.5417 | Eval R2: -55.3686 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4372 | Eval Loss: 0.5024 | Eval R2: -52.0609 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4035 | Eval Loss: 0.4681 | Eval R2: -49.1060 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3741 | Eval Loss: 0.4378 | Eval R2: -46.4257 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3482 | Eval Loss: 0.4106 | Eval R2: -43.9608 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3251 | Eval Loss: 0.3860 | Eval R2: -41.6767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3042 | Eval Loss: 0.3635 | Eval R2: -39.5522 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2853 | Eval Loss: 0.3429 | Eval R2: -37.5664 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2679 | Eval Loss: 0.3239 | Eval R2: -35.7048 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2521 | Eval Loss: 0.3065 | Eval R2: -33.9607 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2377 | Eval Loss: 0.2905 | Eval R2: -32.3328 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.2247 | Eval Loss: 0.2760 | Eval R2: -30.8224 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.2129 | Eval Loss: 0.2629 | Eval R2: -29.4312 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.2024 | Eval Loss: 0.2512 | Eval R2: -28.1605 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1932 | Eval Loss: 0.2408 | Eval R2: -27.0108 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1852 | Eval Loss: 0.2318 | Eval R2: -25.9812 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1783 | Eval Loss: 0.2239 | Eval R2: -25.0689 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1724 | Eval Loss: 0.2173 | Eval R2: -24.2696 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1676 | Eval Loss: 0.2117 | Eval R2: -23.5770 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.127625, test R2 score: -49.634858
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.1675509512424469, 'r2_eval_final': -23.577024459838867, 'loss_eval_final': 0.21165750920772552, 'r2_test': -49.63485817873419, 'loss_test': 0.1276254504919052, 'loss_nodes': [[0.17279943823814392, 0.10889368504285812, 0.11788100004196167, 0.11727610975503922, 0.2720925807952881, 0.10808441787958145, 0.11373510211706161, 0.11658327281475067, 0.11251711845397949, 0.11280911415815353, 0.10963527858257294, 0.11113720387220383, 0.1181657612323761, 0.15471269190311432, 0.10962535440921783, 0.11082317680120468, 0.1180385947227478, 0.14368577301502228, 0.11213406175374985, 0.11187942326068878]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.16755
wandb: loss_eval 0.21166
wandb: loss_test 0.12763
wandb:   r2_eval -23.57702
wandb:   r2_test -49.63486
wandb: 
wandb: ðŸš€ View run royal-resonance-41 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/gwspkcn8
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_151647-gwspkcn8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [6:32:09<15:26:34, 1588.42s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_153453-yneuqt3o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run young-wildflower-42
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/yneuqt3o

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6573 | Eval Loss: 0.5538 | Eval R2: -55.6784 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4328 | Eval Loss: 0.4495 | Eval R2: -45.5657 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3522 | Eval Loss: 0.3719 | Eval R2: -39.1185 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2902 | Eval Loss: 0.3107 | Eval R2: -33.0903 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2420 | Eval Loss: 0.2594 | Eval R2: -27.7261 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2021 | Eval Loss: 0.2153 | Eval R2: -23.0883 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1681 | Eval Loss: 0.1777 | Eval R2: -18.9870 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1399 | Eval Loss: 0.1463 | Eval R2: -15.5743 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1167 | Eval Loss: 0.1207 | Eval R2: -12.8849 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0979 | Eval Loss: 0.1002 | Eval R2: -10.7940 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0826 | Eval Loss: 0.0842 | Eval R2: -9.1934 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0709 | Eval Loss: 0.0719 | Eval R2: -7.9978 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0619 | Eval Loss: 0.0625 | Eval R2: -7.0461 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0547 | Eval Loss: 0.0552 | Eval R2: -6.3060 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0492 | Eval Loss: 0.0495 | Eval R2: -5.7373 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0451 | Eval Loss: 0.0450 | Eval R2: -5.3583 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0418 | Eval Loss: 0.0415 | Eval R2: -5.0419 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0391 | Eval Loss: 0.0389 | Eval R2: -4.8112 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0371 | Eval Loss: 0.0369 | Eval R2: -4.6227 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0357 | Eval Loss: 0.0353 | Eval R2: -4.4842 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0344 | Eval Loss: 0.0340 | Eval R2: -4.3481 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0334 | Eval Loss: 0.0329 | Eval R2: -4.2187 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0325 | Eval Loss: 0.0320 | Eval R2: -4.1514 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0318 | Eval Loss: 0.0312 | Eval R2: -4.0536 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0310 | Eval Loss: 0.0305 | Eval R2: -3.9915 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0305 | Eval Loss: 0.0298 | Eval R2: -3.9036 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0300 | Eval Loss: 0.0292 | Eval R2: -3.8488 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019885, test R2 score: -0.234907
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.029994983226060867, 'r2_eval_final': -3.848764657974243, 'loss_eval_final': 0.02921174094080925, 'r2_test': -0.23490676674224115, 'loss_test': 0.01988532394170761, 'loss_nodes': [[0.009583338163793087, 0.014968340285122395, 0.01885943114757538, 0.019679903984069824, 0.016741234809160233, 0.012855453416705132, 0.01953095570206642, 0.02006017602980137, 0.02120284177362919, 0.01921192742884159, 0.016463950276374817, 0.021908311173319817, 0.022603482007980347, 0.022363794967532158, 0.022911371663212776, 0.016040848568081856, 0.025596145540475845, 0.024490447714924812, 0.023276524618268013, 0.02935803309082985]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02999
wandb: loss_eval 0.02921
wandb: loss_test 0.01989
wandb:   r2_eval -3.84876
wandb:   r2_test -0.23491
wandb: 
wandb: ðŸš€ View run young-wildflower-42 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/yneuqt3o
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_153453-yneuqt3o/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [7:31:32<20:36:58, 2182.90s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_163416-txg8k1ik
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-smoke-43
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/txg8k1ik

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7917 | Eval Loss: 0.5164 | Eval R2: -53.5589 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3822 | Eval Loss: 0.4184 | Eval R2: -44.8344 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3060 | Eval Loss: 0.3466 | Eval R2: -37.8931 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2535 | Eval Loss: 0.2960 | Eval R2: -32.6169 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2178 | Eval Loss: 0.2604 | Eval R2: -28.7501 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1931 | Eval Loss: 0.2354 | Eval R2: -26.0067 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1761 | Eval Loss: 0.2180 | Eval R2: -24.0868 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1645 | Eval Loss: 0.2061 | Eval R2: -22.7332 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1570 | Eval Loss: 0.1983 | Eval R2: -21.7787 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1525 | Eval Loss: 0.1934 | Eval R2: -21.1246 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1499 | Eval Loss: 0.1905 | Eval R2: -20.6994 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1487 | Eval Loss: 0.1889 | Eval R2: -20.4384 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1481 | Eval Loss: 0.1881 | Eval R2: -20.2860 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1479 | Eval Loss: 0.1877 | Eval R2: -20.2007 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1478 | Eval Loss: 0.1875 | Eval R2: -20.1549 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1317 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1209 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1166 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1152 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1147 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1143 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1138 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1132 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1124 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1116 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1108 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1100 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.111834, test R2 score: -38.356727
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.147806778550148, 'r2_eval_final': -20.109994888305664, 'loss_eval_final': 0.18730062246322632, 'r2_test': -38.35672688770183, 'loss_test': 0.11183405667543411, 'loss_nodes': [[0.10658377408981323, 0.10900574177503586, 0.1168871745467186, 0.111124686896801, 0.10906130820512772, 0.10847725719213486, 0.11053680628538132, 0.11728990077972412, 0.11200691014528275, 0.1120700091123581, 0.10925966501235962, 0.11114026606082916, 0.11983019858598709, 0.11205482482910156, 0.10959266126155853, 0.10940238833427429, 0.11147241294384003, 0.11789589375257492, 0.11173909157514572, 0.11125024408102036]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.14781
wandb: loss_eval 0.1873
wandb: loss_test 0.11183
wandb:   r2_eval -20.10999
wandb:   r2_test -38.35673
wandb: 
wandb: ðŸš€ View run flowing-smoke-43 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/txg8k1ik
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_163416-txg8k1ik/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [7:50:57<17:12:13, 1876.78s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_165341-uhmk10qh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dry-universe-44
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/uhmk10qh

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5601 | Eval Loss: 0.5446 | Eval R2: -55.6836 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4060 | Eval Loss: 0.4260 | Eval R2: -45.5388 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3225 | Eval Loss: 0.3513 | Eval R2: -38.1084 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2631 | Eval Loss: 0.2874 | Eval R2: -31.4527 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2151 | Eval Loss: 0.2319 | Eval R2: -25.1911 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1737 | Eval Loss: 0.1841 | Eval R2: -19.9922 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1375 | Eval Loss: 0.1434 | Eval R2: -15.5676 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1074 | Eval Loss: 0.1098 | Eval R2: -11.9490 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0834 | Eval Loss: 0.0838 | Eval R2: -9.1917 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0656 | Eval Loss: 0.0648 | Eval R2: -7.2133 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0526 | Eval Loss: 0.0521 | Eval R2: -5.8853 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0445 | Eval Loss: 0.0444 | Eval R2: -5.0748 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0399 | Eval Loss: 0.0400 | Eval R2: -4.6278 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0371 | Eval Loss: 0.0376 | Eval R2: -4.3609 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0358 | Eval Loss: 0.0360 | Eval R2: -4.1908 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0347 | Eval Loss: 0.0348 | Eval R2: -4.0466 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0338 | Eval R2: -3.9507 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0331 | Eval Loss: 0.0330 | Eval R2: -3.8532 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0325 | Eval Loss: 0.0322 | Eval R2: -3.8090 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0320 | Eval Loss: 0.0316 | Eval R2: -3.7702 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0313 | Eval R2: -3.7362 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0310 | Eval R2: -3.7163 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0305 | Eval Loss: 0.0305 | Eval R2: -3.6961 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0299 | Eval Loss: 0.0303 | Eval R2: -3.6937 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0295 | Eval Loss: 0.0301 | Eval R2: -3.6855 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0291 | Eval Loss: 0.0300 | Eval R2: -3.6777 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0288 | Eval Loss: 0.0297 | Eval R2: -3.6746 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021431, test R2 score: -0.484070
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.028842782601714134, 'r2_eval_final': -3.6746442317962646, 'loss_eval_final': 0.029669931158423424, 'r2_test': -0.4840701399815036, 'loss_test': 0.021431460976600647, 'loss_nodes': [[0.008580573834478855, 0.020281916484236717, 0.022828765213489532, 0.016233496367931366, 0.017392266541719437, 0.02297174744307995, 0.02195732481777668, 0.020490504801273346, 0.019842499867081642, 0.01909630373120308, 0.017229199409484863, 0.02214187942445278, 0.03264974057674408, 0.024735823273658752, 0.025740263983607292, 0.01529536210000515, 0.026793451979756355, 0.02421008236706257, 0.02342572622001171, 0.02673228457570076]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02884
wandb: loss_eval 0.02967
wandb: loss_test 0.02143
wandb:   r2_eval -3.67464
wandb:   r2_test -0.48407
wandb: 
wandb: ðŸš€ View run dry-universe-44 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/uhmk10qh
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_165341-uhmk10qh/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [8:57:56<22:24:12, 2520.38s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_180040-v6rvk07a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-valley-45
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/v6rvk07a

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5601 | Eval Loss: 0.5446 | Eval R2: -55.6836 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4060 | Eval Loss: 0.4260 | Eval R2: -45.5388 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3225 | Eval Loss: 0.3513 | Eval R2: -38.1084 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2631 | Eval Loss: 0.2874 | Eval R2: -31.4527 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2151 | Eval Loss: 0.2319 | Eval R2: -25.1911 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1737 | Eval Loss: 0.1841 | Eval R2: -19.9922 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1375 | Eval Loss: 0.1434 | Eval R2: -15.5676 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1074 | Eval Loss: 0.1098 | Eval R2: -11.9490 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0834 | Eval Loss: 0.0838 | Eval R2: -9.1917 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0656 | Eval Loss: 0.0648 | Eval R2: -7.2133 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0526 | Eval Loss: 0.0521 | Eval R2: -5.8853 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0445 | Eval Loss: 0.0444 | Eval R2: -5.0748 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0399 | Eval Loss: 0.0400 | Eval R2: -4.6278 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0371 | Eval Loss: 0.0376 | Eval R2: -4.3609 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0358 | Eval Loss: 0.0360 | Eval R2: -4.1908 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0347 | Eval Loss: 0.0348 | Eval R2: -4.0466 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0338 | Eval R2: -3.9507 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0331 | Eval Loss: 0.0330 | Eval R2: -3.8532 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0325 | Eval Loss: 0.0322 | Eval R2: -3.8090 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0320 | Eval Loss: 0.0316 | Eval R2: -3.7702 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0313 | Eval R2: -3.7362 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0310 | Eval R2: -3.7163 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0305 | Eval Loss: 0.0305 | Eval R2: -3.6961 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0299 | Eval Loss: 0.0303 | Eval R2: -3.6937 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0295 | Eval Loss: 0.0301 | Eval R2: -3.6855 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0291 | Eval Loss: 0.0300 | Eval R2: -3.6777 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0288 | Eval Loss: 0.0297 | Eval R2: -3.6746 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021431, test R2 score: -0.484070
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.028842782601714134, 'r2_eval_final': -3.6746442317962646, 'loss_eval_final': 0.029669931158423424, 'r2_test': -0.4840701399815036, 'loss_test': 0.021431460976600647, 'loss_nodes': [[0.008580573834478855, 0.020281916484236717, 0.022828765213489532, 0.016233496367931366, 0.017392266541719437, 0.02297174744307995, 0.02195732481777668, 0.020490504801273346, 0.019842499867081642, 0.01909630373120308, 0.017229199409484863, 0.02214187942445278, 0.03264974057674408, 0.024735823273658752, 0.025740263983607292, 0.01529536210000515, 0.026793451979756355, 0.02421008236706257, 0.02342572622001171, 0.02673228457570076]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02884
wandb: loss_eval 0.02967
wandb: loss_test 0.02143
wandb:   r2_eval -3.67464
wandb:   r2_test -0.48407
wandb: 
wandb: ðŸš€ View run warm-valley-45 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/v6rvk07a
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_180040-v6rvk07a/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [10:06:08<25:46:10, 2992.59s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_190852-kirmhrg6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run happy-terrain-46
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/kirmhrg6

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5301 | Eval Loss: 0.5246 | Eval R2: -56.6961 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3775 | Eval Loss: 0.3985 | Eval R2: -44.5523 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2775 | Eval Loss: 0.2930 | Eval R2: -32.9210 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1954 | Eval Loss: 0.1999 | Eval R2: -22.2212 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1323 | Eval Loss: 0.1349 | Eval R2: -13.8524 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0989 | Eval Loss: 0.1010 | Eval R2: -9.5382 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0814 | Eval Loss: 0.0809 | Eval R2: -7.7944 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0682 | Eval Loss: 0.0670 | Eval R2: -6.7268 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0582 | Eval Loss: 0.0572 | Eval R2: -5.8345 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0513 | Eval Loss: 0.0506 | Eval R2: -5.2568 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0464 | Eval Loss: 0.0457 | Eval R2: -4.8528 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0429 | Eval Loss: 0.0418 | Eval R2: -4.5865 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0401 | Eval Loss: 0.0388 | Eval R2: -4.4244 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0379 | Eval Loss: 0.0366 | Eval R2: -4.3185 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0362 | Eval Loss: 0.0350 | Eval R2: -4.2302 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0349 | Eval Loss: 0.0338 | Eval R2: -4.1476 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0329 | Eval R2: -4.0610 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0328 | Eval Loss: 0.0321 | Eval R2: -3.9698 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0323 | Eval Loss: 0.0315 | Eval R2: -3.8840 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0318 | Eval Loss: 0.0310 | Eval R2: -3.8128 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0305 | Eval R2: -3.7516 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0301 | Eval R2: -3.7023 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0304 | Eval Loss: 0.0297 | Eval R2: -3.6609 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0300 | Eval Loss: 0.0293 | Eval R2: -3.6304 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0296 | Eval Loss: 0.0289 | Eval R2: -3.6087 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0293 | Eval Loss: 0.0286 | Eval R2: -3.5724 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0290 | Eval Loss: 0.0283 | Eval R2: -3.5481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019603, test R2 score: -0.144526
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02895587868988514, 'r2_eval_final': -3.548116683959961, 'loss_eval_final': 0.028283270075917244, 'r2_test': -0.14452561994347413, 'loss_test': 0.01960347220301628, 'loss_nodes': [[0.009625226259231567, 0.015390723012387753, 0.017777705565094948, 0.016343189403414726, 0.017771588638424873, 0.013662372715771198, 0.01801620051264763, 0.020987290889024734, 0.020280852913856506, 0.020015131682157516, 0.017248516902327538, 0.02101333811879158, 0.023830709978938103, 0.023288622498512268, 0.021366503089666367, 0.018385767936706543, 0.02307620644569397, 0.02448929101228714, 0.023289477452635765, 0.026210714131593704]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02896
wandb: loss_eval 0.02828
wandb: loss_test 0.0196
wandb:   r2_eval -3.54812
wandb:   r2_test -0.14453
wandb: 
wandb: ðŸš€ View run happy-terrain-46 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/kirmhrg6
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_190852-kirmhrg6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [10:25:48<20:24:09, 2448.33s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_192832-ew22vtza
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dutiful-forest-47
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ew22vtza

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5287 | Eval Loss: 0.5050 | Eval R2: -54.1751 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3397 | Eval Loss: 0.3351 | Eval R2: -36.4295 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2203 | Eval Loss: 0.2155 | Eval R2: -23.1231 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1419 | Eval Loss: 0.1400 | Eval R2: -14.4518 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0963 | Eval Loss: 0.0939 | Eval R2: -9.3708 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0730 | Eval Loss: 0.0733 | Eval R2: -7.3488 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0617 | Eval Loss: 0.0628 | Eval R2: -6.5014 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0543 | Eval Loss: 0.0549 | Eval R2: -5.8425 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0484 | Eval Loss: 0.0489 | Eval R2: -5.3062 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0441 | Eval Loss: 0.0448 | Eval R2: -4.9704 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0412 | Eval Loss: 0.0423 | Eval R2: -4.7427 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0393 | Eval Loss: 0.0403 | Eval R2: -4.5789 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0376 | Eval Loss: 0.0387 | Eval R2: -4.4424 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0362 | Eval Loss: 0.0373 | Eval R2: -4.3202 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0350 | Eval Loss: 0.0363 | Eval R2: -4.1764 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0342 | Eval Loss: 0.0352 | Eval R2: -4.0612 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0335 | Eval Loss: 0.0343 | Eval R2: -3.9346 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0329 | Eval Loss: 0.0336 | Eval R2: -3.8224 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0323 | Eval Loss: 0.0327 | Eval R2: -3.7285 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0316 | Eval Loss: 0.0317 | Eval R2: -3.6584 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0309 | Eval Loss: 0.0311 | Eval R2: -3.6121 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0304 | Eval Loss: 0.0307 | Eval R2: -3.6147 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0300 | Eval Loss: 0.0307 | Eval R2: -3.6482 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0296 | Eval Loss: 0.0308 | Eval R2: -3.6905 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0293 | Eval Loss: 0.0307 | Eval R2: -3.7218 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0288 | Eval Loss: 0.0306 | Eval R2: -3.7457 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0283 | Eval Loss: 0.0306 | Eval R2: -3.7641 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.021486, test R2 score: -1.054936
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02834721840918064, 'r2_eval_final': -3.764072895050049, 'loss_eval_final': 0.030608713626861572, 'r2_test': -1.0549363250182393, 'loss_test': 0.02148621156811714, 'loss_nodes': [[0.011475966311991215, 0.020785469561815262, 0.01803550310432911, 0.019689058884978294, 0.020705116912722588, 0.014271112158894539, 0.022467581555247307, 0.021066023036837578, 0.019969165325164795, 0.023165352642536163, 0.018453460186719894, 0.026327308267354965, 0.02328554168343544, 0.024257799610495567, 0.023452183231711388, 0.01889391429722309, 0.027492377907037735, 0.024176448583602905, 0.024637816473841667, 0.02711702138185501]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02835
wandb: loss_eval 0.03061
wandb: loss_test 0.02149
wandb:   r2_eval -3.76407
wandb:   r2_test -1.05494
wandb: 
wandb: ðŸš€ View run dutiful-forest-47 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ew22vtza
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_192832-ew22vtza/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [11:25:20<22:26:18, 2785.45s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_202803-kegi8cni
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-oath-48
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/kegi8cni

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8709 | Eval Loss: 0.7559 | Eval R2: -77.5919 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6211 | Eval Loss: 0.7038 | Eval R2: -72.8178 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5798 | Eval Loss: 0.6667 | Eval R2: -68.3862 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5462 | Eval Loss: 0.6259 | Eval R2: -65.0089 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5077 | Eval Loss: 0.5805 | Eval R2: -60.3116 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4681 | Eval Loss: 0.5319 | Eval R2: -55.3211 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4270 | Eval Loss: 0.4826 | Eval R2: -50.3723 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3857 | Eval Loss: 0.4341 | Eval R2: -45.4092 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3456 | Eval Loss: 0.3872 | Eval R2: -40.5687 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3074 | Eval Loss: 0.3424 | Eval R2: -35.9225 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2711 | Eval Loss: 0.3001 | Eval R2: -31.5506 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2372 | Eval Loss: 0.2609 | Eval R2: -27.4627 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2063 | Eval Loss: 0.2253 | Eval R2: -23.7196 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1783 | Eval Loss: 0.1933 | Eval R2: -20.3514 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1535 | Eval Loss: 0.1652 | Eval R2: -17.3829 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1317 | Eval Loss: 0.1410 | Eval R2: -14.8283 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1132 | Eval Loss: 0.1205 | Eval R2: -12.6683 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0976 | Eval Loss: 0.1032 | Eval R2: -10.8589 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0843 | Eval Loss: 0.0887 | Eval R2: -9.3711 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0734 | Eval Loss: 0.0769 | Eval R2: -8.1554 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0646 | Eval Loss: 0.0672 | Eval R2: -7.1718 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0575 | Eval Loss: 0.0596 | Eval R2: -6.3943 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0521 | Eval Loss: 0.0537 | Eval R2: -5.7813 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0478 | Eval Loss: 0.0492 | Eval R2: -5.3281 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0447 | Eval Loss: 0.0457 | Eval R2: -4.9938 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0423 | Eval Loss: 0.0431 | Eval R2: -4.7336 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0405 | Eval Loss: 0.0412 | Eval R2: -4.5431 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.028704, test R2 score: -2.454491
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04051689803600311, 'r2_eval_final': -4.5430521965026855, 'loss_eval_final': 0.04124916344881058, 'r2_test': -2.454490675398765, 'loss_test': 0.028703661635518074, 'loss_nodes': [[0.017314834520220757, 0.017021359875798225, 0.02640366181731224, 0.020444121211767197, 0.022841064259409904, 0.0339786671102047, 0.02112027071416378, 0.0335589163005352, 0.020492659881711006, 0.021727072075009346, 0.049467217177152634, 0.03125643730163574, 0.022989194840192795, 0.0335891954600811, 0.022533755749464035, 0.04307997226715088, 0.02364896610379219, 0.03127053752541542, 0.02324608899652958, 0.058089252561330795]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04052
wandb: loss_eval 0.04125
wandb: loss_test 0.0287
wandb:   r2_eval -4.54305
wandb:   r2_test -2.45449
wandb: 
wandb: ðŸš€ View run pretty-oath-48 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/kegi8cni
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_202803-kegi8cni/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [12:31:25<24:25:09, 3139.61s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_213409-0r5ewptd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-water-49
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/0r5ewptd

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7917 | Eval Loss: 0.5164 | Eval R2: -53.5589 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3822 | Eval Loss: 0.4184 | Eval R2: -44.8344 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3060 | Eval Loss: 0.3466 | Eval R2: -37.8931 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2535 | Eval Loss: 0.2960 | Eval R2: -32.6169 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2178 | Eval Loss: 0.2604 | Eval R2: -28.7501 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1931 | Eval Loss: 0.2354 | Eval R2: -26.0067 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1761 | Eval Loss: 0.2180 | Eval R2: -24.0868 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1645 | Eval Loss: 0.2061 | Eval R2: -22.7332 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1570 | Eval Loss: 0.1983 | Eval R2: -21.7787 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1525 | Eval Loss: 0.1934 | Eval R2: -21.1246 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1499 | Eval Loss: 0.1905 | Eval R2: -20.6994 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1487 | Eval Loss: 0.1889 | Eval R2: -20.4384 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1481 | Eval Loss: 0.1881 | Eval R2: -20.2860 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1479 | Eval Loss: 0.1877 | Eval R2: -20.2007 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1478 | Eval Loss: 0.1875 | Eval R2: -20.1549 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1317 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1209 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1166 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1152 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1147 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1143 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1138 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1132 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1124 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1116 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1108 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1100 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.111834, test R2 score: -38.356727
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.147806778550148, 'r2_eval_final': -20.109994888305664, 'loss_eval_final': 0.18730062246322632, 'r2_test': -38.35672688770183, 'loss_test': 0.11183405667543411, 'loss_nodes': [[0.10658377408981323, 0.10900574177503586, 0.1168871745467186, 0.111124686896801, 0.10906130820512772, 0.10847725719213486, 0.11053680628538132, 0.11728990077972412, 0.11200691014528275, 0.1120700091123581, 0.10925966501235962, 0.11114026606082916, 0.11983019858598709, 0.11205482482910156, 0.10959266126155853, 0.10940238833427429, 0.11147241294384003, 0.11789589375257492, 0.11173909157514572, 0.11125024408102036]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.14781
wandb: loss_eval 0.1873
wandb: loss_test 0.11183
wandb:   r2_eval -20.10999
wandb:   r2_test -38.35673
wandb: 
wandb: ðŸš€ View run zesty-water-49 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/0r5ewptd
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_213409-0r5ewptd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [12:51:59<19:15:25, 2567.61s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_215442-0fequjs9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-grass-50
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/0fequjs9

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5741 | Eval Loss: 0.5225 | Eval R2: -57.3303 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3868 | Eval Loss: 0.4383 | Eval R2: -49.5668 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3236 | Eval Loss: 0.3733 | Eval R2: -43.0461 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2733 | Eval Loss: 0.3192 | Eval R2: -37.1185 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2315 | Eval Loss: 0.2726 | Eval R2: -31.7443 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1968 | Eval Loss: 0.2341 | Eval R2: -27.2172 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1694 | Eval Loss: 0.2041 | Eval R2: -23.6055 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1483 | Eval Loss: 0.1795 | Eval R2: -20.5418 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1306 | Eval Loss: 0.1572 | Eval R2: -17.8609 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1139 | Eval Loss: 0.1361 | Eval R2: -15.3858 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0984 | Eval Loss: 0.1169 | Eval R2: -13.0903 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0851 | Eval Loss: 0.1000 | Eval R2: -11.1211 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0741 | Eval Loss: 0.0857 | Eval R2: -9.5626 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0650 | Eval Loss: 0.0742 | Eval R2: -8.3767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0580 | Eval Loss: 0.0649 | Eval R2: -7.3936 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0524 | Eval Loss: 0.0577 | Eval R2: -6.6891 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0479 | Eval Loss: 0.0519 | Eval R2: -6.0665 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0444 | Eval Loss: 0.0473 | Eval R2: -5.5641 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0419 | Eval Loss: 0.0438 | Eval R2: -5.1925 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0397 | Eval Loss: 0.0409 | Eval R2: -4.8666 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0377 | Eval Loss: 0.0388 | Eval R2: -4.6025 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0368 | Eval Loss: 0.0372 | Eval R2: -4.3710 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0357 | Eval Loss: 0.0360 | Eval R2: -4.1726 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0347 | Eval Loss: 0.0354 | Eval R2: -4.0478 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0342 | Eval Loss: 0.0356 | Eval R2: -3.8662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0337 | Eval Loss: 0.0354 | Eval R2: -3.7801 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0334 | Eval Loss: 0.0353 | Eval R2: -3.7221 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.024171, test R2 score: -0.827673
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.03337935730814934, 'r2_eval_final': -3.722141981124878, 'loss_eval_final': 0.03527617081999779, 'r2_test': -0.8276727665762393, 'loss_test': 0.02417088858783245, 'loss_nodes': [[0.014262469485402107, 0.020603522658348083, 0.025897707790136337, 0.022975381463766098, 0.02042223885655403, 0.01804378256201744, 0.020759152248501778, 0.0254078172147274, 0.02259194850921631, 0.02487015351653099, 0.021289037540555, 0.023281671106815338, 0.03190970420837402, 0.027730779722332954, 0.027327505871653557, 0.021040646359324455, 0.02774774469435215, 0.029443811625242233, 0.02695028856396675, 0.03086235374212265]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03338
wandb: loss_eval 0.03528
wandb: loss_test 0.02417
wandb:   r2_eval -3.72214
wandb:   r2_test -0.82767
wandb: 
wandb: ðŸš€ View run sparkling-grass-50 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/0fequjs9
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_215442-0fequjs9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [13:14:47<15:56:45, 2207.92s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_221731-ftq2asxi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-firefly-51
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ftq2asxi

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8709 | Eval Loss: 0.7559 | Eval R2: -77.5919 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6211 | Eval Loss: 0.7038 | Eval R2: -72.8178 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5798 | Eval Loss: 0.6667 | Eval R2: -68.3862 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5462 | Eval Loss: 0.6259 | Eval R2: -65.0089 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5077 | Eval Loss: 0.5805 | Eval R2: -60.3116 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4681 | Eval Loss: 0.5319 | Eval R2: -55.3211 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4270 | Eval Loss: 0.4826 | Eval R2: -50.3723 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3857 | Eval Loss: 0.4341 | Eval R2: -45.4092 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3456 | Eval Loss: 0.3872 | Eval R2: -40.5687 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3074 | Eval Loss: 0.3424 | Eval R2: -35.9225 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2711 | Eval Loss: 0.3001 | Eval R2: -31.5506 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2372 | Eval Loss: 0.2609 | Eval R2: -27.4627 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2063 | Eval Loss: 0.2253 | Eval R2: -23.7196 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1783 | Eval Loss: 0.1933 | Eval R2: -20.3514 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1535 | Eval Loss: 0.1652 | Eval R2: -17.3829 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1317 | Eval Loss: 0.1410 | Eval R2: -14.8283 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1132 | Eval Loss: 0.1205 | Eval R2: -12.6683 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0976 | Eval Loss: 0.1032 | Eval R2: -10.8589 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0843 | Eval Loss: 0.0887 | Eval R2: -9.3711 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0734 | Eval Loss: 0.0769 | Eval R2: -8.1554 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0646 | Eval Loss: 0.0672 | Eval R2: -7.1718 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0575 | Eval Loss: 0.0596 | Eval R2: -6.3943 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0521 | Eval Loss: 0.0537 | Eval R2: -5.7813 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0478 | Eval Loss: 0.0492 | Eval R2: -5.3281 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0447 | Eval Loss: 0.0457 | Eval R2: -4.9938 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0423 | Eval Loss: 0.0431 | Eval R2: -4.7336 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0405 | Eval Loss: 0.0412 | Eval R2: -4.5431 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.028704, test R2 score: -2.454491
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04051689803600311, 'r2_eval_final': -4.5430521965026855, 'loss_eval_final': 0.04124916344881058, 'r2_test': -2.454490675398765, 'loss_test': 0.028703661635518074, 'loss_nodes': [[0.017314834520220757, 0.017021359875798225, 0.02640366181731224, 0.020444121211767197, 0.022841064259409904, 0.0339786671102047, 0.02112027071416378, 0.0335589163005352, 0.020492659881711006, 0.021727072075009346, 0.049467217177152634, 0.03125643730163574, 0.022989194840192795, 0.0335891954600811, 0.022533755749464035, 0.04307997226715088, 0.02364896610379219, 0.03127053752541542, 0.02324608899652958, 0.058089252561330795]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.005 MB of 0.009 MB uploadedwandb: / 0.005 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04052
wandb: loss_eval 0.04125
wandb: loss_test 0.0287
wandb:   r2_eval -4.54305
wandb:   r2_test -2.45449
wandb: 
wandb: ðŸš€ View run curious-firefly-51 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ftq2asxi
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_221731-ftq2asxi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [14:21:23<19:03:28, 2744.35s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_232407-y70iyhr8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run apricot-darkness-52
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/y70iyhr8

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9406 | Eval Loss: 0.7177 | Eval R2: -69.0260 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5665 | Eval Loss: 0.6025 | Eval R2: -61.0530 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4780 | Eval Loss: 0.5325 | Eval R2: -56.1126 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4211 | Eval Loss: 0.4851 | Eval R2: -52.5185 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3819 | Eval Loss: 0.4495 | Eval R2: -49.5841 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3509 | Eval Loss: 0.4173 | Eval R2: -46.5507 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3231 | Eval Loss: 0.3845 | Eval R2: -43.1084 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2960 | Eval Loss: 0.3521 | Eval R2: -39.4927 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2692 | Eval Loss: 0.3195 | Eval R2: -35.7007 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2427 | Eval Loss: 0.2867 | Eval R2: -31.8565 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2167 | Eval Loss: 0.2537 | Eval R2: -27.9908 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1911 | Eval Loss: 0.2212 | Eval R2: -24.2244 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1661 | Eval Loss: 0.1900 | Eval R2: -20.6477 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1430 | Eval Loss: 0.1614 | Eval R2: -17.4101 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1227 | Eval Loss: 0.1366 | Eval R2: -14.6335 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1054 | Eval Loss: 0.1157 | Eval R2: -12.3548 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0910 | Eval Loss: 0.0986 | Eval R2: -10.5112 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0795 | Eval Loss: 0.0851 | Eval R2: -9.0514 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0705 | Eval Loss: 0.0746 | Eval R2: -7.9315 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0636 | Eval Loss: 0.0665 | Eval R2: -7.0611 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0582 | Eval Loss: 0.0601 | Eval R2: -6.4148 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0537 | Eval Loss: 0.0550 | Eval R2: -5.9043 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0504 | Eval Loss: 0.0509 | Eval R2: -5.5040 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0475 | Eval Loss: 0.0474 | Eval R2: -5.1943 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0445 | Eval R2: -4.9677 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0431 | Eval Loss: 0.0423 | Eval R2: -4.7716 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0412 | Eval Loss: 0.0404 | Eval R2: -4.6258 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.027672, test R2 score: -1.808903
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.041193388402462006, 'r2_eval_final': -4.625755310058594, 'loss_eval_final': 0.04035433754324913, 'r2_test': -1.808903404679062, 'loss_test': 0.02767224609851837, 'loss_nodes': [[0.013202655129134655, 0.01554956380277872, 0.03150223195552826, 0.034704551100730896, 0.019489217549562454, 0.03005487471818924, 0.018383193761110306, 0.025080516934394836, 0.019748760387301445, 0.021197233349084854, 0.019077656790614128, 0.022572167217731476, 0.04447653517127037, 0.04018567502498627, 0.022073620930314064, 0.02564123459160328, 0.027177829295396805, 0.02321942150592804, 0.06678865849971771, 0.033319320529699326]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.002 MB of 0.003 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04119
wandb: loss_eval 0.04035
wandb: loss_test 0.02767
wandb:   r2_eval -4.62576
wandb:   r2_test -1.8089
wandb: 
wandb: ðŸš€ View run apricot-darkness-52 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/y70iyhr8
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_232407-y70iyhr8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [14:39:01<14:55:19, 2238.30s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_234145-q2blvdpg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-energy-53
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/q2blvdpg

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5301 | Eval Loss: 0.5246 | Eval R2: -56.6961 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3775 | Eval Loss: 0.3985 | Eval R2: -44.5523 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2775 | Eval Loss: 0.2930 | Eval R2: -32.9210 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1954 | Eval Loss: 0.1999 | Eval R2: -22.2212 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1323 | Eval Loss: 0.1349 | Eval R2: -13.8524 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0989 | Eval Loss: 0.1010 | Eval R2: -9.5382 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0814 | Eval Loss: 0.0809 | Eval R2: -7.7944 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0682 | Eval Loss: 0.0670 | Eval R2: -6.7268 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0582 | Eval Loss: 0.0572 | Eval R2: -5.8345 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0513 | Eval Loss: 0.0506 | Eval R2: -5.2568 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0464 | Eval Loss: 0.0457 | Eval R2: -4.8528 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0429 | Eval Loss: 0.0418 | Eval R2: -4.5865 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0401 | Eval Loss: 0.0388 | Eval R2: -4.4244 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0379 | Eval Loss: 0.0366 | Eval R2: -4.3185 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0362 | Eval Loss: 0.0350 | Eval R2: -4.2302 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0349 | Eval Loss: 0.0338 | Eval R2: -4.1476 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0340 | Eval Loss: 0.0329 | Eval R2: -4.0610 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0328 | Eval Loss: 0.0321 | Eval R2: -3.9698 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0323 | Eval Loss: 0.0315 | Eval R2: -3.8840 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0318 | Eval Loss: 0.0310 | Eval R2: -3.8128 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0313 | Eval Loss: 0.0305 | Eval R2: -3.7516 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0301 | Eval R2: -3.7023 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0304 | Eval Loss: 0.0297 | Eval R2: -3.6609 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0300 | Eval Loss: 0.0293 | Eval R2: -3.6304 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0296 | Eval Loss: 0.0289 | Eval R2: -3.6087 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0293 | Eval Loss: 0.0286 | Eval R2: -3.5724 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0290 | Eval Loss: 0.0283 | Eval R2: -3.5481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019603, test R2 score: -0.144526
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.02895587868988514, 'r2_eval_final': -3.548116683959961, 'loss_eval_final': 0.028283270075917244, 'r2_test': -0.14452561994347413, 'loss_test': 0.01960347220301628, 'loss_nodes': [[0.009625226259231567, 0.015390723012387753, 0.017777705565094948, 0.016343189403414726, 0.017771588638424873, 0.013662372715771198, 0.01801620051264763, 0.020987290889024734, 0.020280852913856506, 0.020015131682157516, 0.017248516902327538, 0.02101333811879158, 0.023830709978938103, 0.023288622498512268, 0.021366503089666367, 0.018385767936706543, 0.02307620644569397, 0.02448929101228714, 0.023289477452635765, 0.026210714131593704]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02896
wandb: loss_eval 0.02828
wandb: loss_test 0.0196
wandb:   r2_eval -3.54812
wandb:   r2_test -0.14453
wandb: 
wandb: ðŸš€ View run flowing-energy-53 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/q2blvdpg
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_234145-q2blvdpg/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [14:57:00<12:04:41, 1890.48s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_235944-0n7u8twz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-dawn-54
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/0n7u8twz

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5251 | Eval Loss: 0.6035 | Eval R2: -63.6341 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4855 | Eval Loss: 0.5644 | Eval R2: -59.7723 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4502 | Eval Loss: 0.5228 | Eval R2: -55.6781 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4132 | Eval Loss: 0.4792 | Eval R2: -51.3556 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3757 | Eval Loss: 0.4346 | Eval R2: -46.7837 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3388 | Eval Loss: 0.3909 | Eval R2: -42.2176 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3039 | Eval Loss: 0.3495 | Eval R2: -37.8841 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2714 | Eval Loss: 0.3111 | Eval R2: -33.8559 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2417 | Eval Loss: 0.2764 | Eval R2: -30.2012 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2151 | Eval Loss: 0.2456 | Eval R2: -26.9148 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1917 | Eval Loss: 0.2185 | Eval R2: -23.9648 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1710 | Eval Loss: 0.1944 | Eval R2: -21.3302 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1529 | Eval Loss: 0.1731 | Eval R2: -18.9586 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1368 | Eval Loss: 0.1543 | Eval R2: -16.8485 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1226 | Eval Loss: 0.1376 | Eval R2: -14.9457 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1101 | Eval Loss: 0.1232 | Eval R2: -13.2301 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0993 | Eval Loss: 0.1108 | Eval R2: -11.7320 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0897 | Eval Loss: 0.1000 | Eval R2: -10.4406 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0813 | Eval Loss: 0.0906 | Eval R2: -9.3571 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0741 | Eval Loss: 0.0822 | Eval R2: -8.4183 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0678 | Eval Loss: 0.0748 | Eval R2: -7.6392 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0627 | Eval Loss: 0.0687 | Eval R2: -7.0003 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0586 | Eval Loss: 0.0635 | Eval R2: -6.4822 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0551 | Eval Loss: 0.0596 | Eval R2: -6.0851 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0523 | Eval Loss: 0.0565 | Eval R2: -5.7457 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0502 | Eval Loss: 0.0544 | Eval R2: -5.4792 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0487 | Eval Loss: 0.0525 | Eval R2: -5.2597 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.037938, test R2 score: -3.797783
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04866040125489235, 'r2_eval_final': -5.259693622589111, 'loss_eval_final': 0.052483875304460526, 'r2_test': -3.7977831499818624, 'loss_test': 0.037938255816698074, 'loss_nodes': [[0.011310283094644547, 0.02363470382988453, 0.05754337087273598, 0.019218984991312027, 0.03563186898827553, 0.017262788489460945, 0.020658180117607117, 0.0986252874135971, 0.04928087815642357, 0.061036355793476105, 0.03840510919690132, 0.04319631680846214, 0.05812251940369606, 0.025751300156116486, 0.02346716821193695, 0.04249831661581993, 0.03319675475358963, 0.025928594172000885, 0.027647702023386955, 0.04634864628314972]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04866
wandb: loss_eval 0.05248
wandb: loss_test 0.03794
wandb:   r2_eval -5.25969
wandb:   r2_test -3.79778
wandb: 
wandb: ðŸš€ View run dainty-dawn-54 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/0n7u8twz
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_235944-0n7u8twz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [15:16:00<10:10:35, 1665.26s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_001843-ou0kv7v4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run pretty-durian-55
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ou0kv7v4

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6322 | Eval Loss: 1.4431 | Eval R2: -121.8103 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2103 | Eval Loss: 1.2183 | Eval R2: -106.5182 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.0115 | Eval Loss: 1.0375 | Eval R2: -93.5331 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.8585 | Eval Loss: 0.9000 | Eval R2: -83.3674 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.7424 | Eval Loss: 0.7942 | Eval R2: -75.3603 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.6526 | Eval Loss: 0.7106 | Eval R2: -68.9137 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5815 | Eval Loss: 0.6433 | Eval R2: -63.6178 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.5240 | Eval Loss: 0.5879 | Eval R2: -59.1703 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4766 | Eval Loss: 0.5417 | Eval R2: -55.3686 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4372 | Eval Loss: 0.5024 | Eval R2: -52.0609 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4035 | Eval Loss: 0.4681 | Eval R2: -49.1060 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3741 | Eval Loss: 0.4378 | Eval R2: -46.4257 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3482 | Eval Loss: 0.4106 | Eval R2: -43.9608 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3251 | Eval Loss: 0.3860 | Eval R2: -41.6767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3042 | Eval Loss: 0.3635 | Eval R2: -39.5522 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2853 | Eval Loss: 0.3429 | Eval R2: -37.5664 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2679 | Eval Loss: 0.3239 | Eval R2: -35.7048 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2521 | Eval Loss: 0.3065 | Eval R2: -33.9607 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2377 | Eval Loss: 0.2905 | Eval R2: -32.3328 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.2247 | Eval Loss: 0.2760 | Eval R2: -30.8224 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.2129 | Eval Loss: 0.2629 | Eval R2: -29.4312 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.2024 | Eval Loss: 0.2512 | Eval R2: -28.1605 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1932 | Eval Loss: 0.2408 | Eval R2: -27.0108 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1852 | Eval Loss: 0.2318 | Eval R2: -25.9812 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1783 | Eval Loss: 0.2239 | Eval R2: -25.0689 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1724 | Eval Loss: 0.2173 | Eval R2: -24.2696 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1676 | Eval Loss: 0.2117 | Eval R2: -23.5770 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.127625, test R2 score: -49.634858
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.1675509512424469, 'r2_eval_final': -23.577024459838867, 'loss_eval_final': 0.21165750920772552, 'r2_test': -49.63485817873419, 'loss_test': 0.1276254504919052, 'loss_nodes': [[0.17279943823814392, 0.10889368504285812, 0.11788100004196167, 0.11727610975503922, 0.2720925807952881, 0.10808441787958145, 0.11373510211706161, 0.11658327281475067, 0.11251711845397949, 0.11280911415815353, 0.10963527858257294, 0.11113720387220383, 0.1181657612323761, 0.15471269190311432, 0.10962535440921783, 0.11082317680120468, 0.1180385947227478, 0.14368577301502228, 0.11213406175374985, 0.11187942326068878]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.005 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.16755
wandb: loss_eval 0.21166
wandb: loss_test 0.12763
wandb:   r2_eval -23.57702
wandb:   r2_test -49.63486
wandb: 
wandb: ðŸš€ View run pretty-durian-55 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ou0kv7v4
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_001843-ou0kv7v4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [15:34:31<8:44:43, 1499.23s/it] Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_003715-yu4980lj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run colorful-paper-56
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/yu4980lj

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6763 | Eval Loss: 0.6243 | Eval R2: -59.9298 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5073 | Eval Loss: 0.5522 | Eval R2: -54.4522 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4438 | Eval Loss: 0.4915 | Eval R2: -49.5612 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3911 | Eval Loss: 0.4390 | Eval R2: -45.0494 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3460 | Eval Loss: 0.3932 | Eval R2: -40.8861 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3077 | Eval Loss: 0.3536 | Eval R2: -37.1289 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2753 | Eval Loss: 0.3198 | Eval R2: -33.8068 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2483 | Eval Loss: 0.2914 | Eval R2: -30.9293 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2259 | Eval Loss: 0.2679 | Eval R2: -28.4905 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2078 | Eval Loss: 0.2488 | Eval R2: -26.4732 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1932 | Eval Loss: 0.2334 | Eval R2: -24.8467 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1817 | Eval Loss: 0.2214 | Eval R2: -23.5660 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1728 | Eval Loss: 0.2122 | Eval R2: -22.5798 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1660 | Eval Loss: 0.2051 | Eval R2: -21.8380 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1608 | Eval Loss: 0.1999 | Eval R2: -21.2946 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1570 | Eval Loss: 0.1960 | Eval R2: -20.9076 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1542 | Eval Loss: 0.1933 | Eval R2: -20.6390 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1521 | Eval Loss: 0.1913 | Eval R2: -20.4567 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1507 | Eval Loss: 0.1900 | Eval R2: -20.3355 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1497 | Eval Loss: 0.1891 | Eval R2: -20.2564 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1490 | Eval Loss: 0.1885 | Eval R2: -20.2059 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1486 | Eval Loss: 0.1881 | Eval R2: -20.1745 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1483 | Eval Loss: 0.1878 | Eval R2: -20.1556 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1481 | Eval Loss: 0.1877 | Eval R2: -20.1448 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1480 | Eval Loss: 0.1876 | Eval R2: -20.1391 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1479 | Eval Loss: 0.1875 | Eval R2: -20.1365 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1478 | Eval Loss: 0.1875 | Eval R2: -20.1357 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.111854, test R2 score: -38.445755
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.14784418046474457, 'r2_eval_final': -20.135665893554688, 'loss_eval_final': 0.1874518245458603, 'r2_test': -38.44575455570788, 'loss_test': 0.11185360699892044, 'loss_nodes': [[0.10646530240774155, 0.1090187132358551, 0.1168610230088234, 0.11113505065441132, 0.10904613882303238, 0.10864491015672684, 0.11052116006612778, 0.11730612814426422, 0.11200553923845291, 0.11206074804067612, 0.10926749557256699, 0.1111818477511406, 0.11995711922645569, 0.11203429102897644, 0.10961897671222687, 0.10937025398015976, 0.11143827438354492, 0.1178245097398758, 0.11203797906637192, 0.11127680540084839]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.14784
wandb: loss_eval 0.18745
wandb: loss_test 0.11185
wandb:   r2_eval -20.13567
wandb:   r2_test -38.44575
wandb: 
wandb: ðŸš€ View run colorful-paper-56 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/yu4980lj
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_003715-yu4980lj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [15:46:08<6:59:26, 1258.33s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_004852-3vhax0ne
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-breeze-57
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/3vhax0ne

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8398 | Eval Loss: 0.6838 | Eval R2: -65.8188 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5368 | Eval Loss: 0.5487 | Eval R2: -54.9755 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4192 | Eval Loss: 0.4329 | Eval R2: -45.0527 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3157 | Eval Loss: 0.3236 | Eval R2: -34.4568 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2311 | Eval Loss: 0.2364 | Eval R2: -25.4943 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1684 | Eval Loss: 0.1723 | Eval R2: -18.8661 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1247 | Eval Loss: 0.1266 | Eval R2: -13.9193 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0957 | Eval Loss: 0.0968 | Eval R2: -10.7463 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0759 | Eval Loss: 0.0763 | Eval R2: -8.4971 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0625 | Eval Loss: 0.0612 | Eval R2: -6.8232 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0530 | Eval Loss: 0.0506 | Eval R2: -5.6913 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0465 | Eval Loss: 0.0444 | Eval R2: -5.0814 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0426 | Eval Loss: 0.0408 | Eval R2: -4.7458 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0400 | Eval Loss: 0.0386 | Eval R2: -4.5528 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0382 | Eval Loss: 0.0370 | Eval R2: -4.4023 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0369 | Eval Loss: 0.0358 | Eval R2: -4.2877 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0359 | Eval Loss: 0.0348 | Eval R2: -4.2120 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0350 | Eval Loss: 0.0340 | Eval R2: -4.1628 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0342 | Eval Loss: 0.0332 | Eval R2: -4.0972 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0336 | Eval Loss: 0.0326 | Eval R2: -4.0323 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0330 | Eval Loss: 0.0320 | Eval R2: -3.9808 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0324 | Eval Loss: 0.0315 | Eval R2: -3.9149 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0320 | Eval Loss: 0.0310 | Eval R2: -3.8945 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0315 | Eval Loss: 0.0306 | Eval R2: -3.8409 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0311 | Eval Loss: 0.0302 | Eval R2: -3.8012 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0307 | Eval Loss: 0.0298 | Eval R2: -3.7776 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0303 | Eval Loss: 0.0295 | Eval R2: -3.7453 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020797, test R2 score: -0.312066
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.030277548357844353, 'r2_eval_final': -3.7453453540802, 'loss_eval_final': 0.029463376849889755, 'r2_test': -0.31206601212804225, 'loss_test': 0.02079673856496811, 'loss_nodes': [[0.009439397603273392, 0.015617665834724903, 0.019613374024629593, 0.02118675969541073, 0.01702275685966015, 0.020523084327578545, 0.016607096418738365, 0.02462189644575119, 0.022398004308342934, 0.021641450002789497, 0.015785641968250275, 0.02323700487613678, 0.021636750549077988, 0.02256280742585659, 0.02284621074795723, 0.020628957077860832, 0.029161712154746056, 0.022419724613428116, 0.02352912351489067, 0.025455420836806297]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03028
wandb: loss_eval 0.02946
wandb: loss_test 0.0208
wandb:   r2_eval -3.74535
wandb:   r2_test -0.31207
wandb: 
wandb: ðŸš€ View run blooming-breeze-57 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/3vhax0ne
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_004852-3vhax0ne/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [16:49:47<10:41:47, 2026.69s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_015231-92iaqs9b
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earnest-thunder-58
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/92iaqs9b

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7351 | Eval Loss: 0.7477 | Eval R2: -72.4240 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5773 | Eval Loss: 0.6098 | Eval R2: -61.0606 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4557 | Eval Loss: 0.4734 | Eval R2: -48.0780 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3390 | Eval Loss: 0.3420 | Eval R2: -35.6685 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2315 | Eval Loss: 0.2273 | Eval R2: -24.2735 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1499 | Eval Loss: 0.1491 | Eval R2: -16.2706 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1019 | Eval Loss: 0.1055 | Eval R2: -11.5468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0772 | Eval Loss: 0.0816 | Eval R2: -9.0650 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0633 | Eval Loss: 0.0681 | Eval R2: -7.6457 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0559 | Eval Loss: 0.0603 | Eval R2: -6.8193 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0514 | Eval Loss: 0.0552 | Eval R2: -6.2358 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0486 | Eval Loss: 0.0515 | Eval R2: -5.8382 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0465 | Eval Loss: 0.0486 | Eval R2: -5.5224 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0444 | Eval Loss: 0.0461 | Eval R2: -5.2871 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0428 | Eval Loss: 0.0437 | Eval R2: -5.0974 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0411 | Eval Loss: 0.0416 | Eval R2: -4.9007 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0398 | Eval Loss: 0.0396 | Eval R2: -4.7099 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0386 | Eval Loss: 0.0380 | Eval R2: -4.5496 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0375 | Eval Loss: 0.0368 | Eval R2: -4.4252 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0368 | Eval Loss: 0.0357 | Eval R2: -4.3562 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0360 | Eval Loss: 0.0349 | Eval R2: -4.2814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0352 | Eval Loss: 0.0341 | Eval R2: -4.2111 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0345 | Eval Loss: 0.0334 | Eval R2: -4.1464 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0338 | Eval Loss: 0.0327 | Eval R2: -4.1091 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0331 | Eval Loss: 0.0321 | Eval R2: -4.0752 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0325 | Eval Loss: 0.0315 | Eval R2: -3.9971 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0321 | Eval Loss: 0.0309 | Eval R2: -3.9364 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020304, test R2 score: -0.279949
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.03208351135253906, 'r2_eval_final': -3.9363527297973633, 'loss_eval_final': 0.030945731326937675, 'r2_test': -0.27994904246294144, 'loss_test': 0.02030363865196705, 'loss_nodes': [[0.010641773231327534, 0.015358714386820793, 0.018952883780002594, 0.022002313286066055, 0.015402951277792454, 0.011733561754226685, 0.01584458351135254, 0.022059109061956406, 0.01984305866062641, 0.018676603212952614, 0.01573602855205536, 0.02302725799381733, 0.03147648647427559, 0.02826499566435814, 0.019888879731297493, 0.016946876421570778, 0.024437211453914642, 0.023501131683588028, 0.024166306480765343, 0.028111999854445457]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03208
wandb: loss_eval 0.03095
wandb: loss_test 0.0203
wandb:   r2_eval -3.93635
wandb:   r2_test -0.27995
wandb: 
wandb: ðŸš€ View run earnest-thunder-58 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/92iaqs9b
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_015231-92iaqs9b/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [17:03:59<8:22:16, 1674.22s/it] Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_020643-rw5i58kl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-pond-59
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/rw5i58kl

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7055 | Eval Loss: 0.6023 | Eval R2: -59.6218 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4702 | Eval Loss: 0.5132 | Eval R2: -52.5423 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3990 | Eval Loss: 0.4439 | Eval R2: -46.0484 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3417 | Eval Loss: 0.3827 | Eval R2: -40.2940 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2910 | Eval Loss: 0.3253 | Eval R2: -34.4739 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2445 | Eval Loss: 0.2705 | Eval R2: -28.4509 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2033 | Eval Loss: 0.2229 | Eval R2: -23.4031 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1682 | Eval Loss: 0.1830 | Eval R2: -19.0638 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1398 | Eval Loss: 0.1511 | Eval R2: -15.6819 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1170 | Eval Loss: 0.1259 | Eval R2: -13.0231 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0992 | Eval Loss: 0.1063 | Eval R2: -11.0135 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0853 | Eval Loss: 0.0915 | Eval R2: -9.5282 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0749 | Eval Loss: 0.0806 | Eval R2: -8.4588 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0672 | Eval Loss: 0.0725 | Eval R2: -7.6801 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0620 | Eval Loss: 0.0664 | Eval R2: -7.1304 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0577 | Eval Loss: 0.0616 | Eval R2: -6.7000 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0545 | Eval Loss: 0.0576 | Eval R2: -6.3527 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0517 | Eval Loss: 0.0541 | Eval R2: -6.0590 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0490 | Eval Loss: 0.0511 | Eval R2: -5.7889 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0470 | Eval Loss: 0.0485 | Eval R2: -5.5751 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0451 | Eval Loss: 0.0462 | Eval R2: -5.3756 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0434 | Eval Loss: 0.0442 | Eval R2: -5.2055 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0417 | Eval Loss: 0.0425 | Eval R2: -5.0715 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0404 | Eval Loss: 0.0410 | Eval R2: -4.9635 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0395 | Eval Loss: 0.0397 | Eval R2: -4.8321 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0384 | Eval Loss: 0.0386 | Eval R2: -4.7664 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0375 | Eval Loss: 0.0376 | Eval R2: -4.6666 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.025540, test R2 score: -0.960499
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.037513233721256256, 'r2_eval_final': -4.6665520668029785, 'loss_eval_final': 0.037558041512966156, 'r2_test': -0.9604990993381751, 'loss_test': 0.025539809837937355, 'loss_nodes': [[0.012147740460932255, 0.020867373794317245, 0.02301986888051033, 0.023715436458587646, 0.025925952941179276, 0.018345696851611137, 0.021468793973326683, 0.03493143618106842, 0.021335752680897713, 0.02169005200266838, 0.027486927807331085, 0.029675425961613655, 0.03318306431174278, 0.026573698967695236, 0.02275596372783184, 0.02203938737511635, 0.026707060635089874, 0.03412370756268501, 0.025139804929494858, 0.03966313228011131]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03751
wandb: loss_eval 0.03756
wandb: loss_test 0.02554
wandb:   r2_eval -4.66655
wandb:   r2_test -0.9605
wandb: 
wandb: ðŸš€ View run silvery-pond-59 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/rw5i58kl
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_020643-rw5i58kl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [17:20:48<6:57:48, 1474.60s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_022332-4z58c855
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dauntless-sponge-60
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/4z58c855

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5741 | Eval Loss: 0.5225 | Eval R2: -57.3303 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3868 | Eval Loss: 0.4383 | Eval R2: -49.5668 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3236 | Eval Loss: 0.3733 | Eval R2: -43.0461 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2733 | Eval Loss: 0.3192 | Eval R2: -37.1185 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2315 | Eval Loss: 0.2726 | Eval R2: -31.7443 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1968 | Eval Loss: 0.2341 | Eval R2: -27.2172 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1694 | Eval Loss: 0.2041 | Eval R2: -23.6055 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1483 | Eval Loss: 0.1795 | Eval R2: -20.5418 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1306 | Eval Loss: 0.1572 | Eval R2: -17.8609 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1139 | Eval Loss: 0.1361 | Eval R2: -15.3858 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0984 | Eval Loss: 0.1169 | Eval R2: -13.0903 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0851 | Eval Loss: 0.1000 | Eval R2: -11.1211 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0741 | Eval Loss: 0.0857 | Eval R2: -9.5626 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0650 | Eval Loss: 0.0742 | Eval R2: -8.3767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0580 | Eval Loss: 0.0649 | Eval R2: -7.3936 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0524 | Eval Loss: 0.0577 | Eval R2: -6.6891 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0479 | Eval Loss: 0.0519 | Eval R2: -6.0665 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0444 | Eval Loss: 0.0473 | Eval R2: -5.5641 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0419 | Eval Loss: 0.0438 | Eval R2: -5.1925 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0397 | Eval Loss: 0.0409 | Eval R2: -4.8666 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0377 | Eval Loss: 0.0388 | Eval R2: -4.6025 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0368 | Eval Loss: 0.0372 | Eval R2: -4.3710 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0357 | Eval Loss: 0.0360 | Eval R2: -4.1726 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0347 | Eval Loss: 0.0354 | Eval R2: -4.0478 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0342 | Eval Loss: 0.0356 | Eval R2: -3.8662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0337 | Eval Loss: 0.0354 | Eval R2: -3.7801 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0334 | Eval Loss: 0.0353 | Eval R2: -3.7221 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.024171, test R2 score: -0.827673
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.03337935730814934, 'r2_eval_final': -3.722141981124878, 'loss_eval_final': 0.03527617081999779, 'r2_test': -0.8276727665762393, 'loss_test': 0.02417088858783245, 'loss_nodes': [[0.014262469485402107, 0.020603522658348083, 0.025897707790136337, 0.022975381463766098, 0.02042223885655403, 0.01804378256201744, 0.020759152248501778, 0.0254078172147274, 0.02259194850921631, 0.02487015351653099, 0.021289037540555, 0.023281671106815338, 0.03190970420837402, 0.027730779722332954, 0.027327505871653557, 0.021040646359324455, 0.02774774469435215, 0.029443811625242233, 0.02695028856396675, 0.03086235374212265]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03338
wandb: loss_eval 0.03528
wandb: loss_test 0.02417
wandb:   r2_eval -3.72214
wandb:   r2_test -0.82767
wandb: 
wandb: ðŸš€ View run dauntless-sponge-60 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/4z58c855
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_022332-4z58c855/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [17:44:05<6:27:00, 1451.27s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_024649-opseg8xp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-plant-61
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/opseg8xp

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9979 | Eval Loss: 0.8213 | Eval R2: -80.6152 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6678 | Eval Loss: 0.7452 | Eval R2: -75.0460 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6065 | Eval Loss: 0.6908 | Eval R2: -70.6658 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5597 | Eval Loss: 0.6441 | Eval R2: -66.7606 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5198 | Eval Loss: 0.6033 | Eval R2: -63.2337 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4852 | Eval Loss: 0.5674 | Eval R2: -60.0161 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4548 | Eval Loss: 0.5352 | Eval R2: -57.0445 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4277 | Eval Loss: 0.5058 | Eval R2: -54.2660 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4030 | Eval Loss: 0.4785 | Eval R2: -51.6184 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3800 | Eval Loss: 0.4528 | Eval R2: -49.0846 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.3585 | Eval Loss: 0.4286 | Eval R2: -46.6588 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3382 | Eval Loss: 0.4056 | Eval R2: -44.3397 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3192 | Eval Loss: 0.3839 | Eval R2: -42.1296 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3013 | Eval Loss: 0.3635 | Eval R2: -40.0339 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2845 | Eval Loss: 0.3445 | Eval R2: -38.0584 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2690 | Eval Loss: 0.3268 | Eval R2: -36.2070 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2547 | Eval Loss: 0.3105 | Eval R2: -34.4806 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2415 | Eval Loss: 0.2956 | Eval R2: -32.8777 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2295 | Eval Loss: 0.2819 | Eval R2: -31.3952 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.2187 | Eval Loss: 0.2695 | Eval R2: -30.0292 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.2089 | Eval Loss: 0.2582 | Eval R2: -28.7763 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.2001 | Eval Loss: 0.2481 | Eval R2: -27.6329 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1922 | Eval Loss: 0.2391 | Eval R2: -26.5959 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1853 | Eval Loss: 0.2311 | Eval R2: -25.6622 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1793 | Eval Loss: 0.2240 | Eval R2: -24.8283 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1740 | Eval Loss: 0.2179 | Eval R2: -24.0901 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1694 | Eval Loss: 0.2126 | Eval R2: -23.4428 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.129693, test R2 score: -48.333738
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.16944706439971924, 'r2_eval_final': -23.44275665283203, 'loss_eval_final': 0.2126348912715912, 'r2_test': -48.333738278847584, 'loss_test': 0.12969258427619934, 'loss_nodes': [[0.11533419787883759, 0.10998618602752686, 0.11684517562389374, 0.12167207151651382, 0.11069349944591522, 0.1272405982017517, 0.11780054122209549, 0.12352266907691956, 0.11911483854055405, 0.1126246377825737, 0.12741659581661224, 0.1447647362947464, 0.32901352643966675, 0.11253992468118668, 0.1309782713651657, 0.11909196525812149, 0.11139724403619766, 0.11656883358955383, 0.11263705044984818, 0.11460893601179123]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.16945
wandb: loss_eval 0.21263
wandb: loss_test 0.12969
wandb:   r2_eval -23.44276
wandb:   r2_test -48.33374
wandb: 
wandb: ðŸš€ View run peach-plant-61 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/opseg8xp
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_024649-opseg8xp/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [18:01:00<5:30:09, 1320.63s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_030344-gml4l186
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run summer-armadillo-62
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/gml4l186

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7055 | Eval Loss: 0.6023 | Eval R2: -59.6218 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4702 | Eval Loss: 0.5132 | Eval R2: -52.5423 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3990 | Eval Loss: 0.4439 | Eval R2: -46.0484 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3417 | Eval Loss: 0.3827 | Eval R2: -40.2940 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2910 | Eval Loss: 0.3253 | Eval R2: -34.4739 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2445 | Eval Loss: 0.2705 | Eval R2: -28.4509 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2033 | Eval Loss: 0.2229 | Eval R2: -23.4031 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1682 | Eval Loss: 0.1830 | Eval R2: -19.0638 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1398 | Eval Loss: 0.1511 | Eval R2: -15.6819 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1170 | Eval Loss: 0.1259 | Eval R2: -13.0231 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0992 | Eval Loss: 0.1063 | Eval R2: -11.0135 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0853 | Eval Loss: 0.0915 | Eval R2: -9.5282 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0749 | Eval Loss: 0.0806 | Eval R2: -8.4588 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0672 | Eval Loss: 0.0725 | Eval R2: -7.6801 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0620 | Eval Loss: 0.0664 | Eval R2: -7.1304 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0577 | Eval Loss: 0.0616 | Eval R2: -6.7000 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0545 | Eval Loss: 0.0576 | Eval R2: -6.3527 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0517 | Eval Loss: 0.0541 | Eval R2: -6.0590 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0490 | Eval Loss: 0.0511 | Eval R2: -5.7889 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0470 | Eval Loss: 0.0485 | Eval R2: -5.5751 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0451 | Eval Loss: 0.0462 | Eval R2: -5.3756 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0434 | Eval Loss: 0.0442 | Eval R2: -5.2055 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0417 | Eval Loss: 0.0425 | Eval R2: -5.0715 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0404 | Eval Loss: 0.0410 | Eval R2: -4.9635 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0395 | Eval Loss: 0.0397 | Eval R2: -4.8321 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0384 | Eval Loss: 0.0386 | Eval R2: -4.7664 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0375 | Eval Loss: 0.0376 | Eval R2: -4.6666 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.025540, test R2 score: -0.960499
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.037513233721256256, 'r2_eval_final': -4.6665520668029785, 'loss_eval_final': 0.037558041512966156, 'r2_test': -0.9604990993381751, 'loss_test': 0.025539809837937355, 'loss_nodes': [[0.012147740460932255, 0.020867373794317245, 0.02301986888051033, 0.023715436458587646, 0.025925952941179276, 0.018345696851611137, 0.021468793973326683, 0.03493143618106842, 0.021335752680897713, 0.02169005200266838, 0.027486927807331085, 0.029675425961613655, 0.03318306431174278, 0.026573698967695236, 0.02275596372783184, 0.02203938737511635, 0.026707060635089874, 0.03412370756268501, 0.025139804929494858, 0.03966313228011131]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03751
wandb: loss_eval 0.03756
wandb: loss_test 0.02554
wandb:   r2_eval -4.66655
wandb:   r2_test -0.9605
wandb: 
wandb: ðŸš€ View run summer-armadillo-62 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/gml4l186
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_030344-gml4l186/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [18:17:28<4:44:50, 1220.74s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_032012-wxwezz3p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run proud-pyramid-63
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/wxwezz3p

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8445 | Eval Loss: 0.5322 | Eval R2: -56.7600 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3732 | Eval Loss: 0.3942 | Eval R2: -44.7547 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2719 | Eval Loss: 0.3052 | Eval R2: -35.8048 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2106 | Eval Loss: 0.2478 | Eval R2: -29.0953 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1757 | Eval Loss: 0.2143 | Eval R2: -24.5240 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1585 | Eval Loss: 0.1975 | Eval R2: -21.9045 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1514 | Eval Loss: 0.1904 | Eval R2: -20.6920 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1488 | Eval Loss: 0.1880 | Eval R2: -20.2415 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1481 | Eval Loss: 0.1874 | Eval R2: -20.1115 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.0966 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1158 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1339 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1392 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1356 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1309 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1286 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1283 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1478 | Eval Loss: 0.1874 | Eval R2: -20.1280 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1270 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1255 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1242 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1230 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1219 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1209 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1198 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1188 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1478 | Eval Loss: 0.1873 | Eval R2: -20.1178 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.111824, test R2 score: -38.383163
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.14779919385910034, 'r2_eval_final': -20.117794036865234, 'loss_eval_final': 0.18731984496116638, 'r2_test': -38.383163411914865, 'loss_test': 0.1118236631155014, 'loss_nodes': [[0.1065354123711586, 0.10898672044277191, 0.11687415838241577, 0.11112451553344727, 0.1090473085641861, 0.10844208300113678, 0.11052823066711426, 0.117258220911026, 0.11200245469808578, 0.11205542087554932, 0.10925173759460449, 0.11114451289176941, 0.11972324550151825, 0.11206243932247162, 0.10960259288549423, 0.10942181944847107, 0.11150512099266052, 0.11786854267120361, 0.1117488443851471, 0.11128991842269897]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.1478
wandb: loss_eval 0.18732
wandb: loss_test 0.11182
wandb:   r2_eval -20.11779
wandb:   r2_test -38.38316
wandb: 
wandb: ðŸš€ View run proud-pyramid-63 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/wxwezz3p
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_032012-wxwezz3p/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [18:34:31<4:11:37, 1161.36s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_033715-6iumk9mj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run solar-totem-64
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/6iumk9mj

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.2568 | Eval Loss: 1.1295 | Eval R2: -101.8356 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8972 | Eval Loss: 0.8594 | Eval R2: -81.9560 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6748 | Eval Loss: 0.6802 | Eval R2: -68.1097 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5284 | Eval Loss: 0.5582 | Eval R2: -58.0949 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4322 | Eval Loss: 0.4764 | Eval R2: -50.8342 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3653 | Eval Loss: 0.4136 | Eval R2: -44.9040 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3137 | Eval Loss: 0.3606 | Eval R2: -39.6444 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2693 | Eval Loss: 0.3079 | Eval R2: -34.0456 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2275 | Eval Loss: 0.2573 | Eval R2: -28.4109 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1887 | Eval Loss: 0.2114 | Eval R2: -23.0894 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1557 | Eval Loss: 0.1728 | Eval R2: -18.5912 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1294 | Eval Loss: 0.1426 | Eval R2: -15.0105 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1102 | Eval Loss: 0.1203 | Eval R2: -12.3987 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0961 | Eval Loss: 0.1040 | Eval R2: -10.6342 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0852 | Eval Loss: 0.0920 | Eval R2: -9.3715 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0772 | Eval Loss: 0.0827 | Eval R2: -8.3829 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0710 | Eval Loss: 0.0754 | Eval R2: -7.6480 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0659 | Eval Loss: 0.0696 | Eval R2: -7.0950 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0616 | Eval Loss: 0.0648 | Eval R2: -6.6677 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0583 | Eval Loss: 0.0609 | Eval R2: -6.3038 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0553 | Eval Loss: 0.0575 | Eval R2: -6.0243 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0525 | Eval Loss: 0.0545 | Eval R2: -5.7920 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0500 | Eval Loss: 0.0517 | Eval R2: -5.5722 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0476 | Eval Loss: 0.0491 | Eval R2: -5.3585 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0457 | Eval Loss: 0.0467 | Eval R2: -5.1638 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0437 | Eval Loss: 0.0449 | Eval R2: -5.0513 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0421 | Eval Loss: 0.0432 | Eval R2: -4.9379 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.029993, test R2 score: -1.799020
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.04206268489360809, 'r2_eval_final': -4.937933444976807, 'loss_eval_final': 0.04319256916642189, 'r2_test': -1.799020162339317, 'loss_test': 0.02999282442033291, 'loss_nodes': [[0.05149027705192566, 0.015877176076173782, 0.02220309153199196, 0.016386831179261208, 0.017888352274894714, 0.035866495221853256, 0.03802168741822243, 0.08438753336668015, 0.021375661715865135, 0.022349746897816658, 0.020658519119024277, 0.025144334882497787, 0.024777615442872047, 0.02197689190506935, 0.02293766662478447, 0.020766161382198334, 0.055561017245054245, 0.029263479635119438, 0.023395398631691933, 0.029528465121984482]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.002 MB of 0.006 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04206
wandb: loss_eval 0.04319
wandb: loss_test 0.02999
wandb:   r2_eval -4.93793
wandb:   r2_test -1.79902
wandb: 
wandb: ðŸš€ View run solar-totem-64 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/6iumk9mj
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_033715-6iumk9mj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [18:58:27<4:08:44, 1243.70s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_040111-1jvvs3ti
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-spaceship-65
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/1jvvs3ti

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8469 | Eval Loss: 0.6166 | Eval R2: -62.4670 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4232 | Eval Loss: 0.4041 | Eval R2: -43.2557 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2733 | Eval Loss: 0.2715 | Eval R2: -30.1345 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1835 | Eval Loss: 0.1888 | Eval R2: -20.0040 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1364 | Eval Loss: 0.1439 | Eval R2: -13.9813 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1135 | Eval Loss: 0.1219 | Eval R2: -11.6725 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1000 | Eval Loss: 0.1085 | Eval R2: -10.6552 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0898 | Eval Loss: 0.0963 | Eval R2: -9.5197 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0805 | Eval Loss: 0.0844 | Eval R2: -8.4025 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0714 | Eval Loss: 0.0732 | Eval R2: -7.3920 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0629 | Eval Loss: 0.0632 | Eval R2: -6.4813 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0553 | Eval Loss: 0.0546 | Eval R2: -5.7147 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0488 | Eval Loss: 0.0478 | Eval R2: -5.1240 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0438 | Eval Loss: 0.0428 | Eval R2: -4.7210 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0402 | Eval Loss: 0.0394 | Eval R2: -4.4457 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0377 | Eval Loss: 0.0371 | Eval R2: -4.2397 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0360 | Eval Loss: 0.0354 | Eval R2: -4.1048 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0345 | Eval Loss: 0.0341 | Eval R2: -3.9942 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0334 | Eval Loss: 0.0329 | Eval R2: -3.9200 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0325 | Eval Loss: 0.0319 | Eval R2: -3.8683 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0317 | Eval Loss: 0.0310 | Eval R2: -3.8074 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0309 | Eval Loss: 0.0301 | Eval R2: -3.7676 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0303 | Eval Loss: 0.0293 | Eval R2: -3.7310 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0297 | Eval Loss: 0.0286 | Eval R2: -3.7064 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0291 | Eval Loss: 0.0280 | Eval R2: -3.6970 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0285 | Eval Loss: 0.0275 | Eval R2: -3.6785 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0279 | Eval Loss: 0.0271 | Eval R2: -3.6893 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.017566, test R2 score: -0.042249
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.027949195355176926, 'r2_eval_final': -3.689293622970581, 'loss_eval_final': 0.027116579934954643, 'r2_test': -0.0422485305574804, 'loss_test': 0.017566492781043053, 'loss_nodes': [[0.01061359141021967, 0.013762978836894035, 0.016460943967103958, 0.013742550276219845, 0.015148378908634186, 0.011305090971291065, 0.015580961480736732, 0.018607521429657936, 0.016537388786673546, 0.016799455508589745, 0.015795554965734482, 0.02146291732788086, 0.02122339978814125, 0.01979907788336277, 0.018871840089559555, 0.016365034505724907, 0.02240491844713688, 0.02184617705643177, 0.021845700219273567, 0.02315639890730381]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02795
wandb: loss_eval 0.02712
wandb: loss_test 0.01757
wandb:   r2_eval -3.68929
wandb:   r2_test -0.04225
wandb: 
wandb: ðŸš€ View run fearless-spaceship-65 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/1jvvs3ti
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_040111-1jvvs3ti/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [20:02:22<6:10:34, 2021.28s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_050506-2dtspkhr
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run still-donkey-66
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/2dtspkhr

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5741 | Eval Loss: 0.5225 | Eval R2: -57.3303 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3868 | Eval Loss: 0.4383 | Eval R2: -49.5668 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3236 | Eval Loss: 0.3733 | Eval R2: -43.0461 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2733 | Eval Loss: 0.3192 | Eval R2: -37.1185 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2315 | Eval Loss: 0.2726 | Eval R2: -31.7443 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1968 | Eval Loss: 0.2341 | Eval R2: -27.2172 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1694 | Eval Loss: 0.2041 | Eval R2: -23.6055 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1483 | Eval Loss: 0.1795 | Eval R2: -20.5418 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1306 | Eval Loss: 0.1572 | Eval R2: -17.8609 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1139 | Eval Loss: 0.1361 | Eval R2: -15.3858 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0984 | Eval Loss: 0.1169 | Eval R2: -13.0903 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0851 | Eval Loss: 0.1000 | Eval R2: -11.1211 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0741 | Eval Loss: 0.0857 | Eval R2: -9.5626 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0650 | Eval Loss: 0.0742 | Eval R2: -8.3767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0580 | Eval Loss: 0.0649 | Eval R2: -7.3936 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0524 | Eval Loss: 0.0577 | Eval R2: -6.6891 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0479 | Eval Loss: 0.0519 | Eval R2: -6.0665 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0444 | Eval Loss: 0.0473 | Eval R2: -5.5641 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0419 | Eval Loss: 0.0438 | Eval R2: -5.1925 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0397 | Eval Loss: 0.0409 | Eval R2: -4.8666 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0377 | Eval Loss: 0.0388 | Eval R2: -4.6025 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0368 | Eval Loss: 0.0372 | Eval R2: -4.3710 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0357 | Eval Loss: 0.0360 | Eval R2: -4.1726 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0347 | Eval Loss: 0.0354 | Eval R2: -4.0478 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0342 | Eval Loss: 0.0356 | Eval R2: -3.8662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0337 | Eval Loss: 0.0354 | Eval R2: -3.7801 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0334 | Eval Loss: 0.0353 | Eval R2: -3.7221 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.024171, test R2 score: -0.827673
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.03337935730814934, 'r2_eval_final': -3.722141981124878, 'loss_eval_final': 0.03527617081999779, 'r2_test': -0.8276727665762393, 'loss_test': 0.02417088858783245, 'loss_nodes': [[0.014262469485402107, 0.020603522658348083, 0.025897707790136337, 0.022975381463766098, 0.02042223885655403, 0.01804378256201744, 0.020759152248501778, 0.0254078172147274, 0.02259194850921631, 0.02487015351653099, 0.021289037540555, 0.023281671106815338, 0.03190970420837402, 0.027730779722332954, 0.027327505871653557, 0.021040646359324455, 0.02774774469435215, 0.029443811625242233, 0.02695028856396675, 0.03086235374212265]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.002 MB of 0.003 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03338
wandb: loss_eval 0.03528
wandb: loss_test 0.02417
wandb:   r2_eval -3.72214
wandb:   r2_test -0.82767
wandb: 
wandb: ðŸš€ View run still-donkey-66 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/2dtspkhr
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_050506-2dtspkhr/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [20:25:39<5:05:38, 1833.86s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_052823-mbimhvyi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run restful-salad-67
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/mbimhvyi

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6322 | Eval Loss: 1.4431 | Eval R2: -121.8103 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2103 | Eval Loss: 1.2183 | Eval R2: -106.5182 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.0115 | Eval Loss: 1.0375 | Eval R2: -93.5331 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.8585 | Eval Loss: 0.9000 | Eval R2: -83.3674 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.7424 | Eval Loss: 0.7942 | Eval R2: -75.3603 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.6526 | Eval Loss: 0.7106 | Eval R2: -68.9137 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5815 | Eval Loss: 0.6433 | Eval R2: -63.6178 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.5240 | Eval Loss: 0.5879 | Eval R2: -59.1703 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4766 | Eval Loss: 0.5417 | Eval R2: -55.3686 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4372 | Eval Loss: 0.5024 | Eval R2: -52.0609 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4035 | Eval Loss: 0.4681 | Eval R2: -49.1060 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3741 | Eval Loss: 0.4378 | Eval R2: -46.4257 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3482 | Eval Loss: 0.4106 | Eval R2: -43.9608 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3251 | Eval Loss: 0.3860 | Eval R2: -41.6767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3042 | Eval Loss: 0.3635 | Eval R2: -39.5522 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2853 | Eval Loss: 0.3429 | Eval R2: -37.5664 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2679 | Eval Loss: 0.3239 | Eval R2: -35.7048 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2521 | Eval Loss: 0.3065 | Eval R2: -33.9607 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2377 | Eval Loss: 0.2905 | Eval R2: -32.3328 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.2247 | Eval Loss: 0.2760 | Eval R2: -30.8224 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.2129 | Eval Loss: 0.2629 | Eval R2: -29.4312 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.2024 | Eval Loss: 0.2512 | Eval R2: -28.1605 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1932 | Eval Loss: 0.2408 | Eval R2: -27.0108 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1852 | Eval Loss: 0.2318 | Eval R2: -25.9812 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1783 | Eval Loss: 0.2239 | Eval R2: -25.0689 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1724 | Eval Loss: 0.2173 | Eval R2: -24.2696 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1676 | Eval Loss: 0.2117 | Eval R2: -23.5770 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.127625, test R2 score: -49.634858
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.1675509512424469, 'r2_eval_final': -23.577024459838867, 'loss_eval_final': 0.21165750920772552, 'r2_test': -49.63485817873419, 'loss_test': 0.1276254504919052, 'loss_nodes': [[0.17279943823814392, 0.10889368504285812, 0.11788100004196167, 0.11727610975503922, 0.2720925807952881, 0.10808441787958145, 0.11373510211706161, 0.11658327281475067, 0.11251711845397949, 0.11280911415815353, 0.10963527858257294, 0.11113720387220383, 0.1181657612323761, 0.15471269190311432, 0.10962535440921783, 0.11082317680120468, 0.1180385947227478, 0.14368577301502228, 0.11213406175374985, 0.11187942326068878]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.16755
wandb: loss_eval 0.21166
wandb: loss_test 0.12763
wandb:   r2_eval -23.57702
wandb:   r2_test -49.63486
wandb: 
wandb: ðŸš€ View run restful-salad-67 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/mbimhvyi
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_052823-mbimhvyi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [20:44:57<4:04:39, 1631.02s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_054741-3f2yaame
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run zesty-sea-68
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/3f2yaame

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6573 | Eval Loss: 0.5538 | Eval R2: -55.6784 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4328 | Eval Loss: 0.4495 | Eval R2: -45.5657 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3522 | Eval Loss: 0.3719 | Eval R2: -39.1185 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2902 | Eval Loss: 0.3107 | Eval R2: -33.0903 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2420 | Eval Loss: 0.2594 | Eval R2: -27.7261 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2021 | Eval Loss: 0.2153 | Eval R2: -23.0883 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1681 | Eval Loss: 0.1777 | Eval R2: -18.9870 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1399 | Eval Loss: 0.1463 | Eval R2: -15.5743 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1167 | Eval Loss: 0.1207 | Eval R2: -12.8849 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0979 | Eval Loss: 0.1002 | Eval R2: -10.7940 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0826 | Eval Loss: 0.0842 | Eval R2: -9.1934 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0709 | Eval Loss: 0.0719 | Eval R2: -7.9978 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0619 | Eval Loss: 0.0625 | Eval R2: -7.0461 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0547 | Eval Loss: 0.0552 | Eval R2: -6.3060 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0492 | Eval Loss: 0.0495 | Eval R2: -5.7373 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0451 | Eval Loss: 0.0450 | Eval R2: -5.3583 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0418 | Eval Loss: 0.0415 | Eval R2: -5.0419 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0391 | Eval Loss: 0.0389 | Eval R2: -4.8112 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0371 | Eval Loss: 0.0369 | Eval R2: -4.6227 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0357 | Eval Loss: 0.0353 | Eval R2: -4.4842 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0344 | Eval Loss: 0.0340 | Eval R2: -4.3481 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0334 | Eval Loss: 0.0329 | Eval R2: -4.2187 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0325 | Eval Loss: 0.0320 | Eval R2: -4.1514 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0318 | Eval Loss: 0.0312 | Eval R2: -4.0536 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0310 | Eval Loss: 0.0305 | Eval R2: -3.9915 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0305 | Eval Loss: 0.0298 | Eval R2: -3.9036 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0300 | Eval Loss: 0.0292 | Eval R2: -3.8488 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019885, test R2 score: -0.234907
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.029994983226060867, 'r2_eval_final': -3.848764657974243, 'loss_eval_final': 0.02921174094080925, 'r2_test': -0.23490676674224115, 'loss_test': 0.01988532394170761, 'loss_nodes': [[0.009583338163793087, 0.014968340285122395, 0.01885943114757538, 0.019679903984069824, 0.016741234809160233, 0.012855453416705132, 0.01953095570206642, 0.02006017602980137, 0.02120284177362919, 0.01921192742884159, 0.016463950276374817, 0.021908311173319817, 0.022603482007980347, 0.022363794967532158, 0.022911371663212776, 0.016040848568081856, 0.025596145540475845, 0.024490447714924812, 0.023276524618268013, 0.02935803309082985]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02999
wandb: loss_eval 0.02921
wandb: loss_test 0.01989
wandb:   r2_eval -3.84876
wandb:   r2_test -0.23491
wandb: 
wandb: ðŸš€ View run zesty-sea-68 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/3f2yaame
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_054741-3f2yaame/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [21:44:33<4:55:16, 2214.58s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_064717-km5fa9a4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silvery-hill-69
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/km5fa9a4

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7629 | Eval Loss: 0.6680 | Eval R2: -68.1269 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5394 | Eval Loss: 0.6045 | Eval R2: -64.6722 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4834 | Eval Loss: 0.5611 | Eval R2: -59.2717 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4575 | Eval Loss: 0.5304 | Eval R2: -56.8084 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4304 | Eval Loss: 0.5022 | Eval R2: -54.2313 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4039 | Eval Loss: 0.4685 | Eval R2: -50.6551 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3787 | Eval Loss: 0.4381 | Eval R2: -47.4747 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3533 | Eval Loss: 0.4068 | Eval R2: -44.1294 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3282 | Eval Loss: 0.3768 | Eval R2: -40.9379 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3034 | Eval Loss: 0.3470 | Eval R2: -37.6626 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2791 | Eval Loss: 0.3177 | Eval R2: -34.4374 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2554 | Eval Loss: 0.2891 | Eval R2: -31.2676 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2324 | Eval Loss: 0.2612 | Eval R2: -28.1583 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.2098 | Eval Loss: 0.2342 | Eval R2: -25.1810 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1882 | Eval Loss: 0.2082 | Eval R2: -22.3081 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1674 | Eval Loss: 0.1835 | Eval R2: -19.5809 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1478 | Eval Loss: 0.1600 | Eval R2: -17.0223 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1292 | Eval Loss: 0.1380 | Eval R2: -14.6372 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1122 | Eval Loss: 0.1180 | Eval R2: -12.4662 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0970 | Eval Loss: 0.1003 | Eval R2: -10.5780 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0837 | Eval Loss: 0.0850 | Eval R2: -8.9780 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0726 | Eval Loss: 0.0725 | Eval R2: -7.6490 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0635 | Eval Loss: 0.0624 | Eval R2: -6.6183 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0563 | Eval Loss: 0.0547 | Eval R2: -5.8377 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0508 | Eval Loss: 0.0489 | Eval R2: -5.2571 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0467 | Eval Loss: 0.0448 | Eval R2: -4.8281 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0438 | Eval Loss: 0.0418 | Eval R2: -4.5162 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.029200, test R2 score: -2.786701
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04384234920144081, 'r2_eval_final': -4.51624870300293, 'loss_eval_final': 0.04175064340233803, 'r2_test': -2.786701296817568, 'loss_test': 0.02919994480907917, 'loss_nodes': [[0.016319578513503075, 0.06808862835168839, 0.015428868122398853, 0.019241470843553543, 0.046555664390325546, 0.013780093751847744, 0.022660549730062485, 0.03664028272032738, 0.018521985039114952, 0.020175809040665627, 0.030234981328248978, 0.028636328876018524, 0.030157089233398438, 0.02989945560693741, 0.022946398705244064, 0.021371280774474144, 0.02477015182375908, 0.025136446580290794, 0.057735342532396317, 0.035698551684617996]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04384
wandb: loss_eval 0.04175
wandb: loss_test 0.0292
wandb:   r2_eval -4.51625
wandb:   r2_test -2.7867
wandb: 
wandb: ðŸš€ View run silvery-hill-69 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/km5fa9a4
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_064717-km5fa9a4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [22:45:37<5:09:06, 2649.52s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_074821-1mo1tlln
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peachy-hill-70
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/1mo1tlln

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6322 | Eval Loss: 1.4431 | Eval R2: -121.8103 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2103 | Eval Loss: 1.2183 | Eval R2: -106.5182 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.0115 | Eval Loss: 1.0375 | Eval R2: -93.5331 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.8585 | Eval Loss: 0.9000 | Eval R2: -83.3674 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.7424 | Eval Loss: 0.7942 | Eval R2: -75.3603 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.6526 | Eval Loss: 0.7106 | Eval R2: -68.9137 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5815 | Eval Loss: 0.6433 | Eval R2: -63.6178 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.5240 | Eval Loss: 0.5879 | Eval R2: -59.1703 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4766 | Eval Loss: 0.5417 | Eval R2: -55.3686 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.4372 | Eval Loss: 0.5024 | Eval R2: -52.0609 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4035 | Eval Loss: 0.4681 | Eval R2: -49.1060 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.3741 | Eval Loss: 0.4378 | Eval R2: -46.4257 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3482 | Eval Loss: 0.4106 | Eval R2: -43.9608 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3251 | Eval Loss: 0.3860 | Eval R2: -41.6767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3042 | Eval Loss: 0.3635 | Eval R2: -39.5522 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2853 | Eval Loss: 0.3429 | Eval R2: -37.5664 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2679 | Eval Loss: 0.3239 | Eval R2: -35.7048 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2521 | Eval Loss: 0.3065 | Eval R2: -33.9607 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.2377 | Eval Loss: 0.2905 | Eval R2: -32.3328 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.2247 | Eval Loss: 0.2760 | Eval R2: -30.8224 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.2129 | Eval Loss: 0.2629 | Eval R2: -29.4312 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.2024 | Eval Loss: 0.2512 | Eval R2: -28.1605 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1932 | Eval Loss: 0.2408 | Eval R2: -27.0108 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1852 | Eval Loss: 0.2318 | Eval R2: -25.9812 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1783 | Eval Loss: 0.2239 | Eval R2: -25.0689 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1724 | Eval Loss: 0.2173 | Eval R2: -24.2696 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1676 | Eval Loss: 0.2117 | Eval R2: -23.5770 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.127625, test R2 score: -49.634858
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.1675509512424469, 'r2_eval_final': -23.577024459838867, 'loss_eval_final': 0.21165750920772552, 'r2_test': -49.63485817873419, 'loss_test': 0.1276254504919052, 'loss_nodes': [[0.17279943823814392, 0.10889368504285812, 0.11788100004196167, 0.11727610975503922, 0.2720925807952881, 0.10808441787958145, 0.11373510211706161, 0.11658327281475067, 0.11251711845397949, 0.11280911415815353, 0.10963527858257294, 0.11113720387220383, 0.1181657612323761, 0.15471269190311432, 0.10962535440921783, 0.11082317680120468, 0.1180385947227478, 0.14368577301502228, 0.11213406175374985, 0.11187942326068878]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.16755
wandb: loss_eval 0.21166
wandb: loss_test 0.12763
wandb:   r2_eval -23.57702
wandb:   r2_test -49.63486
wandb: 
wandb: ðŸš€ View run peachy-hill-70 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/1mo1tlln
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_074821-1mo1tlln/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [23:03:13<3:37:07, 2171.30s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_080557-rjcnbnr1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sparkling-fire-71
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/rjcnbnr1

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6573 | Eval Loss: 0.5538 | Eval R2: -55.6784 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4328 | Eval Loss: 0.4495 | Eval R2: -45.5657 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3522 | Eval Loss: 0.3719 | Eval R2: -39.1185 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2902 | Eval Loss: 0.3107 | Eval R2: -33.0903 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2420 | Eval Loss: 0.2594 | Eval R2: -27.7261 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2021 | Eval Loss: 0.2153 | Eval R2: -23.0883 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1681 | Eval Loss: 0.1777 | Eval R2: -18.9870 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1399 | Eval Loss: 0.1463 | Eval R2: -15.5743 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1167 | Eval Loss: 0.1207 | Eval R2: -12.8849 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0979 | Eval Loss: 0.1002 | Eval R2: -10.7940 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0826 | Eval Loss: 0.0842 | Eval R2: -9.1934 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0709 | Eval Loss: 0.0719 | Eval R2: -7.9978 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0619 | Eval Loss: 0.0625 | Eval R2: -7.0461 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0547 | Eval Loss: 0.0552 | Eval R2: -6.3060 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0492 | Eval Loss: 0.0495 | Eval R2: -5.7373 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0451 | Eval Loss: 0.0450 | Eval R2: -5.3583 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0418 | Eval Loss: 0.0415 | Eval R2: -5.0419 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0391 | Eval Loss: 0.0389 | Eval R2: -4.8112 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0371 | Eval Loss: 0.0369 | Eval R2: -4.6227 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0357 | Eval Loss: 0.0353 | Eval R2: -4.4842 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0344 | Eval Loss: 0.0340 | Eval R2: -4.3481 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0334 | Eval Loss: 0.0329 | Eval R2: -4.2187 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0325 | Eval Loss: 0.0320 | Eval R2: -4.1514 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0318 | Eval Loss: 0.0312 | Eval R2: -4.0536 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0310 | Eval Loss: 0.0305 | Eval R2: -3.9915 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0305 | Eval Loss: 0.0298 | Eval R2: -3.9036 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0300 | Eval Loss: 0.0292 | Eval R2: -3.8488 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.019885, test R2 score: -0.234907
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.029994983226060867, 'r2_eval_final': -3.848764657974243, 'loss_eval_final': 0.02921174094080925, 'r2_test': -0.23490676674224115, 'loss_test': 0.01988532394170761, 'loss_nodes': [[0.009583338163793087, 0.014968340285122395, 0.01885943114757538, 0.019679903984069824, 0.016741234809160233, 0.012855453416705132, 0.01953095570206642, 0.02006017602980137, 0.02120284177362919, 0.01921192742884159, 0.016463950276374817, 0.021908311173319817, 0.022603482007980347, 0.022363794967532158, 0.022911371663212776, 0.016040848568081856, 0.025596145540475845, 0.024490447714924812, 0.023276524618268013, 0.02935803309082985]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.02999
wandb: loss_eval 0.02921
wandb: loss_test 0.01989
wandb:   r2_eval -3.84876
wandb:   r2_test -0.23491
wandb: 
wandb: ðŸš€ View run sparkling-fire-71 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/rjcnbnr1
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_080557-rjcnbnr1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [24:03:33<3:37:10, 2606.10s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_090617-yklq5fjx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-leaf-72
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/yklq5fjx

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6261 | Eval Loss: 0.5709 | Eval R2: -57.1410 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4274 | Eval Loss: 0.4358 | Eval R2: -45.4906 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3273 | Eval Loss: 0.3507 | Eval R2: -37.1678 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2594 | Eval Loss: 0.2754 | Eval R2: -29.2438 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2032 | Eval Loss: 0.2119 | Eval R2: -22.3500 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1583 | Eval Loss: 0.1621 | Eval R2: -17.2018 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1230 | Eval Loss: 0.1243 | Eval R2: -13.2963 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0964 | Eval Loss: 0.0964 | Eval R2: -10.4435 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0769 | Eval Loss: 0.0764 | Eval R2: -8.4365 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0630 | Eval Loss: 0.0626 | Eval R2: -7.0472 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0538 | Eval Loss: 0.0534 | Eval R2: -6.0608 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0479 | Eval Loss: 0.0477 | Eval R2: -5.4544 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0445 | Eval Loss: 0.0443 | Eval R2: -5.0966 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0420 | Eval Loss: 0.0420 | Eval R2: -4.8758 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0402 | Eval Loss: 0.0402 | Eval R2: -4.7364 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0389 | Eval Loss: 0.0389 | Eval R2: -4.6049 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0378 | Eval Loss: 0.0378 | Eval R2: -4.4810 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0369 | Eval Loss: 0.0369 | Eval R2: -4.3738 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0361 | Eval Loss: 0.0360 | Eval R2: -4.2798 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0353 | Eval Loss: 0.0351 | Eval R2: -4.1959 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0346 | Eval Loss: 0.0344 | Eval R2: -4.1084 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0338 | Eval Loss: 0.0336 | Eval R2: -4.0520 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0330 | Eval Loss: 0.0329 | Eval R2: -3.9888 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0325 | Eval Loss: 0.0323 | Eval R2: -3.9371 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0320 | Eval Loss: 0.0317 | Eval R2: -3.9041 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0313 | Eval Loss: 0.0312 | Eval R2: -3.8492 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0309 | Eval Loss: 0.0307 | Eval R2: -3.7879 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020741, test R2 score: -0.482774
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.030914204195141792, 'r2_eval_final': -3.7878568172454834, 'loss_eval_final': 0.03071488067507744, 'r2_test': -0.48277436952565145, 'loss_test': 0.020741168409585953, 'loss_nodes': [[0.013963560573756695, 0.018581172451376915, 0.01936526969075203, 0.017603620886802673, 0.01727796532213688, 0.012921551242470741, 0.0164689552038908, 0.020481104031205177, 0.01915520615875721, 0.03151549771428108, 0.017141615971922874, 0.022337591275572777, 0.02261660434305668, 0.028234414756298065, 0.02190316841006279, 0.01887679472565651, 0.023703057318925858, 0.024083010852336884, 0.02475770004093647, 0.023835541680455208]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03091
wandb: loss_eval 0.03071
wandb: loss_test 0.02074
wandb:   r2_eval -3.78786
wandb:   r2_test -0.48277
wandb: 
wandb: ðŸš€ View run dulcet-leaf-72 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/yklq5fjx
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_090617-yklq5fjx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [24:27:24<2:30:14, 2253.52s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_093008-ecxncnbo
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run misty-disco-73
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ecxncnbo

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5251 | Eval Loss: 0.6035 | Eval R2: -63.6341 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4855 | Eval Loss: 0.5644 | Eval R2: -59.7723 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4502 | Eval Loss: 0.5228 | Eval R2: -55.6781 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4132 | Eval Loss: 0.4792 | Eval R2: -51.3556 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3757 | Eval Loss: 0.4346 | Eval R2: -46.7837 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3388 | Eval Loss: 0.3909 | Eval R2: -42.2176 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3039 | Eval Loss: 0.3495 | Eval R2: -37.8841 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2714 | Eval Loss: 0.3111 | Eval R2: -33.8559 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2417 | Eval Loss: 0.2764 | Eval R2: -30.2012 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2151 | Eval Loss: 0.2456 | Eval R2: -26.9148 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1917 | Eval Loss: 0.2185 | Eval R2: -23.9648 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1710 | Eval Loss: 0.1944 | Eval R2: -21.3302 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1529 | Eval Loss: 0.1731 | Eval R2: -18.9586 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1368 | Eval Loss: 0.1543 | Eval R2: -16.8485 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1226 | Eval Loss: 0.1376 | Eval R2: -14.9457 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1101 | Eval Loss: 0.1232 | Eval R2: -13.2301 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0993 | Eval Loss: 0.1108 | Eval R2: -11.7320 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0897 | Eval Loss: 0.1000 | Eval R2: -10.4406 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0813 | Eval Loss: 0.0906 | Eval R2: -9.3571 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0741 | Eval Loss: 0.0822 | Eval R2: -8.4183 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0678 | Eval Loss: 0.0748 | Eval R2: -7.6392 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0627 | Eval Loss: 0.0687 | Eval R2: -7.0003 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0586 | Eval Loss: 0.0635 | Eval R2: -6.4822 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0551 | Eval Loss: 0.0596 | Eval R2: -6.0851 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0523 | Eval Loss: 0.0565 | Eval R2: -5.7457 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0502 | Eval Loss: 0.0544 | Eval R2: -5.4792 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0487 | Eval Loss: 0.0525 | Eval R2: -5.2597 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.037938, test R2 score: -3.797783
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04866040125489235, 'r2_eval_final': -5.259693622589111, 'loss_eval_final': 0.052483875304460526, 'r2_test': -3.7977831499818624, 'loss_test': 0.037938255816698074, 'loss_nodes': [[0.011310283094644547, 0.02363470382988453, 0.05754337087273598, 0.019218984991312027, 0.03563186898827553, 0.017262788489460945, 0.020658180117607117, 0.0986252874135971, 0.04928087815642357, 0.061036355793476105, 0.03840510919690132, 0.04319631680846214, 0.05812251940369606, 0.025751300156116486, 0.02346716821193695, 0.04249831661581993, 0.03319675475358963, 0.025928594172000885, 0.027647702023386955, 0.04634864628314972]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04866
wandb: loss_eval 0.05248
wandb: loss_test 0.03794
wandb:   r2_eval -5.25969
wandb:   r2_test -3.79778
wandb: 
wandb: ðŸš€ View run misty-disco-73 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/ecxncnbo
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_093008-ecxncnbo/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [24:48:33<1:37:54, 1958.12s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_095117-o75ye3wl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run celestial-sunset-74
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/o75ye3wl

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7069 | Eval Loss: 0.6195 | Eval R2: -59.9663 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4880 | Eval Loss: 0.5389 | Eval R2: -55.7159 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4320 | Eval Loss: 0.5035 | Eval R2: -53.3592 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4027 | Eval Loss: 0.4796 | Eval R2: -51.7215 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3830 | Eval Loss: 0.4626 | Eval R2: -50.4483 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3681 | Eval Loss: 0.4480 | Eval R2: -49.2242 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3547 | Eval Loss: 0.4334 | Eval R2: -47.8942 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3413 | Eval Loss: 0.4179 | Eval R2: -46.4132 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3274 | Eval Loss: 0.4016 | Eval R2: -44.7822 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3130 | Eval Loss: 0.3843 | Eval R2: -43.0178 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2979 | Eval Loss: 0.3663 | Eval R2: -41.1411 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2824 | Eval Loss: 0.3475 | Eval R2: -39.1732 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2667 | Eval Loss: 0.3283 | Eval R2: -37.1367 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.2508 | Eval Loss: 0.3090 | Eval R2: -35.0581 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2349 | Eval Loss: 0.2899 | Eval R2: -32.9657 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2195 | Eval Loss: 0.2712 | Eval R2: -30.9026 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2047 | Eval Loss: 0.2533 | Eval R2: -28.8972 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1908 | Eval Loss: 0.2366 | Eval R2: -26.9891 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1779 | Eval Loss: 0.2212 | Eval R2: -25.2132 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1662 | Eval Loss: 0.2071 | Eval R2: -23.5566 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1558 | Eval Loss: 0.1944 | Eval R2: -22.0445 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1466 | Eval Loss: 0.1830 | Eval R2: -20.6690 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1382 | Eval Loss: 0.1727 | Eval R2: -19.3882 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.1310 | Eval Loss: 0.1633 | Eval R2: -18.2194 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.1242 | Eval Loss: 0.1545 | Eval R2: -17.1199 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.1181 | Eval Loss: 0.1463 | Eval R2: -16.0892 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.1120 | Eval Loss: 0.1384 | Eval R2: -15.0925 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.073979, test R2 score: -21.453895
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.11196622252464294, 'r2_eval_final': -15.092475891113281, 'loss_eval_final': 0.13842248916625977, 'r2_test': -21.453894639493047, 'loss_test': 0.07397891581058502, 'loss_nodes': [[0.06267796456813812, 0.060839906334877014, 0.04508930444717407, 0.07660488784313202, 0.10459381341934204, 0.10234763473272324, 0.06741274893283844, 0.05656306818127632, 0.04264690354466438, 0.07069870829582214, 0.08638381212949753, 0.0664684846997261, 0.06563951820135117, 0.08836404979228973, 0.1227060928940773, 0.0510149784386158, 0.07761146873235703, 0.10526128858327866, 0.046435583382844925, 0.08021818101406097]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.11197
wandb: loss_eval 0.13842
wandb: loss_test 0.07398
wandb:   r2_eval -15.09248
wandb:   r2_test -21.45389
wandb: 
wandb: ðŸš€ View run celestial-sunset-74 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/o75ye3wl
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_095117-o75ye3wl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [25:12:20<59:57, 1798.74s/it]  Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_101504-x2quszqu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run twilight-energy-75
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/x2quszqu

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7351 | Eval Loss: 0.7477 | Eval R2: -72.4240 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5773 | Eval Loss: 0.6098 | Eval R2: -61.0606 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4557 | Eval Loss: 0.4734 | Eval R2: -48.0780 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3390 | Eval Loss: 0.3420 | Eval R2: -35.6685 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2315 | Eval Loss: 0.2273 | Eval R2: -24.2735 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1499 | Eval Loss: 0.1491 | Eval R2: -16.2706 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1019 | Eval Loss: 0.1055 | Eval R2: -11.5468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0772 | Eval Loss: 0.0816 | Eval R2: -9.0650 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0633 | Eval Loss: 0.0681 | Eval R2: -7.6457 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0559 | Eval Loss: 0.0603 | Eval R2: -6.8193 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0514 | Eval Loss: 0.0552 | Eval R2: -6.2358 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0486 | Eval Loss: 0.0515 | Eval R2: -5.8382 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0465 | Eval Loss: 0.0486 | Eval R2: -5.5224 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0444 | Eval Loss: 0.0461 | Eval R2: -5.2871 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0428 | Eval Loss: 0.0437 | Eval R2: -5.0974 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0411 | Eval Loss: 0.0416 | Eval R2: -4.9007 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0398 | Eval Loss: 0.0396 | Eval R2: -4.7099 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0386 | Eval Loss: 0.0380 | Eval R2: -4.5496 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0375 | Eval Loss: 0.0368 | Eval R2: -4.4252 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0368 | Eval Loss: 0.0357 | Eval R2: -4.3562 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0360 | Eval Loss: 0.0349 | Eval R2: -4.2814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0352 | Eval Loss: 0.0341 | Eval R2: -4.2111 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0345 | Eval Loss: 0.0334 | Eval R2: -4.1464 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0338 | Eval Loss: 0.0327 | Eval R2: -4.1091 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0331 | Eval Loss: 0.0321 | Eval R2: -4.0752 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0325 | Eval Loss: 0.0315 | Eval R2: -3.9971 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0321 | Eval Loss: 0.0309 | Eval R2: -3.9364 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.020304, test R2 score: -0.279949
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.03208351135253906, 'r2_eval_final': -3.9363527297973633, 'loss_eval_final': 0.030945731326937675, 'r2_test': -0.27994904246294144, 'loss_test': 0.02030363865196705, 'loss_nodes': [[0.010641773231327534, 0.015358714386820793, 0.018952883780002594, 0.022002313286066055, 0.015402951277792454, 0.011733561754226685, 0.01584458351135254, 0.022059109061956406, 0.01984305866062641, 0.018676603212952614, 0.01573602855205536, 0.02302725799381733, 0.03147648647427559, 0.02826499566435814, 0.019888879731297493, 0.016946876421570778, 0.024437211453914642, 0.023501131683588028, 0.024166306480765343, 0.028111999854445457]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.03208
wandb: loss_eval 0.03095
wandb: loss_test 0.0203
wandb:   r2_eval -3.93635
wandb:   r2_test -0.27995
wandb: 
wandb: ðŸš€ View run twilight-energy-75 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/x2quszqu
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_101504-x2quszqu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [25:23:33<24:20, 1460.96s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_102617-8oe7yd4s
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fearless-firefly-76
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/8oe7yd4s

==================== DATASET INFO ===================

Train dataset: 2176
Validation dataset: 476
Test dataset: 442

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6448 | Eval Loss: 0.5631 | Eval R2: -54.4314 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4543 | Eval Loss: 0.4931 | Eval R2: -49.0844 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4021 | Eval Loss: 0.4561 | Eval R2: -46.4221 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3702 | Eval Loss: 0.4304 | Eval R2: -44.9423 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3460 | Eval Loss: 0.4071 | Eval R2: -42.9494 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3269 | Eval Loss: 0.3880 | Eval R2: -41.3505 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3095 | Eval Loss: 0.3684 | Eval R2: -39.3788 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2926 | Eval Loss: 0.3482 | Eval R2: -37.3184 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2749 | Eval Loss: 0.3260 | Eval R2: -34.8906 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2561 | Eval Loss: 0.3020 | Eval R2: -32.2790 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2358 | Eval Loss: 0.2755 | Eval R2: -29.3098 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2145 | Eval Loss: 0.2475 | Eval R2: -26.1988 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1926 | Eval Loss: 0.2194 | Eval R2: -23.1073 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1715 | Eval Loss: 0.1925 | Eval R2: -20.1806 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1515 | Eval Loss: 0.1671 | Eval R2: -17.4703 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1322 | Eval Loss: 0.1433 | Eval R2: -14.9270 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1142 | Eval Loss: 0.1215 | Eval R2: -12.6115 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0979 | Eval Loss: 0.1024 | Eval R2: -10.5500 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0841 | Eval Loss: 0.0865 | Eval R2: -8.8806 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0729 | Eval Loss: 0.0743 | Eval R2: -7.6224 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0645 | Eval Loss: 0.0655 | Eval R2: -6.7066 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0585 | Eval Loss: 0.0594 | Eval R2: -6.0983 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0545 | Eval Loss: 0.0552 | Eval R2: -5.6686 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0516 | Eval Loss: 0.0522 | Eval R2: -5.3793 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0494 | Eval Loss: 0.0500 | Eval R2: -5.1911 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0478 | Eval Loss: 0.0483 | Eval R2: -5.0420 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0463 | Eval Loss: 0.0469 | Eval R2: -4.9097 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.033728, test R2 score: -1.525953
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04625960811972618, 'r2_eval_final': -4.909668922424316, 'loss_eval_final': 0.04689572751522064, 'r2_test': -1.5259526402178647, 'loss_test': 0.033727530390024185, 'loss_nodes': [[0.024620551615953445, 0.04412495344877243, 0.023768579587340355, 0.04268081858754158, 0.02749869041144848, 0.024952607229351997, 0.024510907009243965, 0.02766241319477558, 0.023080427199602127, 0.026463616639375687, 0.02588610164821148, 0.07539325952529907, 0.03236325457692146, 0.02581595443189144, 0.028208188712596893, 0.020186152309179306, 0.026295548304915428, 0.037476588040590286, 0.06164729967713356, 0.051914796233177185]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04626
wandb: loss_eval 0.0469
wandb: loss_test 0.03373
wandb:   r2_eval -4.90967
wandb:   r2_test -1.52595
wandb: 
wandb: ðŸš€ View run fearless-firefly-76 at: https://wandb.ai/maragumar01/mtgnn_branch_fault/runs/8oe7yd4s
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_fault
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_102617-8oe7yd4s/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [25:45:23<00:00, 1415.90s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [25:45:23<00:00, 1854.48s/it]

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.017566, test R2 score: -0.042249

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.020435
1   LSTM_NOBATCH  ...   0.020590
2    DyGrEncoder  ...   0.018094
3      MPNN_LSTM  ...   0.020219
4         MSTGCN  ...   0.022348
5      EvolveGCN  ...   0.129529
6         ASTGCN  ...   0.023180
7          AGCRN  ...   0.031952
8          DCRNN  ...   0.019569
9          MTGNN  ...   0.126450
10         MTGNN  ...   0.041193
11         MTGNN  ...   0.032206
12         MTGNN  ...   0.028956
13         MTGNN  ...   0.027949
14         MTGNN  ...   0.027949

[15 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

