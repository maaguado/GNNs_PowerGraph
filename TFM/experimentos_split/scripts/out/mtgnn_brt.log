Processing dataset...
Node:  0  not included, including...
Node:  1  not included, including...
Node:  2  not included, including...
Node:  3  not included, including...
Node:  4  not included, including...
Node:  5  not included, including...
Node:  6  not included, including...
Node:  7  not included, including...
Node:  8  not included, including...
Node:  9  not included, including...
Node:  10  not included, including...
Node:  11  not included, including...
Node:  12  not included, including...
Node:  13  not included, including...
Node:  14  not included, including...
Node:  15  not included, including...
Node:  16  not included, including...
Node:  17  not included, including...
Node:  18  not included, including...
Node:  19  not included, including...
Node:  20  not included, including...
Node:  21  not included, including...
Node:  22  not included, including...
Skipping  row_328
Ajustando modelo para branch_trip...
Number of situations:  549
Number of timestamps:  800
Number of situations of the selected type:  101
  0%|          | 0/50 [00:00<?, ?it/s]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: Currently logged in as: maragumar01. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_090256-rch5o3ex
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run swift-pine-31
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/rch5o3ex

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6336 | Eval Loss: 1.3177 | Eval R2: -1722.9482 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2659 | Eval Loss: 1.0761 | Eval R2: -1409.7224 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.0469 | Eval Loss: 0.9101 | Eval R2: -1190.9221 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.8712 | Eval Loss: 0.7382 | Eval R2: -961.8068 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.6920 | Eval Loss: 0.5625 | Eval R2: -726.2089 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.5145 | Eval Loss: 0.3975 | Eval R2: -503.6746 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3512 | Eval Loss: 0.2503 | Eval R2: -305.1384 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2103 | Eval Loss: 0.1323 | Eval R2: -146.4582 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1081 | Eval Loss: 0.0595 | Eval R2: -49.3096 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0549 | Eval Loss: 0.0319 | Eval R2: -13.9011 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0382 | Eval Loss: 0.0254 | Eval R2: -7.8586 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0332 | Eval Loss: 0.0221 | Eval R2: -6.0990 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0293 | Eval Loss: 0.0192 | Eval R2: -4.7896 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0261 | Eval Loss: 0.0168 | Eval R2: -3.9511 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0234 | Eval Loss: 0.0148 | Eval R2: -3.3616 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0212 | Eval Loss: 0.0131 | Eval R2: -2.8575 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0193 | Eval Loss: 0.0118 | Eval R2: -2.4475 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0178 | Eval Loss: 0.0108 | Eval R2: -2.0990 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0167 | Eval Loss: 0.0100 | Eval R2: -1.8286 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0156 | Eval Loss: 0.0094 | Eval R2: -1.6348 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0149 | Eval Loss: 0.0090 | Eval R2: -1.4787 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0142 | Eval Loss: 0.0085 | Eval R2: -1.3590 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0137 | Eval Loss: 0.0082 | Eval R2: -1.2637 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0132 | Eval Loss: 0.0079 | Eval R2: -1.1688 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0127 | Eval Loss: 0.0076 | Eval R2: -1.0974 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0123 | Eval Loss: 0.0073 | Eval R2: -1.0150 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0119 | Eval Loss: 0.0070 | Eval R2: -0.9415 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006603, test R2 score: -1.716263

==================== GUARDANDO RESULTADOS ===================

         Modelo  ... Loss_final
0          LSTM  ...   0.003296
1  LSTM_NOBATCH  ...   0.004827
2   DyGrEncoder  ...   0.002677
3     MPNN_LSTM  ...   0.003865
4        MSTGCN  ...   0.003641
5     EvolveGCN  ...   0.038322
6        ASTGCN  ...   0.003370
7         AGCRN  ...   0.009095
8         DCRNN  ...   0.003779
9         MTGNN  ...   0.011934

[10 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.011933927424252033, 'r2_eval_final': -0.9414734840393066, 'loss_eval_final': 0.0070438077673316, 'r2_test': -1.71626306223348, 'loss_test': 0.006603345740586519, 'loss_nodes': [[0.007029117550700903, 0.004021259490400553, 0.0027977379504591227, 0.003368125529959798, 0.00679105706512928, 0.004150369204580784, 0.0045714047737419605, 0.004603955429047346, 0.007054948713630438, 0.010566606186330318, 0.005449820309877396, 0.011874761432409286, 0.00598151283338666, 0.007425489369779825, 0.005584006197750568, 0.005476698745042086, 0.009190562181174755, 0.011629229411482811, 0.006959096062928438, 0.007541145663708448]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.007 MB of 0.011 MB uploadedwandb: / 0.014 MB of 0.015 MB uploadedwandb: - 0.014 MB of 0.015 MB uploadedwandb: \ 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01193
wandb: loss_eval 0.00704
wandb: loss_test 0.0066
wandb:   r2_eval -0.94147
wandb:   r2_test -1.71626
wandb: 
wandb: ðŸš€ View run swift-pine-31 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/rch5o3ex
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_090256-rch5o3ex/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  2%|â–         | 1/50 [23:05<18:51:47, 1385.87s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_092600-c5nz9tbb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visionary-meadow-32
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/c5nz9tbb

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.4771 | Eval Loss: 1.1092 | Eval R2: -1456.4575 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.9603 | Eval Loss: 0.7069 | Eval R2: -926.5992 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6059 | Eval Loss: 0.4293 | Eval R2: -559.7743 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3694 | Eval Loss: 0.2525 | Eval R2: -325.6609 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2157 | Eval Loss: 0.1386 | Eval R2: -173.8532 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1197 | Eval Loss: 0.0724 | Eval R2: -85.0931 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0647 | Eval Loss: 0.0363 | Eval R2: -37.1404 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0356 | Eval Loss: 0.0184 | Eval R2: -13.6414 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0216 | Eval Loss: 0.0111 | Eval R2: -4.2548 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0161 | Eval Loss: 0.0088 | Eval R2: -1.6429 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0143 | Eval Loss: 0.0081 | Eval R2: -1.2037 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0133 | Eval Loss: 0.0077 | Eval R2: -1.0832 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0124 | Eval Loss: 0.0073 | Eval R2: -0.9196 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0117 | Eval Loss: 0.0069 | Eval R2: -0.8183 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0110 | Eval Loss: 0.0066 | Eval R2: -0.7245 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0104 | Eval Loss: 0.0063 | Eval R2: -0.6396 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0099 | Eval Loss: 0.0061 | Eval R2: -0.5889 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0093 | Eval Loss: 0.0058 | Eval R2: -0.5129 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0088 | Eval Loss: 0.0056 | Eval R2: -0.4359 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0083 | Eval Loss: 0.0053 | Eval R2: -0.3789 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0080 | Eval Loss: 0.0052 | Eval R2: -0.3300 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0076 | Eval Loss: 0.0050 | Eval R2: -0.2756 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0072 | Eval Loss: 0.0048 | Eval R2: -0.2275 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0069 | Eval Loss: 0.0046 | Eval R2: -0.1827 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0066 | Eval Loss: 0.0045 | Eval R2: -0.1452 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0064 | Eval Loss: 0.0044 | Eval R2: -0.1173 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0061 | Eval Loss: 0.0042 | Eval R2: -0.0830 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.003772, test R2 score: -0.503625

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.003296
1   LSTM_NOBATCH  ...   0.004827
2    DyGrEncoder  ...   0.002677
3      MPNN_LSTM  ...   0.003865
4         MSTGCN  ...   0.003641
5      EvolveGCN  ...   0.038322
6         ASTGCN  ...   0.003370
7          AGCRN  ...   0.009095
8          DCRNN  ...   0.003779
9          MTGNN  ...   0.011934
10         MTGNN  ...   0.006137

[11 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.006137287709861994, 'r2_eval_final': -0.08300533145666122, 'loss_eval_final': 0.004229082725942135, 'r2_test': -0.5036245481516022, 'loss_test': 0.003771590068936348, 'loss_nodes': [[0.001237940858118236, 0.003674688283354044, 0.0015434410888701677, 0.003098956076428294, 0.0015254857717081904, 0.0012416751123964787, 0.0019193013431504369, 0.003833367954939604, 0.005477069411426783, 0.0022304262965917587, 0.0022708200849592686, 0.005205524619668722, 0.0064371866174042225, 0.003656538436189294, 0.005485201720148325, 0.003751211566850543, 0.004891593009233475, 0.005332895088940859, 0.005764184053987265, 0.006854300387203693]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.010 MB uploadedwandb: / 0.002 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00614
wandb: loss_eval 0.00423
wandb: loss_test 0.00377
wandb:   r2_eval -0.08301
wandb:   r2_test -0.50362
wandb: 
wandb: ðŸš€ View run visionary-meadow-32 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/c5nz9tbb
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_092600-c5nz9tbb/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  4%|â–         | 2/50 [46:27<18:36:20, 1395.43s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_094923-vdhn27cw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-wave-33
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/vdhn27cw

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.2951 | Eval Loss: 1.0281 | Eval R2: -1352.3916 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.9302 | Eval Loss: 0.7427 | Eval R2: -978.0564 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6727 | Eval Loss: 0.5229 | Eval R2: -685.7012 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4659 | Eval Loss: 0.3427 | Eval R2: -442.1570 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2972 | Eval Loss: 0.2021 | Eval R2: -248.9798 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1723 | Eval Loss: 0.1090 | Eval R2: -117.8461 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0965 | Eval Loss: 0.0616 | Eval R2: -47.8021 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0612 | Eval Loss: 0.0437 | Eval R2: -19.4805 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0489 | Eval Loss: 0.0387 | Eval R2: -11.5883 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0459 | Eval Loss: 0.0376 | Eval R2: -10.0744 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0452 | Eval Loss: 0.0372 | Eval R2: -9.7240 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0450 | Eval Loss: 0.0371 | Eval R2: -9.6742 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7060 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7199 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7098 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6973 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6936 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6937 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6938 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6932 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6921 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6910 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6899 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6890 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6882 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6873 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.6864 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032327, test R2 score: -11.405066
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.04494500160217285, 'r2_eval_final': -9.686365127563477, 'loss_eval_final': 0.037005044519901276, 'r2_test': -11.405066280445894, 'loss_test': 0.03232717886567116, 'loss_nodes': [[0.031102469190955162, 0.032648518681526184, 0.032810110598802567, 0.03139191493391991, 0.033011969178915024, 0.03204761818051338, 0.03303040564060211, 0.033430855721235275, 0.03202075511217117, 0.03213440626859665, 0.031861502677202225, 0.03238191083073616, 0.03260339796543121, 0.03131609782576561, 0.03297536447644234, 0.0312057975679636, 0.03250150382518768, 0.03298616036772728, 0.03247753530740738, 0.032605186104774475]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04495
wandb: loss_eval 0.03701
wandb: loss_test 0.03233
wandb:   r2_eval -9.68637
wandb:   r2_test -11.40507
wandb: 
wandb: ðŸš€ View run fragrant-wave-33 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/vdhn27cw
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_094923-vdhn27cw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  6%|â–Œ         | 3/50 [1:04:00<16:10:33, 1239.01s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_100655-xgis4xqz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run hardy-bird-34
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/xgis4xqz

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1809 | Eval Loss: 0.6845 | Eval R2: -889.2283 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6024 | Eval Loss: 0.4362 | Eval R2: -559.7561 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3742 | Eval Loss: 0.2543 | Eval R2: -318.2298 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2168 | Eval Loss: 0.1389 | Eval R2: -157.8815 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1189 | Eval Loss: 0.0713 | Eval R2: -65.1418 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0675 | Eval Loss: 0.0427 | Eval R2: -22.9938 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0470 | Eval Loss: 0.0329 | Eval R2: -10.0606 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0403 | Eval Loss: 0.0300 | Eval R2: -7.4625 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0376 | Eval Loss: 0.0279 | Eval R2: -6.8241 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0352 | Eval Loss: 0.0259 | Eval R2: -6.6199 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0328 | Eval Loss: 0.0242 | Eval R2: -6.1058 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0304 | Eval Loss: 0.0227 | Eval R2: -5.6550 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0282 | Eval Loss: 0.0214 | Eval R2: -5.1907 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0263 | Eval Loss: 0.0204 | Eval R2: -5.1093 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0247 | Eval Loss: 0.0194 | Eval R2: -4.8743 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0233 | Eval Loss: 0.0183 | Eval R2: -4.7128 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0219 | Eval Loss: 0.0172 | Eval R2: -4.3684 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0206 | Eval Loss: 0.0162 | Eval R2: -4.0423 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0194 | Eval Loss: 0.0151 | Eval R2: -3.6187 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0182 | Eval Loss: 0.0141 | Eval R2: -3.2070 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0172 | Eval Loss: 0.0132 | Eval R2: -2.8767 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0162 | Eval Loss: 0.0122 | Eval R2: -2.4997 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0152 | Eval Loss: 0.0114 | Eval R2: -2.2635 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0143 | Eval Loss: 0.0106 | Eval R2: -2.0453 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0134 | Eval Loss: 0.0099 | Eval R2: -1.8230 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0126 | Eval Loss: 0.0091 | Eval R2: -1.6025 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0118 | Eval Loss: 0.0084 | Eval R2: -1.4559 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007460, test R2 score: -2.123678
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.011777092702686787, 'r2_eval_final': -1.455873727798462, 'loss_eval_final': 0.008393367752432823, 'r2_test': -2.123678478777353, 'loss_test': 0.007460397202521563, 'loss_nodes': [[0.0016526860417798162, 0.0072550661861896515, 0.002525540767237544, 0.0031719361431896687, 0.00437835743650794, 0.004677698016166687, 0.01035041268914938, 0.01378068421036005, 0.003694946877658367, 0.00351976091042161, 0.004009096417576075, 0.007270724978297949, 0.005388121120631695, 0.03937581554055214, 0.00513819744810462, 0.004500977229326963, 0.007339851465076208, 0.006774858105927706, 0.007997332140803337, 0.006405897904187441]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01178
wandb: loss_eval 0.00839
wandb: loss_test 0.00746
wandb:   r2_eval -1.45587
wandb:   r2_test -2.12368
wandb: 
wandb: ðŸš€ View run hardy-bird-34 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/xgis4xqz
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_100655-xgis4xqz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  8%|â–Š         | 4/50 [2:15:50<31:19:19, 2451.30s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_111845-q06pqv6a
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comic-sponge-35
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/q06pqv6a

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9041 | Eval Loss: 0.7612 | Eval R2: -996.9238 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7647 | Eval Loss: 0.6846 | Eval R2: -899.0084 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6941 | Eval Loss: 0.6216 | Eval R2: -812.9354 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6296 | Eval Loss: 0.5585 | Eval R2: -728.5205 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5629 | Eval Loss: 0.4948 | Eval R2: -642.3735 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4953 | Eval Loss: 0.4306 | Eval R2: -554.8746 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4288 | Eval Loss: 0.3687 | Eval R2: -471.0504 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3659 | Eval Loss: 0.3106 | Eval R2: -392.5980 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3078 | Eval Loss: 0.2579 | Eval R2: -321.1087 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2554 | Eval Loss: 0.2111 | Eval R2: -257.8965 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2094 | Eval Loss: 0.1710 | Eval R2: -203.8057 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1707 | Eval Loss: 0.1376 | Eval R2: -159.1200 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1388 | Eval Loss: 0.1106 | Eval R2: -123.3797 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1132 | Eval Loss: 0.0893 | Eval R2: -95.6407 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0928 | Eval Loss: 0.0726 | Eval R2: -74.2588 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0764 | Eval Loss: 0.0594 | Eval R2: -57.8652 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0631 | Eval Loss: 0.0487 | Eval R2: -45.1380 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0520 | Eval Loss: 0.0403 | Eval R2: -35.2741 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0429 | Eval Loss: 0.0332 | Eval R2: -26.9667 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0275 | Eval R2: -20.1591 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0300 | Eval Loss: 0.0230 | Eval R2: -14.6814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0257 | Eval Loss: 0.0196 | Eval R2: -10.7098 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0225 | Eval Loss: 0.0171 | Eval R2: -7.9165 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0200 | Eval Loss: 0.0151 | Eval R2: -5.9292 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0180 | Eval Loss: 0.0137 | Eval R2: -4.8086 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0165 | Eval Loss: 0.0124 | Eval R2: -3.8270 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0151 | Eval Loss: 0.0114 | Eval R2: -3.1922 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.010108, test R2 score: -3.593988
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.01514763105660677, 'r2_eval_final': -3.192204236984253, 'loss_eval_final': 0.011433062143623829, 'r2_test': -3.593987595411585, 'loss_test': 0.010108032263815403, 'loss_nodes': [[0.0049039022997021675, 0.019939454272389412, 0.007327746134251356, 0.0029728764202445745, 0.0033477996475994587, 0.023724354803562164, 0.003918927162885666, 0.012832381762564182, 0.004782555159181356, 0.003684115596115589, 0.00440628919750452, 0.008504842408001423, 0.013646646402776241, 0.01789482869207859, 0.0067116073332726955, 0.012484489008784294, 0.009863875806331635, 0.015354786068201065, 0.013131584040820599, 0.01272757537662983]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01515
wandb: loss_eval 0.01143
wandb: loss_test 0.01011
wandb:   r2_eval -3.1922
wandb:   r2_test -3.59399
wandb: 
wandb: ðŸš€ View run comic-sponge-35 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/q06pqv6a
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_111845-q06pqv6a/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 10%|â–ˆ         | 5/50 [2:36:40<25:13:33, 2018.07s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_113935-ksc27x0x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run curious-moon-36
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/ksc27x0x

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3704 | Eval Loss: 1.1588 | Eval R2: -1522.6636 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.1019 | Eval Loss: 0.9470 | Eval R2: -1245.9148 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.9013 | Eval Loss: 0.7650 | Eval R2: -1006.7076 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7234 | Eval Loss: 0.5988 | Eval R2: -786.5292 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5588 | Eval Loss: 0.4462 | Eval R2: -582.3038 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4103 | Eval Loss: 0.3139 | Eval R2: -403.0302 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2855 | Eval Loss: 0.2090 | Eval R2: -258.7387 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1901 | Eval Loss: 0.1339 | Eval R2: -153.9694 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1248 | Eval Loss: 0.0867 | Eval R2: -86.5291 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0855 | Eval Loss: 0.0607 | Eval R2: -48.0117 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0643 | Eval Loss: 0.0478 | Eval R2: -27.9694 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0539 | Eval Loss: 0.0419 | Eval R2: -18.1874 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0490 | Eval Loss: 0.0391 | Eval R2: -13.5753 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0468 | Eval Loss: 0.0379 | Eval R2: -11.4338 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0457 | Eval Loss: 0.0373 | Eval R2: -10.4576 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0453 | Eval Loss: 0.0371 | Eval R2: -10.0297 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0451 | Eval Loss: 0.0370 | Eval R2: -9.8503 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7739 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7381 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7208 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7130 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7096 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7080 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7071 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7065 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7060 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7055 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032325, test R2 score: -11.425828
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.04493798688054085, 'r2_eval_final': -9.705501556396484, 'loss_eval_final': 0.036991655826568604, 'r2_test': -11.42582807908623, 'loss_test': 0.03232539817690849, 'loss_nodes': [[0.03110206313431263, 0.03264728561043739, 0.03281053900718689, 0.03139190375804901, 0.03299935907125473, 0.03204686939716339, 0.033029913902282715, 0.033427681773900986, 0.032020606100559235, 0.032134972512722015, 0.03186018764972687, 0.032381873577833176, 0.03260040283203125, 0.03131607919931412, 0.03297436982393265, 0.031206253916025162, 0.032495930790901184, 0.03298694267868996, 0.032477378845214844, 0.03259730339050293]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04494
wandb: loss_eval 0.03699
wandb: loss_test 0.03233
wandb:   r2_eval -9.7055
wandb:   r2_test -11.42583
wandb: 
wandb: ðŸš€ View run curious-moon-36 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/ksc27x0x
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_113935-ksc27x0x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 12%|â–ˆâ–        | 6/50 [2:50:49<19:48:21, 1620.49s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_115344-eyrxe1e4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-firefly-37
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/eyrxe1e4

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6260 | Eval Loss: 0.9184 | Eval R2: -1193.3700 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8937 | Eval Loss: 0.7492 | Eval R2: -975.5931 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7005 | Eval Loss: 0.5693 | Eval R2: -735.8911 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5378 | Eval Loss: 0.4308 | Eval R2: -546.5880 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4036 | Eval Loss: 0.3122 | Eval R2: -390.7799 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2946 | Eval Loss: 0.2221 | Eval R2: -267.9971 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2110 | Eval Loss: 0.1546 | Eval R2: -176.8956 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1498 | Eval Loss: 0.1064 | Eval R2: -112.7954 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1069 | Eval Loss: 0.0738 | Eval R2: -69.6711 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0781 | Eval Loss: 0.0518 | Eval R2: -42.1352 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0587 | Eval Loss: 0.0366 | Eval R2: -25.0063 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0456 | Eval Loss: 0.0276 | Eval R2: -15.5697 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0361 | Eval Loss: 0.0218 | Eval R2: -10.7827 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0291 | Eval Loss: 0.0183 | Eval R2: -8.6954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0245 | Eval Loss: 0.0168 | Eval R2: -8.4377 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0140 | Eval R2: -5.5006 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0118 | Eval R2: -3.5593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0112 | Eval R2: -3.2574 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0171 | Eval Loss: 0.0105 | Eval R2: -2.7739 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0163 | Eval Loss: 0.0100 | Eval R2: -2.2395 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0157 | Eval Loss: 0.0096 | Eval R2: -1.9573 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0152 | Eval Loss: 0.0093 | Eval R2: -1.7769 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0148 | Eval Loss: 0.0090 | Eval R2: -1.6431 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0144 | Eval Loss: 0.0089 | Eval R2: -1.6965 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0140 | Eval Loss: 0.0087 | Eval R2: -1.6233 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0137 | Eval Loss: 0.0086 | Eval R2: -1.6083 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0133 | Eval Loss: 0.0084 | Eval R2: -1.6082 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007463, test R2 score: -2.204159
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.013302180916070938, 'r2_eval_final': -1.6081876754760742, 'loss_eval_final': 0.008430029265582561, 'r2_test': -2.2041592608908744, 'loss_test': 0.007463482208549976, 'loss_nodes': [[0.009258203208446503, 0.002844726899638772, 0.012989882379770279, 0.002518606139346957, 0.0025782219599932432, 0.009530997835099697, 0.003936316817998886, 0.0069677941501140594, 0.009197692386806011, 0.003009133506566286, 0.006136271636933088, 0.004906109068542719, 0.00696664210408926, 0.0121295265853405, 0.00849416945129633, 0.007642752956598997, 0.0073801446706056595, 0.014945722185075283, 0.008596071042120457, 0.00924064964056015]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0133
wandb: loss_eval 0.00843
wandb: loss_test 0.00746
wandb:   r2_eval -1.60819
wandb:   r2_test -2.20416
wandb: 
wandb: ðŸš€ View run autumn-firefly-37 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/eyrxe1e4
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_115344-eyrxe1e4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 14%|â–ˆâ–        | 7/50 [3:58:32<28:53:46, 2419.22s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_130127-4lnr3jwc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dainty-pine-38
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/4lnr3jwc

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1016 | Eval Loss: 0.6733 | Eval R2: -868.3806 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5785 | Eval Loss: 0.4192 | Eval R2: -535.0624 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3631 | Eval Loss: 0.2578 | Eval R2: -321.3682 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2267 | Eval Loss: 0.1572 | Eval R2: -187.5701 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1406 | Eval Loss: 0.0930 | Eval R2: -102.5438 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0850 | Eval Loss: 0.0530 | Eval R2: -50.3472 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0516 | Eval Loss: 0.0304 | Eval R2: -21.3650 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0334 | Eval Loss: 0.0196 | Eval R2: -8.2236 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0252 | Eval Loss: 0.0157 | Eval R2: -4.0900 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0219 | Eval Loss: 0.0140 | Eval R2: -3.1503 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0202 | Eval Loss: 0.0129 | Eval R2: -2.8087 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0188 | Eval Loss: 0.0118 | Eval R2: -2.4210 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0177 | Eval Loss: 0.0109 | Eval R2: -2.1284 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0166 | Eval Loss: 0.0103 | Eval R2: -1.9321 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0158 | Eval Loss: 0.0098 | Eval R2: -1.8366 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0153 | Eval Loss: 0.0095 | Eval R2: -1.7299 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0148 | Eval Loss: 0.0093 | Eval R2: -1.6327 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0144 | Eval Loss: 0.0091 | Eval R2: -1.5570 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0139 | Eval Loss: 0.0088 | Eval R2: -1.5204 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0134 | Eval Loss: 0.0084 | Eval R2: -1.4074 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0129 | Eval Loss: 0.0081 | Eval R2: -1.3138 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0124 | Eval Loss: 0.0078 | Eval R2: -1.2234 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0119 | Eval Loss: 0.0075 | Eval R2: -1.1695 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0117 | Eval Loss: 0.0073 | Eval R2: -1.1285 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0112 | Eval Loss: 0.0070 | Eval R2: -1.0096 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0108 | Eval Loss: 0.0068 | Eval R2: -0.9335 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0102 | Eval Loss: 0.0064 | Eval R2: -0.8891 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006224, test R2 score: -1.726074
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010232152417302132, 'r2_eval_final': -0.8891273736953735, 'loss_eval_final': 0.006420284043997526, 'r2_test': -1.726074414595915, 'loss_test': 0.006224362179636955, 'loss_nodes': [[0.00255738478153944, 0.0044868104159832, 0.003281604964286089, 0.0038428755942732096, 0.003409602679312229, 0.0035783343482762575, 0.004129703156650066, 0.006960547063499689, 0.007775180507451296, 0.0045297881588339806, 0.0048217675648629665, 0.009158851578831673, 0.005898178555071354, 0.006226325873285532, 0.00698615750297904, 0.009684575721621513, 0.008867115713655949, 0.009593985043466091, 0.010390919633209705, 0.008307537995278835]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01023
wandb: loss_eval 0.00642
wandb: loss_test 0.00622
wandb:   r2_eval -0.88913
wandb:   r2_test -1.72607
wandb: 
wandb: ðŸš€ View run dainty-pine-38 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/4lnr3jwc
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_130127-4lnr3jwc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 16%|â–ˆâ–Œ        | 8/50 [4:19:58<24:00:54, 2058.43s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_132253-0r3j6gnd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run kind-cosmos-39
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/0r3j6gnd

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6260 | Eval Loss: 0.9184 | Eval R2: -1193.3700 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8937 | Eval Loss: 0.7492 | Eval R2: -975.5931 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7005 | Eval Loss: 0.5693 | Eval R2: -735.8911 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5378 | Eval Loss: 0.4308 | Eval R2: -546.5880 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4036 | Eval Loss: 0.3122 | Eval R2: -390.7799 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2946 | Eval Loss: 0.2221 | Eval R2: -267.9971 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2110 | Eval Loss: 0.1546 | Eval R2: -176.8956 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1498 | Eval Loss: 0.1064 | Eval R2: -112.7954 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1069 | Eval Loss: 0.0738 | Eval R2: -69.6711 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0781 | Eval Loss: 0.0518 | Eval R2: -42.1352 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0587 | Eval Loss: 0.0366 | Eval R2: -25.0063 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0456 | Eval Loss: 0.0276 | Eval R2: -15.5697 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0361 | Eval Loss: 0.0218 | Eval R2: -10.7827 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0291 | Eval Loss: 0.0183 | Eval R2: -8.6954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0245 | Eval Loss: 0.0168 | Eval R2: -8.4377 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0140 | Eval R2: -5.5006 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0118 | Eval R2: -3.5593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0112 | Eval R2: -3.2574 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0171 | Eval Loss: 0.0105 | Eval R2: -2.7739 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0163 | Eval Loss: 0.0100 | Eval R2: -2.2395 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0157 | Eval Loss: 0.0096 | Eval R2: -1.9573 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0152 | Eval Loss: 0.0093 | Eval R2: -1.7769 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0148 | Eval Loss: 0.0090 | Eval R2: -1.6431 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0144 | Eval Loss: 0.0089 | Eval R2: -1.6965 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0140 | Eval Loss: 0.0087 | Eval R2: -1.6233 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0137 | Eval Loss: 0.0086 | Eval R2: -1.6083 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0133 | Eval Loss: 0.0084 | Eval R2: -1.6082 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007463, test R2 score: -2.204159
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.013302180916070938, 'r2_eval_final': -1.6081876754760742, 'loss_eval_final': 0.008430029265582561, 'r2_test': -2.2041592608908744, 'loss_test': 0.007463482208549976, 'loss_nodes': [[0.009258203208446503, 0.002844726899638772, 0.012989882379770279, 0.002518606139346957, 0.0025782219599932432, 0.009530997835099697, 0.003936316817998886, 0.0069677941501140594, 0.009197692386806011, 0.003009133506566286, 0.006136271636933088, 0.004906109068542719, 0.00696664210408926, 0.0121295265853405, 0.00849416945129633, 0.007642752956598997, 0.0073801446706056595, 0.014945722185075283, 0.008596071042120457, 0.00924064964056015]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0133
wandb: loss_eval 0.00843
wandb: loss_test 0.00746
wandb:   r2_eval -1.60819
wandb:   r2_test -2.20416
wandb: 
wandb: ðŸš€ View run kind-cosmos-39 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/0r3j6gnd
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_132253-0r3j6gnd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 18%|â–ˆâ–Š        | 9/50 [5:28:40<30:47:19, 2703.41s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_143135-sc1o6047
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cerulean-meadow-40
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/sc1o6047

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6037 | Eval Loss: 1.3235 | Eval R2: -1729.7567 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2474 | Eval Loss: 1.0598 | Eval R2: -1386.9615 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.0112 | Eval Loss: 0.8600 | Eval R2: -1125.2484 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.8274 | Eval Loss: 0.7014 | Eval R2: -915.4377 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.6751 | Eval Loss: 0.5654 | Eval R2: -734.3500 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.5423 | Eval Loss: 0.4440 | Eval R2: -572.3257 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4221 | Eval Loss: 0.3342 | Eval R2: -424.9161 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3133 | Eval Loss: 0.2372 | Eval R2: -294.4873 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2191 | Eval Loss: 0.1568 | Eval R2: -186.5816 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1437 | Eval Loss: 0.0964 | Eval R2: -105.3630 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0894 | Eval Loss: 0.0563 | Eval R2: -51.8447 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0554 | Eval Loss: 0.0339 | Eval R2: -22.5081 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0377 | Eval Loss: 0.0239 | Eval R2: -9.7937 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0301 | Eval Loss: 0.0201 | Eval R2: -5.6118 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0272 | Eval Loss: 0.0187 | Eval R2: -4.4547 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0258 | Eval Loss: 0.0182 | Eval R2: -4.0647 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0249 | Eval Loss: 0.0177 | Eval R2: -3.8671 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0241 | Eval Loss: 0.0173 | Eval R2: -3.7169 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0233 | Eval Loss: 0.0168 | Eval R2: -3.5759 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0226 | Eval Loss: 0.0165 | Eval R2: -3.5357 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0219 | Eval Loss: 0.0160 | Eval R2: -3.3730 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0213 | Eval Loss: 0.0158 | Eval R2: -3.4130 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0206 | Eval Loss: 0.0154 | Eval R2: -3.3962 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0199 | Eval Loss: 0.0152 | Eval R2: -3.6498 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0193 | Eval Loss: 0.0151 | Eval R2: -4.0138 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0185 | Eval Loss: 0.0145 | Eval R2: -4.1862 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0179 | Eval Loss: 0.0141 | Eval R2: -4.1343 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.013168, test R2 score: -4.589671
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.01785048097372055, 'r2_eval_final': -4.134255409240723, 'loss_eval_final': 0.014061445370316505, 'r2_test': -4.589671265726612, 'loss_test': 0.013168364763259888, 'loss_nodes': [[0.0037669409066438675, 0.021805422380566597, 0.033631548285484314, 0.0091044120490551, 0.005908290855586529, 0.009246245957911015, 0.020137950778007507, 0.006764761637896299, 0.005711577832698822, 0.006203227210789919, 0.005674563813954592, 0.03834736719727516, 0.00652287807315588, 0.009299303404986858, 0.02370711788535118, 0.010707885026931763, 0.015437405556440353, 0.00904020294547081, 0.008018750697374344, 0.014331413432955742]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01785
wandb: loss_eval 0.01406
wandb: loss_test 0.01317
wandb:   r2_eval -4.13426
wandb:   r2_test -4.58967
wandb: 
wandb: ðŸš€ View run cerulean-meadow-40 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/sc1o6047
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_143135-sc1o6047/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 20%|â–ˆâ–ˆ        | 10/50 [5:54:07<26:00:08, 2340.22s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_145702-kdeu1quy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glorious-disco-41
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/kdeu1quy

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.3282 | Eval Loss: 1.6572 | Eval R2: -2150.4080 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.5703 | Eval Loss: 1.3522 | Eval R2: -1755.8922 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.3168 | Eval Loss: 1.1583 | Eval R2: -1508.8317 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.1326 | Eval Loss: 0.9951 | Eval R2: -1296.5870 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9874 | Eval Loss: 0.8761 | Eval R2: -1141.4620 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.8677 | Eval Loss: 0.7706 | Eval R2: -1003.1132 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.7659 | Eval Loss: 0.6806 | Eval R2: -884.1174 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.6778 | Eval Loss: 0.6020 | Eval R2: -779.5423 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.6011 | Eval Loss: 0.5333 | Eval R2: -688.2289 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.5342 | Eval Loss: 0.4734 | Eval R2: -608.5543 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4755 | Eval Loss: 0.4208 | Eval R2: -538.8421 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.4237 | Eval Loss: 0.3742 | Eval R2: -477.2992 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3777 | Eval Loss: 0.3328 | Eval R2: -422.7291 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3367 | Eval Loss: 0.2958 | Eval R2: -374.0954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2997 | Eval Loss: 0.2625 | Eval R2: -330.5504 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2665 | Eval Loss: 0.2326 | Eval R2: -291.4294 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2365 | Eval Loss: 0.2054 | Eval R2: -256.1612 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2092 | Eval Loss: 0.1809 | Eval R2: -224.3621 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1845 | Eval Loss: 0.1586 | Eval R2: -195.6070 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1622 | Eval Loss: 0.1385 | Eval R2: -169.6536 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1419 | Eval Loss: 0.1203 | Eval R2: -146.1759 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1236 | Eval Loss: 0.1038 | Eval R2: -125.0420 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1070 | Eval Loss: 0.0891 | Eval R2: -106.0660 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0923 | Eval Loss: 0.0759 | Eval R2: -89.1881 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0792 | Eval Loss: 0.0644 | Eval R2: -74.3706 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0677 | Eval Loss: 0.0544 | Eval R2: -61.4893 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0577 | Eval Loss: 0.0457 | Eval R2: -50.3745 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.046126, test R2 score: -44.306491
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05773135647177696, 'r2_eval_final': -50.37446594238281, 'loss_eval_final': 0.045736148953437805, 'r2_test': -44.306490943999, 'loss_test': 0.04612583667039871, 'loss_nodes': [[0.2803061306476593, 0.01629689894616604, 0.010288788005709648, 0.004464718047529459, 0.007843839935958385, 0.00891904253512621, 0.015844933688640594, 0.009098434820771217, 0.013574357144534588, 0.00996175967156887, 0.01902969740331173, 0.05328703671693802, 0.010281577706336975, 0.015395130030810833, 0.010568448342382908, 0.022547461092472076, 0.33414772152900696, 0.014339873567223549, 0.014233856461942196, 0.05208705738186836]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.002 MB of 0.003 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05773
wandb: loss_eval 0.04574
wandb: loss_test 0.04613
wandb:   r2_eval -50.37447
wandb:   r2_test -44.30649
wandb: 
wandb: ðŸš€ View run glorious-disco-41 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/kdeu1quy
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_145702-kdeu1quy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 22%|â–ˆâ–ˆâ–       | 11/50 [6:06:17<20:00:48, 1847.40s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_150912-sl79j8tl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run snowy-frog-42
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/sl79j8tl

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.1175 | Eval Loss: 1.3449 | Eval R2: -1754.3196 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2933 | Eval Loss: 1.1474 | Eval R2: -1504.5608 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1382 | Eval Loss: 1.0401 | Eval R2: -1366.1879 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0502 | Eval Loss: 0.9654 | Eval R2: -1269.3722 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9783 | Eval Loss: 0.9011 | Eval R2: -1185.9937 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9168 | Eval Loss: 0.8435 | Eval R2: -1110.6115 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8616 | Eval Loss: 0.7932 | Eval R2: -1044.3988 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.8103 | Eval Loss: 0.7449 | Eval R2: -980.2150 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.7593 | Eval Loss: 0.6945 | Eval R2: -913.0714 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.7051 | Eval Loss: 0.6380 | Eval R2: -836.5386 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.6470 | Eval Loss: 0.5811 | Eval R2: -759.8204 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.5867 | Eval Loss: 0.5214 | Eval R2: -679.7445 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.5240 | Eval Loss: 0.4600 | Eval R2: -596.6365 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.4599 | Eval Loss: 0.3988 | Eval R2: -512.8257 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3968 | Eval Loss: 0.3396 | Eval R2: -431.8340 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.3367 | Eval Loss: 0.2841 | Eval R2: -356.0157 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2811 | Eval Loss: 0.2339 | Eval R2: -287.4590 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2317 | Eval Loss: 0.1904 | Eval R2: -227.9376 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1895 | Eval Loss: 0.1541 | Eval R2: -178.0724 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1546 | Eval Loss: 0.1246 | Eval R2: -137.6694 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1266 | Eval Loss: 0.1013 | Eval R2: -105.7636 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1047 | Eval Loss: 0.0831 | Eval R2: -81.0401 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0876 | Eval Loss: 0.0691 | Eval R2: -62.1930 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0746 | Eval Loss: 0.0585 | Eval R2: -47.9218 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0647 | Eval Loss: 0.0503 | Eval R2: -37.2229 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0569 | Eval Loss: 0.0441 | Eval R2: -29.1635 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0511 | Eval Loss: 0.0392 | Eval R2: -23.1495 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.037154, test R2 score: -22.710716
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05107304826378822, 'r2_eval_final': -23.149497985839844, 'loss_eval_final': 0.0392075851559639, 'r2_test': -22.710716384304437, 'loss_test': 0.037153519690036774, 'loss_nodes': [[0.025696467608213425, 0.028831860050559044, 0.020902646705508232, 0.06295966356992722, 0.026749033480882645, 0.028263935819268227, 0.02382110431790352, 0.033269356936216354, 0.029641836881637573, 0.03743165358901024, 0.02297825738787651, 0.02518240176141262, 0.02551611140370369, 0.038175154477357864, 0.029250068590044975, 0.03381653502583504, 0.028005244210362434, 0.024492226541042328, 0.06226000189781189, 0.1358269900083542]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05107
wandb: loss_eval 0.03921
wandb: loss_test 0.03715
wandb:   r2_eval -23.1495
wandb:   r2_test -22.71072
wandb: 
wandb: ðŸš€ View run snowy-frog-42 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/sl79j8tl
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_150912-sl79j8tl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 24%|â–ˆâ–ˆâ–       | 12/50 [7:10:45<25:59:18, 2462.05s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_161340-2x7bbqol
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run playful-sound-43
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/2x7bbqol

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6260 | Eval Loss: 0.9184 | Eval R2: -1193.3700 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8937 | Eval Loss: 0.7492 | Eval R2: -975.5931 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7005 | Eval Loss: 0.5693 | Eval R2: -735.8911 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5378 | Eval Loss: 0.4308 | Eval R2: -546.5880 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4036 | Eval Loss: 0.3122 | Eval R2: -390.7799 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2946 | Eval Loss: 0.2221 | Eval R2: -267.9971 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2110 | Eval Loss: 0.1546 | Eval R2: -176.8956 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1498 | Eval Loss: 0.1064 | Eval R2: -112.7954 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1069 | Eval Loss: 0.0738 | Eval R2: -69.6711 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0781 | Eval Loss: 0.0518 | Eval R2: -42.1352 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0587 | Eval Loss: 0.0366 | Eval R2: -25.0063 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0456 | Eval Loss: 0.0276 | Eval R2: -15.5697 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0361 | Eval Loss: 0.0218 | Eval R2: -10.7827 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0291 | Eval Loss: 0.0183 | Eval R2: -8.6954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0245 | Eval Loss: 0.0168 | Eval R2: -8.4377 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0140 | Eval R2: -5.5006 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0118 | Eval R2: -3.5593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0112 | Eval R2: -3.2574 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0171 | Eval Loss: 0.0105 | Eval R2: -2.7739 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0163 | Eval Loss: 0.0100 | Eval R2: -2.2395 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0157 | Eval Loss: 0.0096 | Eval R2: -1.9573 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0152 | Eval Loss: 0.0093 | Eval R2: -1.7769 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0148 | Eval Loss: 0.0090 | Eval R2: -1.6431 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0144 | Eval Loss: 0.0089 | Eval R2: -1.6965 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0140 | Eval Loss: 0.0087 | Eval R2: -1.6233 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0137 | Eval Loss: 0.0086 | Eval R2: -1.6083 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0133 | Eval Loss: 0.0084 | Eval R2: -1.6082 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007463, test R2 score: -2.204159
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.013302180916070938, 'r2_eval_final': -1.6081876754760742, 'loss_eval_final': 0.008430029265582561, 'r2_test': -2.2041592608908744, 'loss_test': 0.007463482208549976, 'loss_nodes': [[0.009258203208446503, 0.002844726899638772, 0.012989882379770279, 0.002518606139346957, 0.0025782219599932432, 0.009530997835099697, 0.003936316817998886, 0.0069677941501140594, 0.009197692386806011, 0.003009133506566286, 0.006136271636933088, 0.004906109068542719, 0.00696664210408926, 0.0121295265853405, 0.00849416945129633, 0.007642752956598997, 0.0073801446706056595, 0.014945722185075283, 0.008596071042120457, 0.00924064964056015]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0133
wandb: loss_eval 0.00843
wandb: loss_test 0.00746
wandb:   r2_eval -1.60819
wandb:   r2_test -2.20416
wandb: 
wandb: ðŸš€ View run playful-sound-43 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/2x7bbqol
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_161340-2x7bbqol/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 26%|â–ˆâ–ˆâ–Œ       | 13/50 [8:20:02<30:35:01, 2975.71s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_172257-8pmzls8n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deft-salad-44
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/8pmzls8n

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1016 | Eval Loss: 0.6733 | Eval R2: -868.3806 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5785 | Eval Loss: 0.4192 | Eval R2: -535.0624 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3631 | Eval Loss: 0.2578 | Eval R2: -321.3682 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2267 | Eval Loss: 0.1572 | Eval R2: -187.5701 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1406 | Eval Loss: 0.0930 | Eval R2: -102.5438 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0850 | Eval Loss: 0.0530 | Eval R2: -50.3472 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0516 | Eval Loss: 0.0304 | Eval R2: -21.3650 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0334 | Eval Loss: 0.0196 | Eval R2: -8.2236 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0252 | Eval Loss: 0.0157 | Eval R2: -4.0900 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0219 | Eval Loss: 0.0140 | Eval R2: -3.1503 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0202 | Eval Loss: 0.0129 | Eval R2: -2.8087 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0188 | Eval Loss: 0.0118 | Eval R2: -2.4210 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0177 | Eval Loss: 0.0109 | Eval R2: -2.1284 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0166 | Eval Loss: 0.0103 | Eval R2: -1.9321 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0158 | Eval Loss: 0.0098 | Eval R2: -1.8366 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0153 | Eval Loss: 0.0095 | Eval R2: -1.7299 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0148 | Eval Loss: 0.0093 | Eval R2: -1.6327 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0144 | Eval Loss: 0.0091 | Eval R2: -1.5570 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0139 | Eval Loss: 0.0088 | Eval R2: -1.5204 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0134 | Eval Loss: 0.0084 | Eval R2: -1.4074 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0129 | Eval Loss: 0.0081 | Eval R2: -1.3138 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0124 | Eval Loss: 0.0078 | Eval R2: -1.2234 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0119 | Eval Loss: 0.0075 | Eval R2: -1.1695 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0117 | Eval Loss: 0.0073 | Eval R2: -1.1285 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0112 | Eval Loss: 0.0070 | Eval R2: -1.0096 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0108 | Eval Loss: 0.0068 | Eval R2: -0.9335 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0102 | Eval Loss: 0.0064 | Eval R2: -0.8891 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006224, test R2 score: -1.726074
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010232152417302132, 'r2_eval_final': -0.8891273736953735, 'loss_eval_final': 0.006420284043997526, 'r2_test': -1.726074414595915, 'loss_test': 0.006224362179636955, 'loss_nodes': [[0.00255738478153944, 0.0044868104159832, 0.003281604964286089, 0.0038428755942732096, 0.003409602679312229, 0.0035783343482762575, 0.004129703156650066, 0.006960547063499689, 0.007775180507451296, 0.0045297881588339806, 0.0048217675648629665, 0.009158851578831673, 0.005898178555071354, 0.006226325873285532, 0.00698615750297904, 0.009684575721621513, 0.008867115713655949, 0.009593985043466091, 0.010390919633209705, 0.008307537995278835]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01023
wandb: loss_eval 0.00642
wandb: loss_test 0.00622
wandb:   r2_eval -0.88913
wandb:   r2_test -1.72607
wandb: 
wandb: ðŸš€ View run deft-salad-44 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/8pmzls8n
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_172257-8pmzls8n/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 28%|â–ˆâ–ˆâ–Š       | 14/50 [8:42:18<24:48:14, 2480.40s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_174513-gw872rk7
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run blooming-sun-45
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/gw872rk7

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1647 | Eval Loss: 0.8971 | Eval R2: -1185.9130 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8880 | Eval Loss: 0.7891 | Eval R2: -1044.2429 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7825 | Eval Loss: 0.6907 | Eval R2: -914.2036 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6835 | Eval Loss: 0.5955 | Eval R2: -787.6053 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5857 | Eval Loss: 0.5005 | Eval R2: -660.3394 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4885 | Eval Loss: 0.4078 | Eval R2: -535.3315 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3952 | Eval Loss: 0.3220 | Eval R2: -418.4061 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3114 | Eval Loss: 0.2482 | Eval R2: -316.2601 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2405 | Eval Loss: 0.1883 | Eval R2: -232.0029 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1842 | Eval Loss: 0.1425 | Eval R2: -166.3681 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1415 | Eval Loss: 0.1089 | Eval R2: -117.3879 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1103 | Eval Loss: 0.0848 | Eval R2: -81.9802 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0882 | Eval Loss: 0.0681 | Eval R2: -57.0837 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0729 | Eval Loss: 0.0566 | Eval R2: -40.0630 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0625 | Eval Loss: 0.0491 | Eval R2: -28.7597 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0558 | Eval Loss: 0.0442 | Eval R2: -21.4604 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0515 | Eval Loss: 0.0412 | Eval R2: -16.8597 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0488 | Eval Loss: 0.0394 | Eval R2: -14.0144 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0472 | Eval Loss: 0.0383 | Eval R2: -12.2792 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0462 | Eval Loss: 0.0377 | Eval R2: -11.2326 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0457 | Eval Loss: 0.0374 | Eval R2: -10.6074 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0453 | Eval Loss: 0.0372 | Eval R2: -10.2375 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0452 | Eval Loss: 0.0371 | Eval R2: -10.0206 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0451 | Eval Loss: 0.0370 | Eval R2: -9.8944 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.8212 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7788 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7541 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032334, test R2 score: -11.474966
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04495033621788025, 'r2_eval_final': -9.754081726074219, 'loss_eval_final': 0.036978621035814285, 'r2_test': -11.474966239449667, 'loss_test': 0.03233417496085167, 'loss_nodes': [[0.031103795394301414, 0.032650381326675415, 0.03280549868941307, 0.03139203041791916, 0.03299599140882492, 0.032074905931949615, 0.03305134177207947, 0.03343836963176727, 0.03205472603440285, 0.032135386019945145, 0.03186318650841713, 0.032436106353998184, 0.03259802982211113, 0.03131670504808426, 0.03297997638583183, 0.031207647174596786, 0.03250662982463837, 0.03298945724964142, 0.03248253092169762, 0.03260083124041557]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.005 MB of 0.009 MB uploadedwandb: / 0.005 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04495
wandb: loss_eval 0.03698
wandb: loss_test 0.03233
wandb:   r2_eval -9.75408
wandb:   r2_test -11.47497
wandb: 
wandb: ðŸš€ View run blooming-sun-45 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/gw872rk7
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_174513-gw872rk7/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [9:02:10<20:20:14, 2091.86s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_180505-8rmxreux
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run bright-valley-46
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/8rmxreux

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1647 | Eval Loss: 0.8971 | Eval R2: -1185.9130 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8880 | Eval Loss: 0.7891 | Eval R2: -1044.2429 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7825 | Eval Loss: 0.6907 | Eval R2: -914.2036 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6835 | Eval Loss: 0.5955 | Eval R2: -787.6053 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5857 | Eval Loss: 0.5005 | Eval R2: -660.3394 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4885 | Eval Loss: 0.4078 | Eval R2: -535.3315 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3952 | Eval Loss: 0.3220 | Eval R2: -418.4061 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3114 | Eval Loss: 0.2482 | Eval R2: -316.2601 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2405 | Eval Loss: 0.1883 | Eval R2: -232.0029 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1842 | Eval Loss: 0.1425 | Eval R2: -166.3681 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1415 | Eval Loss: 0.1089 | Eval R2: -117.3879 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1103 | Eval Loss: 0.0848 | Eval R2: -81.9802 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0882 | Eval Loss: 0.0681 | Eval R2: -57.0837 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0729 | Eval Loss: 0.0566 | Eval R2: -40.0630 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0625 | Eval Loss: 0.0491 | Eval R2: -28.7597 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0558 | Eval Loss: 0.0442 | Eval R2: -21.4604 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0515 | Eval Loss: 0.0412 | Eval R2: -16.8597 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0488 | Eval Loss: 0.0394 | Eval R2: -14.0144 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0472 | Eval Loss: 0.0383 | Eval R2: -12.2792 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0462 | Eval Loss: 0.0377 | Eval R2: -11.2326 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0457 | Eval Loss: 0.0374 | Eval R2: -10.6074 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0453 | Eval Loss: 0.0372 | Eval R2: -10.2375 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0452 | Eval Loss: 0.0371 | Eval R2: -10.0206 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0451 | Eval Loss: 0.0370 | Eval R2: -9.8944 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.8212 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7788 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7541 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032334, test R2 score: -11.474966
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.04495033621788025, 'r2_eval_final': -9.754081726074219, 'loss_eval_final': 0.036978621035814285, 'r2_test': -11.474966239449667, 'loss_test': 0.03233417496085167, 'loss_nodes': [[0.031103795394301414, 0.032650381326675415, 0.03280549868941307, 0.03139203041791916, 0.03299599140882492, 0.032074905931949615, 0.03305134177207947, 0.03343836963176727, 0.03205472603440285, 0.032135386019945145, 0.03186318650841713, 0.032436106353998184, 0.03259802982211113, 0.03131670504808426, 0.03297997638583183, 0.031207647174596786, 0.03250662982463837, 0.03298945724964142, 0.03248253092169762, 0.03260083124041557]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04495
wandb: loss_eval 0.03698
wandb: loss_test 0.03233
wandb:   r2_eval -9.75408
wandb:   r2_test -11.47497
wandb: 
wandb: ðŸš€ View run bright-valley-46 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/8rmxreux
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_180505-8rmxreux/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [9:22:56<17:21:14, 1837.49s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_182552-uqsm2a9j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-vortex-47
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/uqsm2a9j

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9194 | Eval Loss: 0.6593 | Eval R2: -864.3677 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6421 | Eval Loss: 0.5349 | Eval R2: -696.6136 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5257 | Eval Loss: 0.4308 | Eval R2: -559.8137 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4209 | Eval Loss: 0.3341 | Eval R2: -428.2673 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3210 | Eval Loss: 0.2409 | Eval R2: -301.2177 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2264 | Eval Loss: 0.1564 | Eval R2: -187.2426 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1456 | Eval Loss: 0.0905 | Eval R2: -98.3464 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0873 | Eval Loss: 0.0489 | Eval R2: -43.0070 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0538 | Eval Loss: 0.0288 | Eval R2: -17.1743 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0383 | Eval Loss: 0.0207 | Eval R2: -7.6842 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0311 | Eval Loss: 0.0174 | Eval R2: -4.7720 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0273 | Eval Loss: 0.0155 | Eval R2: -3.7666 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0244 | Eval Loss: 0.0139 | Eval R2: -3.3801 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0218 | Eval Loss: 0.0126 | Eval R2: -3.5862 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0194 | Eval Loss: 0.0117 | Eval R2: -3.3260 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0179 | Eval Loss: 0.0109 | Eval R2: -2.9284 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0168 | Eval Loss: 0.0104 | Eval R2: -2.6831 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0159 | Eval Loss: 0.0102 | Eval R2: -2.7834 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0152 | Eval Loss: 0.0100 | Eval R2: -2.9554 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0146 | Eval Loss: 0.0097 | Eval R2: -2.8247 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0140 | Eval Loss: 0.0096 | Eval R2: -2.9354 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0134 | Eval Loss: 0.0093 | Eval R2: -2.8358 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0129 | Eval Loss: 0.0091 | Eval R2: -2.8040 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0087 | Eval R2: -2.5221 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0118 | Eval Loss: 0.0086 | Eval R2: -2.6712 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0114 | Eval Loss: 0.0083 | Eval R2: -2.4447 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0109 | Eval Loss: 0.0081 | Eval R2: -2.3499 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008231, test R2 score: -2.964317
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.010922707617282867, 'r2_eval_final': -2.3498620986938477, 'loss_eval_final': 0.008073534816503525, 'r2_test': -2.9643172658565593, 'loss_test': 0.008230721578001976, 'loss_nodes': [[0.0028403077740222216, 0.005961157381534576, 0.024906020611524582, 0.008386037312448025, 0.005682543385773897, 0.0034494951833039522, 0.0030910796485841274, 0.006750152446329594, 0.006295640952885151, 0.007915401831269264, 0.005978188011795282, 0.00793306902050972, 0.004925440531224012, 0.014202958904206753, 0.016627872362732887, 0.01348226610571146, 0.0068251509219408035, 0.007773402612656355, 0.0054478272795677185, 0.006140387617051601]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01092
wandb: loss_eval 0.00807
wandb: loss_test 0.00823
wandb:   r2_eval -2.34986
wandb:   r2_test -2.96432
wandb: 
wandb: ðŸš€ View run charmed-vortex-47 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/uqsm2a9j
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_182552-uqsm2a9j/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [9:50:50<16:23:34, 1788.32s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_185346-y1ddgbtn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wise-lake-48
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/y1ddgbtn

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9194 | Eval Loss: 0.6593 | Eval R2: -864.3677 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6421 | Eval Loss: 0.5349 | Eval R2: -696.6136 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5257 | Eval Loss: 0.4308 | Eval R2: -559.8137 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4209 | Eval Loss: 0.3341 | Eval R2: -428.2673 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3210 | Eval Loss: 0.2409 | Eval R2: -301.2177 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2264 | Eval Loss: 0.1564 | Eval R2: -187.2426 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1456 | Eval Loss: 0.0905 | Eval R2: -98.3464 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0873 | Eval Loss: 0.0489 | Eval R2: -43.0070 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0538 | Eval Loss: 0.0288 | Eval R2: -17.1743 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0383 | Eval Loss: 0.0207 | Eval R2: -7.6842 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0311 | Eval Loss: 0.0174 | Eval R2: -4.7720 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0273 | Eval Loss: 0.0155 | Eval R2: -3.7666 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0244 | Eval Loss: 0.0139 | Eval R2: -3.3801 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0218 | Eval Loss: 0.0126 | Eval R2: -3.5862 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0194 | Eval Loss: 0.0117 | Eval R2: -3.3260 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0179 | Eval Loss: 0.0109 | Eval R2: -2.9284 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0168 | Eval Loss: 0.0104 | Eval R2: -2.6831 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0159 | Eval Loss: 0.0102 | Eval R2: -2.7834 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0152 | Eval Loss: 0.0100 | Eval R2: -2.9554 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0146 | Eval Loss: 0.0097 | Eval R2: -2.8247 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0140 | Eval Loss: 0.0096 | Eval R2: -2.9354 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0134 | Eval Loss: 0.0093 | Eval R2: -2.8358 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0129 | Eval Loss: 0.0091 | Eval R2: -2.8040 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0087 | Eval R2: -2.5221 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0118 | Eval Loss: 0.0086 | Eval R2: -2.6712 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0114 | Eval Loss: 0.0083 | Eval R2: -2.4447 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0109 | Eval Loss: 0.0081 | Eval R2: -2.3499 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008231, test R2 score: -2.964317
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.010922707617282867, 'r2_eval_final': -2.3498620986938477, 'loss_eval_final': 0.008073534816503525, 'r2_test': -2.9643172658565593, 'loss_test': 0.008230721578001976, 'loss_nodes': [[0.0028403077740222216, 0.005961157381534576, 0.024906020611524582, 0.008386037312448025, 0.005682543385773897, 0.0034494951833039522, 0.0030910796485841274, 0.006750152446329594, 0.006295640952885151, 0.007915401831269264, 0.005978188011795282, 0.00793306902050972, 0.004925440531224012, 0.014202958904206753, 0.016627872362732887, 0.01348226610571146, 0.0068251509219408035, 0.007773402612656355, 0.0054478272795677185, 0.006140387617051601]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01092
wandb: loss_eval 0.00807
wandb: loss_test 0.00823
wandb:   r2_eval -2.34986
wandb:   r2_test -2.96432
wandb: 
wandb: ðŸš€ View run wise-lake-48 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/y1ddgbtn
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_185346-y1ddgbtn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [10:18:29<15:33:02, 1749.44s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_192124-5rw113de
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run toasty-aardvark-49
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/5rw113de

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3984 | Eval Loss: 0.9513 | Eval R2: -1247.5632 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7276 | Eval Loss: 0.4089 | Eval R2: -513.5609 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2575 | Eval Loss: 0.1043 | Eval R2: -100.4691 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0757 | Eval Loss: 0.0429 | Eval R2: -20.6384 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0451 | Eval Loss: 0.0315 | Eval R2: -10.0340 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0367 | Eval Loss: 0.0238 | Eval R2: -7.0185 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0292 | Eval Loss: 0.0176 | Eval R2: -4.8196 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0240 | Eval Loss: 0.0144 | Eval R2: -3.5952 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0201 | Eval Loss: 0.0124 | Eval R2: -2.7052 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0175 | Eval Loss: 0.0110 | Eval R2: -2.1662 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0159 | Eval Loss: 0.0103 | Eval R2: -1.9067 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0152 | Eval Loss: 0.0098 | Eval R2: -1.7693 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0146 | Eval Loss: 0.0093 | Eval R2: -1.6352 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0139 | Eval Loss: 0.0088 | Eval R2: -1.4991 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0133 | Eval Loss: 0.0085 | Eval R2: -1.3505 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0126 | Eval Loss: 0.0080 | Eval R2: -1.2274 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0119 | Eval Loss: 0.0077 | Eval R2: -1.1147 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0113 | Eval Loss: 0.0073 | Eval R2: -1.0024 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0108 | Eval Loss: 0.0069 | Eval R2: -0.8902 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0102 | Eval Loss: 0.0066 | Eval R2: -0.7920 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0098 | Eval Loss: 0.0063 | Eval R2: -0.7086 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0093 | Eval Loss: 0.0061 | Eval R2: -0.6219 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0088 | Eval Loss: 0.0059 | Eval R2: -0.5541 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0084 | Eval Loss: 0.0057 | Eval R2: -0.5006 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0082 | Eval Loss: 0.0055 | Eval R2: -0.4558 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0079 | Eval Loss: 0.0053 | Eval R2: -0.4142 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0076 | Eval Loss: 0.0052 | Eval R2: -0.3704 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.004660, test R2 score: -0.920842
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.007595960516482592, 'r2_eval_final': -0.3703700602054596, 'loss_eval_final': 0.005152712110430002, 'r2_test': -0.92084187800612, 'loss_test': 0.004660170990973711, 'loss_nodes': [[0.003372827311977744, 0.005095046944916248, 0.0023228430654853582, 0.0023671421222388744, 0.002287001581862569, 0.003568254876881838, 0.0023949765600264072, 0.003668769495561719, 0.0032979585230350494, 0.005981083028018475, 0.003596970345824957, 0.0041025783866643906, 0.005883212201297283, 0.004800948780030012, 0.0052741337567567825, 0.010215455666184425, 0.005729535594582558, 0.005404310766607523, 0.00662642065435648, 0.007213954348117113]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0076
wandb: loss_eval 0.00515
wandb: loss_test 0.00466
wandb:   r2_eval -0.37037
wandb:   r2_test -0.92084
wandb: 
wandb: ðŸš€ View run toasty-aardvark-49 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/5rw113de
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_192124-5rw113de/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [11:26:52<21:09:02, 2456.22s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_202947-sxtgnke3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-frog-50
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/sxtgnke3

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3182 | Eval Loss: 1.0123 | Eval R2: -1324.4379 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.9830 | Eval Loss: 0.8684 | Eval R2: -1138.7653 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.8501 | Eval Loss: 0.7513 | Eval R2: -986.9944 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7412 | Eval Loss: 0.6547 | Eval R2: -860.9456 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.6499 | Eval Loss: 0.5724 | Eval R2: -752.7508 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.5707 | Eval Loss: 0.5003 | Eval R2: -657.0286 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5003 | Eval Loss: 0.4358 | Eval R2: -570.6354 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4369 | Eval Loss: 0.3779 | Eval R2: -492.2425 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3798 | Eval Loss: 0.3264 | Eval R2: -421.6185 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3289 | Eval Loss: 0.2810 | Eval R2: -358.8567 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2843 | Eval Loss: 0.2416 | Eval R2: -303.8835 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2456 | Eval Loss: 0.2077 | Eval R2: -256.3029 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2123 | Eval Loss: 0.1788 | Eval R2: -215.4800 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1839 | Eval Loss: 0.1542 | Eval R2: -180.6871 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1598 | Eval Loss: 0.1334 | Eval R2: -151.1991 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1395 | Eval Loss: 0.1159 | Eval R2: -126.3311 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1224 | Eval Loss: 0.1013 | Eval R2: -105.4564 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1082 | Eval Loss: 0.0892 | Eval R2: -88.0139 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0963 | Eval Loss: 0.0791 | Eval R2: -73.5083 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0865 | Eval Loss: 0.0709 | Eval R2: -61.5028 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0785 | Eval Loss: 0.0641 | Eval R2: -51.6137 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0718 | Eval Loss: 0.0586 | Eval R2: -43.5052 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0665 | Eval Loss: 0.0542 | Eval R2: -36.8851 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0621 | Eval Loss: 0.0506 | Eval R2: -31.5015 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0586 | Eval Loss: 0.0477 | Eval R2: -27.1397 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0557 | Eval Loss: 0.0454 | Eval R2: -23.6179 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0535 | Eval Loss: 0.0435 | Eval R2: -20.7835 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.039728, test R2 score: -21.116098
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05345681682229042, 'r2_eval_final': -20.783491134643555, 'loss_eval_final': 0.04352850094437599, 'r2_test': -21.116098323979795, 'loss_test': 0.0397278368473053, 'loss_nodes': [[0.03219415992498398, 0.0338757149875164, 0.03295576572418213, 0.031400248408317566, 0.04492153972387314, 0.11406455934047699, 0.03303079679608345, 0.033421099185943604, 0.03823458030819893, 0.04304229095578194, 0.03185929358005524, 0.033143021166324615, 0.032705556601285934, 0.036894381046295166, 0.032979100942611694, 0.05868392065167427, 0.032557424157857895, 0.033269815146923065, 0.03256424143910408, 0.03275926783680916]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05346
wandb: loss_eval 0.04353
wandb: loss_test 0.03973
wandb:   r2_eval -20.78349
wandb:   r2_test -21.1161
wandb: 
wandb: ðŸš€ View run super-frog-50 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/sxtgnke3
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_202947-sxtgnke3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [11:43:50<16:52:09, 2024.31s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_204645-ghqm817n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run serene-grass-51
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/ghqm817n

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9194 | Eval Loss: 0.6593 | Eval R2: -864.3677 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6421 | Eval Loss: 0.5349 | Eval R2: -696.6136 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5257 | Eval Loss: 0.4308 | Eval R2: -559.8137 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4209 | Eval Loss: 0.3341 | Eval R2: -428.2673 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3210 | Eval Loss: 0.2409 | Eval R2: -301.2177 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2264 | Eval Loss: 0.1564 | Eval R2: -187.2426 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1456 | Eval Loss: 0.0905 | Eval R2: -98.3464 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0873 | Eval Loss: 0.0489 | Eval R2: -43.0070 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0538 | Eval Loss: 0.0288 | Eval R2: -17.1743 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0383 | Eval Loss: 0.0207 | Eval R2: -7.6842 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0311 | Eval Loss: 0.0174 | Eval R2: -4.7720 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0273 | Eval Loss: 0.0155 | Eval R2: -3.7666 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0244 | Eval Loss: 0.0139 | Eval R2: -3.3801 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0218 | Eval Loss: 0.0126 | Eval R2: -3.5862 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0194 | Eval Loss: 0.0117 | Eval R2: -3.3260 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0179 | Eval Loss: 0.0109 | Eval R2: -2.9284 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0168 | Eval Loss: 0.0104 | Eval R2: -2.6831 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0159 | Eval Loss: 0.0102 | Eval R2: -2.7834 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0152 | Eval Loss: 0.0100 | Eval R2: -2.9554 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0146 | Eval Loss: 0.0097 | Eval R2: -2.8247 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0140 | Eval Loss: 0.0096 | Eval R2: -2.9354 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0134 | Eval Loss: 0.0093 | Eval R2: -2.8358 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0129 | Eval Loss: 0.0091 | Eval R2: -2.8040 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0087 | Eval R2: -2.5221 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0118 | Eval Loss: 0.0086 | Eval R2: -2.6712 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0114 | Eval Loss: 0.0083 | Eval R2: -2.4447 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0109 | Eval Loss: 0.0081 | Eval R2: -2.3499 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008231, test R2 score: -2.964317
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.010922707617282867, 'r2_eval_final': -2.3498620986938477, 'loss_eval_final': 0.008073534816503525, 'r2_test': -2.9643172658565593, 'loss_test': 0.008230721578001976, 'loss_nodes': [[0.0028403077740222216, 0.005961157381534576, 0.024906020611524582, 0.008386037312448025, 0.005682543385773897, 0.0034494951833039522, 0.0030910796485841274, 0.006750152446329594, 0.006295640952885151, 0.007915401831269264, 0.005978188011795282, 0.00793306902050972, 0.004925440531224012, 0.014202958904206753, 0.016627872362732887, 0.01348226610571146, 0.0068251509219408035, 0.007773402612656355, 0.0054478272795677185, 0.006140387617051601]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.003 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01092
wandb: loss_eval 0.00807
wandb: loss_test 0.00823
wandb:   r2_eval -2.34986
wandb:   r2_test -2.96432
wandb: 
wandb: ðŸš€ View run serene-grass-51 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/ghqm817n
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_204645-ghqm817n/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [12:11:06<15:22:04, 1907.75s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_211401-0wemc5kx
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run legendary-darkness-52
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/0wemc5kx

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9041 | Eval Loss: 0.7612 | Eval R2: -996.9238 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7647 | Eval Loss: 0.6846 | Eval R2: -899.0084 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6941 | Eval Loss: 0.6216 | Eval R2: -812.9354 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6296 | Eval Loss: 0.5585 | Eval R2: -728.5205 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5629 | Eval Loss: 0.4948 | Eval R2: -642.3735 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4953 | Eval Loss: 0.4306 | Eval R2: -554.8746 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4288 | Eval Loss: 0.3687 | Eval R2: -471.0504 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3659 | Eval Loss: 0.3106 | Eval R2: -392.5980 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3078 | Eval Loss: 0.2579 | Eval R2: -321.1087 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2554 | Eval Loss: 0.2111 | Eval R2: -257.8965 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2094 | Eval Loss: 0.1710 | Eval R2: -203.8057 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1707 | Eval Loss: 0.1376 | Eval R2: -159.1200 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1388 | Eval Loss: 0.1106 | Eval R2: -123.3797 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1132 | Eval Loss: 0.0893 | Eval R2: -95.6407 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0928 | Eval Loss: 0.0726 | Eval R2: -74.2588 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0764 | Eval Loss: 0.0594 | Eval R2: -57.8652 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0631 | Eval Loss: 0.0487 | Eval R2: -45.1380 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0520 | Eval Loss: 0.0403 | Eval R2: -35.2741 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0429 | Eval Loss: 0.0332 | Eval R2: -26.9667 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0275 | Eval R2: -20.1591 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0300 | Eval Loss: 0.0230 | Eval R2: -14.6814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0257 | Eval Loss: 0.0196 | Eval R2: -10.7098 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0225 | Eval Loss: 0.0171 | Eval R2: -7.9165 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0200 | Eval Loss: 0.0151 | Eval R2: -5.9292 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0180 | Eval Loss: 0.0137 | Eval R2: -4.8086 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0165 | Eval Loss: 0.0124 | Eval R2: -3.8270 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0151 | Eval Loss: 0.0114 | Eval R2: -3.1922 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.010108, test R2 score: -3.593988
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.01514763105660677, 'r2_eval_final': -3.192204236984253, 'loss_eval_final': 0.011433062143623829, 'r2_test': -3.593987595411585, 'loss_test': 0.010108032263815403, 'loss_nodes': [[0.0049039022997021675, 0.019939454272389412, 0.007327746134251356, 0.0029728764202445745, 0.0033477996475994587, 0.023724354803562164, 0.003918927162885666, 0.012832381762564182, 0.004782555159181356, 0.003684115596115589, 0.00440628919750452, 0.008504842408001423, 0.013646646402776241, 0.01789482869207859, 0.0067116073332726955, 0.012484489008784294, 0.009863875806331635, 0.015354786068201065, 0.013131584040820599, 0.01272757537662983]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01515
wandb: loss_eval 0.01143
wandb: loss_test 0.01011
wandb:   r2_eval -3.1922
wandb:   r2_test -3.59399
wandb: 
wandb: ðŸš€ View run legendary-darkness-52 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/0wemc5kx
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_211401-0wemc5kx/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [12:32:12<13:20:29, 1715.36s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_213507-kfr1yovf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run youthful-sun-53
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/kfr1yovf

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3704 | Eval Loss: 1.1588 | Eval R2: -1522.6636 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.1019 | Eval Loss: 0.9470 | Eval R2: -1245.9148 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.9013 | Eval Loss: 0.7650 | Eval R2: -1006.7076 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7234 | Eval Loss: 0.5988 | Eval R2: -786.5292 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5588 | Eval Loss: 0.4462 | Eval R2: -582.3038 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4103 | Eval Loss: 0.3139 | Eval R2: -403.0302 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2855 | Eval Loss: 0.2090 | Eval R2: -258.7387 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1901 | Eval Loss: 0.1339 | Eval R2: -153.9694 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1248 | Eval Loss: 0.0867 | Eval R2: -86.5291 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0855 | Eval Loss: 0.0607 | Eval R2: -48.0117 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0643 | Eval Loss: 0.0478 | Eval R2: -27.9694 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0539 | Eval Loss: 0.0419 | Eval R2: -18.1874 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0490 | Eval Loss: 0.0391 | Eval R2: -13.5753 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0468 | Eval Loss: 0.0379 | Eval R2: -11.4338 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0457 | Eval Loss: 0.0373 | Eval R2: -10.4576 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0453 | Eval Loss: 0.0371 | Eval R2: -10.0297 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0451 | Eval Loss: 0.0370 | Eval R2: -9.8503 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7739 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7381 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7208 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7130 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7096 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7080 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7071 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7065 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7060 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7055 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032325, test R2 score: -11.425828
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.04493798688054085, 'r2_eval_final': -9.705501556396484, 'loss_eval_final': 0.036991655826568604, 'r2_test': -11.42582807908623, 'loss_test': 0.03232539817690849, 'loss_nodes': [[0.03110206313431263, 0.03264728561043739, 0.03281053900718689, 0.03139190375804901, 0.03299935907125473, 0.03204686939716339, 0.033029913902282715, 0.033427681773900986, 0.032020606100559235, 0.032134972512722015, 0.03186018764972687, 0.032381873577833176, 0.03260040283203125, 0.03131607919931412, 0.03297436982393265, 0.031206253916025162, 0.032495930790901184, 0.03298694267868996, 0.032477378845214844, 0.03259730339050293]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04494
wandb: loss_eval 0.03699
wandb: loss_test 0.03233
wandb:   r2_eval -9.7055
wandb:   r2_test -11.42583
wandb: 
wandb: ðŸš€ View run youthful-sun-53 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/kfr1yovf
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_213507-kfr1yovf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [12:47:31<11:04:20, 1476.33s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_215026-r9lrxh9i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cool-firebrand-54
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/r9lrxh9i

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1016 | Eval Loss: 0.6733 | Eval R2: -868.3806 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5785 | Eval Loss: 0.4192 | Eval R2: -535.0624 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3631 | Eval Loss: 0.2578 | Eval R2: -321.3682 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2267 | Eval Loss: 0.1572 | Eval R2: -187.5701 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1406 | Eval Loss: 0.0930 | Eval R2: -102.5438 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0850 | Eval Loss: 0.0530 | Eval R2: -50.3472 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0516 | Eval Loss: 0.0304 | Eval R2: -21.3650 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0334 | Eval Loss: 0.0196 | Eval R2: -8.2236 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0252 | Eval Loss: 0.0157 | Eval R2: -4.0900 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0219 | Eval Loss: 0.0140 | Eval R2: -3.1503 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0202 | Eval Loss: 0.0129 | Eval R2: -2.8087 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0188 | Eval Loss: 0.0118 | Eval R2: -2.4210 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0177 | Eval Loss: 0.0109 | Eval R2: -2.1284 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0166 | Eval Loss: 0.0103 | Eval R2: -1.9321 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0158 | Eval Loss: 0.0098 | Eval R2: -1.8366 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0153 | Eval Loss: 0.0095 | Eval R2: -1.7299 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0148 | Eval Loss: 0.0093 | Eval R2: -1.6327 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0144 | Eval Loss: 0.0091 | Eval R2: -1.5570 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0139 | Eval Loss: 0.0088 | Eval R2: -1.5204 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0134 | Eval Loss: 0.0084 | Eval R2: -1.4074 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0129 | Eval Loss: 0.0081 | Eval R2: -1.3138 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0124 | Eval Loss: 0.0078 | Eval R2: -1.2234 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0119 | Eval Loss: 0.0075 | Eval R2: -1.1695 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0117 | Eval Loss: 0.0073 | Eval R2: -1.1285 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0112 | Eval Loss: 0.0070 | Eval R2: -1.0096 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0108 | Eval Loss: 0.0068 | Eval R2: -0.9335 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0102 | Eval Loss: 0.0064 | Eval R2: -0.8891 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006224, test R2 score: -1.726074
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010232152417302132, 'r2_eval_final': -0.8891273736953735, 'loss_eval_final': 0.006420284043997526, 'r2_test': -1.726074414595915, 'loss_test': 0.006224362179636955, 'loss_nodes': [[0.00255738478153944, 0.0044868104159832, 0.003281604964286089, 0.0038428755942732096, 0.003409602679312229, 0.0035783343482762575, 0.004129703156650066, 0.006960547063499689, 0.007775180507451296, 0.0045297881588339806, 0.0048217675648629665, 0.009158851578831673, 0.005898178555071354, 0.006226325873285532, 0.00698615750297904, 0.009684575721621513, 0.008867115713655949, 0.009593985043466091, 0.010390919633209705, 0.008307537995278835]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01023
wandb: loss_eval 0.00642
wandb: loss_test 0.00622
wandb:   r2_eval -0.88913
wandb:   r2_test -1.72607
wandb: 
wandb: ðŸš€ View run cool-firebrand-54 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/r9lrxh9i
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_215026-r9lrxh9i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [13:08:09<10:08:43, 1404.75s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_221104-filqhcjv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-waterfall-55
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/filqhcjv

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.0580 | Eval Loss: 0.8867 | Eval R2: -1168.0939 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8949 | Eval Loss: 0.8118 | Eval R2: -1068.4769 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.8369 | Eval Loss: 0.7693 | Eval R2: -1011.8654 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7922 | Eval Loss: 0.7254 | Eval R2: -953.1858 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.7447 | Eval Loss: 0.6770 | Eval R2: -887.8880 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.6909 | Eval Loss: 0.6209 | Eval R2: -812.4103 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.6281 | Eval Loss: 0.5545 | Eval R2: -723.3395 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.5539 | Eval Loss: 0.4769 | Eval R2: -619.1458 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.4677 | Eval Loss: 0.3888 | Eval R2: -501.7986 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3726 | Eval Loss: 0.2963 | Eval R2: -379.0565 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2764 | Eval Loss: 0.2089 | Eval R2: -263.5908 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1899 | Eval Loss: 0.1358 | Eval R2: -167.2171 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1216 | Eval Loss: 0.0819 | Eval R2: -96.1563 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0736 | Eval Loss: 0.0466 | Eval R2: -50.1146 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0437 | Eval Loss: 0.0259 | Eval R2: -23.4988 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0271 | Eval Loss: 0.0156 | Eval R2: -10.3093 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0190 | Eval Loss: 0.0109 | Eval R2: -4.5571 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0154 | Eval Loss: 0.0091 | Eval R2: -2.3864 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0138 | Eval Loss: 0.0083 | Eval R2: -1.5679 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0129 | Eval Loss: 0.0079 | Eval R2: -1.2481 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0124 | Eval Loss: 0.0077 | Eval R2: -1.1037 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0119 | Eval Loss: 0.0075 | Eval R2: -1.0041 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0116 | Eval Loss: 0.0073 | Eval R2: -0.9465 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0113 | Eval Loss: 0.0072 | Eval R2: -0.9149 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0110 | Eval Loss: 0.0071 | Eval R2: -0.8978 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0107 | Eval Loss: 0.0070 | Eval R2: -0.9034 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0104 | Eval Loss: 0.0069 | Eval R2: -0.8256 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006214, test R2 score: -1.416030
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.010441798716783524, 'r2_eval_final': -0.8255558609962463, 'loss_eval_final': 0.00689147412776947, 'r2_test': -1.4160299029270453, 'loss_test': 0.0062137749046087265, 'loss_nodes': [[0.0019808446522802114, 0.006342739332467318, 0.002170783467590809, 0.006168199237436056, 0.0039450787007808685, 0.0021330423187464476, 0.011246684938669205, 0.0035480400547385216, 0.004729566164314747, 0.0035514063201844692, 0.0057295095175504684, 0.011764840222895145, 0.004955677781254053, 0.004506395198404789, 0.005533996503800154, 0.004934762604534626, 0.01560758613049984, 0.006937006488442421, 0.010910805314779282, 0.007578535936772823]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01044
wandb: loss_eval 0.00689
wandb: loss_test 0.00621
wandb:   r2_eval -0.82556
wandb:   r2_test -1.41603
wandb: 
wandb: ðŸš€ View run ancient-waterfall-55 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/filqhcjv
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_221104-filqhcjv/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [14:17:07<15:26:58, 2224.75s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_232002-dy1w736x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run royal-mountain-56
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/dy1w736x

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.0337 | Eval Loss: 0.7933 | Eval R2: -1042.4657 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7872 | Eval Loss: 0.6690 | Eval R2: -879.5557 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6585 | Eval Loss: 0.5482 | Eval R2: -717.2827 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5360 | Eval Loss: 0.4317 | Eval R2: -558.2559 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4109 | Eval Loss: 0.3109 | Eval R2: -394.5335 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2873 | Eval Loss: 0.1999 | Eval R2: -243.8269 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1831 | Eval Loss: 0.1172 | Eval R2: -132.0788 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1112 | Eval Loss: 0.0657 | Eval R2: -62.7667 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0687 | Eval Loss: 0.0390 | Eval R2: -26.8953 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0475 | Eval Loss: 0.0273 | Eval R2: -11.9896 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0379 | Eval Loss: 0.0226 | Eval R2: -6.7223 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0335 | Eval Loss: 0.0202 | Eval R2: -5.0845 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0185 | Eval R2: -4.4338 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0282 | Eval Loss: 0.0169 | Eval R2: -4.0049 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0258 | Eval Loss: 0.0153 | Eval R2: -3.6145 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0235 | Eval Loss: 0.0137 | Eval R2: -3.2292 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0214 | Eval Loss: 0.0122 | Eval R2: -2.9122 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0193 | Eval Loss: 0.0110 | Eval R2: -2.6363 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0175 | Eval Loss: 0.0099 | Eval R2: -2.3717 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0159 | Eval Loss: 0.0091 | Eval R2: -2.0724 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0085 | Eval R2: -1.7670 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0138 | Eval Loss: 0.0081 | Eval R2: -1.5797 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0131 | Eval Loss: 0.0079 | Eval R2: -1.4502 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0126 | Eval Loss: 0.0077 | Eval R2: -1.3556 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0122 | Eval Loss: 0.0075 | Eval R2: -1.3664 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0118 | Eval Loss: 0.0073 | Eval R2: -1.3398 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0115 | Eval Loss: 0.0072 | Eval R2: -1.4419 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006700, test R2 score: -2.012458
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.011464916169643402, 'r2_eval_final': -1.4419054985046387, 'loss_eval_final': 0.007208006922155619, 'r2_test': -2.0124577484154904, 'loss_test': 0.00669993506744504, 'loss_nodes': [[0.006285675335675478, 0.007602578029036522, 0.004641087260097265, 0.004516795743256807, 0.0038700795266777277, 0.003085093107074499, 0.006708220113068819, 0.005903664510697126, 0.006014675833284855, 0.004023987799882889, 0.006565439980477095, 0.0071082101203501225, 0.006082074251025915, 0.012020086869597435, 0.008868887089192867, 0.009200847707688808, 0.007203719578683376, 0.00757422111928463, 0.008437776938080788, 0.008285571821033955]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01146
wandb: loss_eval 0.00721
wandb: loss_test 0.0067
wandb:   r2_eval -1.44191
wandb:   r2_test -2.01246
wandb: 
wandb: ðŸš€ View run royal-mountain-56 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/dy1w736x
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_232002-dy1w736x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [14:37:55<12:52:41, 1931.72s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_234050-4nkjkv4j
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run jumping-forest-57
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/4nkjkv4j

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.3282 | Eval Loss: 1.6572 | Eval R2: -2150.4080 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.5703 | Eval Loss: 1.3522 | Eval R2: -1755.8922 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.3168 | Eval Loss: 1.1583 | Eval R2: -1508.8317 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.1326 | Eval Loss: 0.9951 | Eval R2: -1296.5870 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9874 | Eval Loss: 0.8761 | Eval R2: -1141.4620 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.8677 | Eval Loss: 0.7706 | Eval R2: -1003.1132 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.7659 | Eval Loss: 0.6806 | Eval R2: -884.1174 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.6778 | Eval Loss: 0.6020 | Eval R2: -779.5423 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.6011 | Eval Loss: 0.5333 | Eval R2: -688.2289 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.5342 | Eval Loss: 0.4734 | Eval R2: -608.5543 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4755 | Eval Loss: 0.4208 | Eval R2: -538.8421 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.4237 | Eval Loss: 0.3742 | Eval R2: -477.2992 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3777 | Eval Loss: 0.3328 | Eval R2: -422.7291 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3367 | Eval Loss: 0.2958 | Eval R2: -374.0954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2997 | Eval Loss: 0.2625 | Eval R2: -330.5504 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2665 | Eval Loss: 0.2326 | Eval R2: -291.4294 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2365 | Eval Loss: 0.2054 | Eval R2: -256.1612 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2092 | Eval Loss: 0.1809 | Eval R2: -224.3621 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1845 | Eval Loss: 0.1586 | Eval R2: -195.6070 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1622 | Eval Loss: 0.1385 | Eval R2: -169.6536 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1419 | Eval Loss: 0.1203 | Eval R2: -146.1759 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1236 | Eval Loss: 0.1038 | Eval R2: -125.0420 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1070 | Eval Loss: 0.0891 | Eval R2: -106.0660 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0923 | Eval Loss: 0.0759 | Eval R2: -89.1881 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0792 | Eval Loss: 0.0644 | Eval R2: -74.3706 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0677 | Eval Loss: 0.0544 | Eval R2: -61.4893 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0577 | Eval Loss: 0.0457 | Eval R2: -50.3745 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.046126, test R2 score: -44.306491
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05773135647177696, 'r2_eval_final': -50.37446594238281, 'loss_eval_final': 0.045736148953437805, 'r2_test': -44.306490943999, 'loss_test': 0.04612583667039871, 'loss_nodes': [[0.2803061306476593, 0.01629689894616604, 0.010288788005709648, 0.004464718047529459, 0.007843839935958385, 0.00891904253512621, 0.015844933688640594, 0.009098434820771217, 0.013574357144534588, 0.00996175967156887, 0.01902969740331173, 0.05328703671693802, 0.010281577706336975, 0.015395130030810833, 0.010568448342382908, 0.022547461092472076, 0.33414772152900696, 0.014339873567223549, 0.014233856461942196, 0.05208705738186836]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05773
wandb: loss_eval 0.04574
wandb: loss_test 0.04613
wandb:   r2_eval -50.37447
wandb:   r2_test -44.30649
wandb: 
wandb: ðŸš€ View run jumping-forest-57 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/4nkjkv4j
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_234050-4nkjkv4j/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 27/50 [14:48:59<9:54:45, 1551.53s/it] Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240726_235154-kr3lqxqw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run autumn-pyramid-58
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/kr3lqxqw

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.5714 | Eval Loss: 1.1964 | Eval R2: -1571.2195 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2065 | Eval Loss: 1.1030 | Eval R2: -1450.0873 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1196 | Eval Loss: 1.0303 | Eval R2: -1355.4410 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0491 | Eval Loss: 0.9661 | Eval R2: -1271.2419 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9822 | Eval Loss: 0.9007 | Eval R2: -1184.9846 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9122 | Eval Loss: 0.8322 | Eval R2: -1093.8607 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8388 | Eval Loss: 0.7601 | Eval R2: -997.5918 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.7628 | Eval Loss: 0.6853 | Eval R2: -898.0237 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.6851 | Eval Loss: 0.6099 | Eval R2: -797.5688 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.6069 | Eval Loss: 0.5349 | Eval R2: -697.9406 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.5296 | Eval Loss: 0.4618 | Eval R2: -600.9948 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.4545 | Eval Loss: 0.3919 | Eval R2: -508.5742 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3835 | Eval Loss: 0.3271 | Eval R2: -422.7261 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3189 | Eval Loss: 0.2692 | Eval R2: -345.6285 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2622 | Eval Loss: 0.2190 | Eval R2: -278.6515 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2133 | Eval Loss: 0.1765 | Eval R2: -222.0999 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1724 | Eval Loss: 0.1414 | Eval R2: -175.4744 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1388 | Eval Loss: 0.1130 | Eval R2: -137.7932 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1117 | Eval Loss: 0.0903 | Eval R2: -107.6463 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0900 | Eval Loss: 0.0722 | Eval R2: -83.6785 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0728 | Eval Loss: 0.0578 | Eval R2: -64.7004 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0591 | Eval Loss: 0.0464 | Eval R2: -49.7143 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0482 | Eval Loss: 0.0374 | Eval R2: -37.9432 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0396 | Eval Loss: 0.0304 | Eval R2: -28.7670 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0329 | Eval Loss: 0.0250 | Eval R2: -21.6695 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0278 | Eval Loss: 0.0207 | Eval R2: -16.2065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0237 | Eval Loss: 0.0175 | Eval R2: -12.0824 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.016875, test R2 score: -11.476670
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.023703522980213165, 'r2_eval_final': -12.082376480102539, 'loss_eval_final': 0.017496595159173012, 'r2_test': -11.476669669861437, 'loss_test': 0.016875458881258965, 'loss_nodes': [[0.0072154453955590725, 0.02029954455792904, 0.016651349142193794, 0.018049491569399834, 0.009962285868823528, 0.003366741118952632, 0.008111903443932533, 0.014381170272827148, 0.01449483260512352, 0.007540568709373474, 0.13177204132080078, 0.0073443991132080555, 0.00740716652944684, 0.005997508764266968, 0.009333358146250248, 0.010610408149659634, 0.013398177921772003, 0.009064704179763794, 0.009495525620877743, 0.013012601062655449]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0237
wandb: loss_eval 0.0175
wandb: loss_test 0.01688
wandb:   r2_eval -12.08238
wandb:   r2_test -11.47667
wandb: 
wandb: ðŸš€ View run autumn-pyramid-58 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/kr3lqxqw
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240726_235154-kr3lqxqw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [15:15:30<9:33:09, 1563.17s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_001825-lpdc9a8m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run giddy-firebrand-59
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/lpdc9a8m

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.5379 | Eval Loss: 1.0794 | Eval R2: -1413.8086 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.9511 | Eval Loss: 0.6997 | Eval R2: -912.4971 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5647 | Eval Loss: 0.3337 | Eval R2: -421.9374 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2173 | Eval Loss: 0.0769 | Eval R2: -74.2154 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0564 | Eval Loss: 0.0295 | Eval R2: -14.7602 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0358 | Eval Loss: 0.0228 | Eval R2: -10.0575 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0290 | Eval Loss: 0.0170 | Eval R2: -5.6357 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0230 | Eval Loss: 0.0136 | Eval R2: -3.2696 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0199 | Eval Loss: 0.0121 | Eval R2: -2.7034 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0179 | Eval Loss: 0.0109 | Eval R2: -2.2492 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0165 | Eval Loss: 0.0101 | Eval R2: -1.9407 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0153 | Eval Loss: 0.0095 | Eval R2: -1.7138 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0145 | Eval Loss: 0.0090 | Eval R2: -1.5560 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0138 | Eval Loss: 0.0086 | Eval R2: -1.4378 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0133 | Eval Loss: 0.0084 | Eval R2: -1.3452 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0129 | Eval Loss: 0.0081 | Eval R2: -1.2802 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0125 | Eval Loss: 0.0079 | Eval R2: -1.2243 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0122 | Eval Loss: 0.0077 | Eval R2: -1.1734 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0118 | Eval Loss: 0.0075 | Eval R2: -1.1228 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0116 | Eval Loss: 0.0074 | Eval R2: -1.0703 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0113 | Eval Loss: 0.0072 | Eval R2: -1.0313 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0110 | Eval Loss: 0.0070 | Eval R2: -0.9772 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0108 | Eval Loss: 0.0069 | Eval R2: -0.9304 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0106 | Eval Loss: 0.0067 | Eval R2: -0.8892 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0103 | Eval Loss: 0.0065 | Eval R2: -0.8318 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0100 | Eval Loss: 0.0064 | Eval R2: -0.7789 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0098 | Eval Loss: 0.0062 | Eval R2: -0.7257 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.005840, test R2 score: -1.469189
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.00981559045612812, 'r2_eval_final': -0.725715696811676, 'loss_eval_final': 0.006209260318428278, 'r2_test': -1.469188737427965, 'loss_test': 0.0058398377150297165, 'loss_nodes': [[0.0027025986928492785, 0.0030328715220093727, 0.008086497895419598, 0.003191474126651883, 0.0033979539293795824, 0.005246423650532961, 0.004997492767870426, 0.00573272630572319, 0.005796073470264673, 0.005732393823564053, 0.003829710418358445, 0.00647067790850997, 0.004999298602342606, 0.00811027642339468, 0.009489281103014946, 0.005777704529464245, 0.007116316352039576, 0.006715688854455948, 0.008956882171332836, 0.007414425257593393]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00982
wandb: loss_eval 0.00621
wandb: loss_test 0.00584
wandb:   r2_eval -0.72572
wandb:   r2_test -1.46919
wandb: 
wandb: ðŸš€ View run giddy-firebrand-59 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/lpdc9a8m
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_001825-lpdc9a8m/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 29/50 [16:21:02<13:15:54, 2274.02s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_012357-6nluwx9g
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-energy-60
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/6nluwx9g

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1016 | Eval Loss: 0.6733 | Eval R2: -868.3806 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5785 | Eval Loss: 0.4192 | Eval R2: -535.0624 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3631 | Eval Loss: 0.2578 | Eval R2: -321.3682 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2267 | Eval Loss: 0.1572 | Eval R2: -187.5701 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1406 | Eval Loss: 0.0930 | Eval R2: -102.5438 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0850 | Eval Loss: 0.0530 | Eval R2: -50.3472 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0516 | Eval Loss: 0.0304 | Eval R2: -21.3650 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0334 | Eval Loss: 0.0196 | Eval R2: -8.2236 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0252 | Eval Loss: 0.0157 | Eval R2: -4.0900 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0219 | Eval Loss: 0.0140 | Eval R2: -3.1503 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0202 | Eval Loss: 0.0129 | Eval R2: -2.8087 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0188 | Eval Loss: 0.0118 | Eval R2: -2.4210 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0177 | Eval Loss: 0.0109 | Eval R2: -2.1284 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0166 | Eval Loss: 0.0103 | Eval R2: -1.9321 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0158 | Eval Loss: 0.0098 | Eval R2: -1.8366 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0153 | Eval Loss: 0.0095 | Eval R2: -1.7299 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0148 | Eval Loss: 0.0093 | Eval R2: -1.6327 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0144 | Eval Loss: 0.0091 | Eval R2: -1.5570 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0139 | Eval Loss: 0.0088 | Eval R2: -1.5204 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0134 | Eval Loss: 0.0084 | Eval R2: -1.4074 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0129 | Eval Loss: 0.0081 | Eval R2: -1.3138 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0124 | Eval Loss: 0.0078 | Eval R2: -1.2234 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0119 | Eval Loss: 0.0075 | Eval R2: -1.1695 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0117 | Eval Loss: 0.0073 | Eval R2: -1.1285 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0112 | Eval Loss: 0.0070 | Eval R2: -1.0096 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0108 | Eval Loss: 0.0068 | Eval R2: -0.9335 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0102 | Eval Loss: 0.0064 | Eval R2: -0.8891 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006224, test R2 score: -1.726074
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010232152417302132, 'r2_eval_final': -0.8891273736953735, 'loss_eval_final': 0.006420284043997526, 'r2_test': -1.726074414595915, 'loss_test': 0.006224362179636955, 'loss_nodes': [[0.00255738478153944, 0.0044868104159832, 0.003281604964286089, 0.0038428755942732096, 0.003409602679312229, 0.0035783343482762575, 0.004129703156650066, 0.006960547063499689, 0.007775180507451296, 0.0045297881588339806, 0.0048217675648629665, 0.009158851578831673, 0.005898178555071354, 0.006226325873285532, 0.00698615750297904, 0.009684575721621513, 0.008867115713655949, 0.009593985043466091, 0.010390919633209705, 0.008307537995278835]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01023
wandb: loss_eval 0.00642
wandb: loss_test 0.00622
wandb:   r2_eval -0.88913
wandb:   r2_test -1.72607
wandb: 
wandb: ðŸš€ View run lemon-energy-60 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/6nluwx9g
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_012357-6nluwx9g/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [16:43:20<11:04:24, 1993.24s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_014615-aih9a88q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run polar-dawn-61
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/aih9a88q

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9125 | Eval Loss: 0.6356 | Eval R2: -828.1833 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5501 | Eval Loss: 0.3538 | Eval R2: -450.4642 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2996 | Eval Loss: 0.1727 | Eval R2: -202.7828 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1489 | Eval Loss: 0.0786 | Eval R2: -74.9680 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0817 | Eval Loss: 0.0437 | Eval R2: -26.6139 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0566 | Eval Loss: 0.0313 | Eval R2: -11.2287 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0477 | Eval Loss: 0.0270 | Eval R2: -7.3468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0448 | Eval Loss: 0.0257 | Eval R2: -6.6191 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0424 | Eval Loss: 0.0241 | Eval R2: -6.0571 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0400 | Eval Loss: 0.0227 | Eval R2: -5.8690 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0373 | Eval Loss: 0.0210 | Eval R2: -5.8803 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0342 | Eval Loss: 0.0196 | Eval R2: -6.9657 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0188 | Eval R2: -9.1237 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0270 | Eval Loss: 0.0187 | Eval R2: -12.1291 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0240 | Eval Loss: 0.0178 | Eval R2: -12.9015 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0149 | Eval R2: -9.8526 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0123 | Eval R2: -6.9338 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0107 | Eval R2: -5.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0168 | Eval Loss: 0.0097 | Eval R2: -4.6913 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0156 | Eval Loss: 0.0087 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0080 | Eval R2: -3.3115 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0137 | Eval Loss: 0.0074 | Eval R2: -2.8324 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0130 | Eval Loss: 0.0069 | Eval R2: -2.5535 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0063 | Eval R2: -2.2374 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0113 | Eval Loss: 0.0059 | Eval R2: -1.9303 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0106 | Eval Loss: 0.0056 | Eval R2: -1.7210 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0100 | Eval Loss: 0.0054 | Eval R2: -1.5654 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.005597, test R2 score: -2.004708
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009975460357964039, 'r2_eval_final': -1.565418004989624, 'loss_eval_final': 0.005364386364817619, 'r2_test': -2.0047076365614602, 'loss_test': 0.005596609320491552, 'loss_nodes': [[0.002452634274959564, 0.0033022717107087374, 0.0026522281114012003, 0.004800139926373959, 0.004224689677357674, 0.003856227034702897, 0.005307621322572231, 0.007563397753983736, 0.005532231647521257, 0.006012852303683758, 0.0037624333053827286, 0.004094624426215887, 0.005834830924868584, 0.008045963943004608, 0.007648222614079714, 0.004923588130623102, 0.0066497838124632835, 0.009574453346431255, 0.006938895210623741, 0.008755107410252094]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00998
wandb: loss_eval 0.00536
wandb: loss_test 0.0056
wandb:   r2_eval -1.56542
wandb:   r2_test -2.00471
wandb: 
wandb: ðŸš€ View run polar-dawn-61 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/aih9a88q
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_014615-aih9a88q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 31/50 [17:05:37<9:28:50, 1796.33s/it] Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_020832-qqrb7r0y
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-plasma-62
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/qqrb7r0y

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.1175 | Eval Loss: 1.3449 | Eval R2: -1754.3196 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2933 | Eval Loss: 1.1474 | Eval R2: -1504.5608 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1382 | Eval Loss: 1.0401 | Eval R2: -1366.1879 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0502 | Eval Loss: 0.9654 | Eval R2: -1269.3722 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9783 | Eval Loss: 0.9011 | Eval R2: -1185.9937 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9168 | Eval Loss: 0.8435 | Eval R2: -1110.6115 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8616 | Eval Loss: 0.7932 | Eval R2: -1044.3988 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.8103 | Eval Loss: 0.7449 | Eval R2: -980.2150 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.7593 | Eval Loss: 0.6945 | Eval R2: -913.0714 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.7051 | Eval Loss: 0.6380 | Eval R2: -836.5386 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.6470 | Eval Loss: 0.5811 | Eval R2: -759.8204 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.5867 | Eval Loss: 0.5214 | Eval R2: -679.7445 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.5240 | Eval Loss: 0.4600 | Eval R2: -596.6365 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.4599 | Eval Loss: 0.3988 | Eval R2: -512.8257 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3968 | Eval Loss: 0.3396 | Eval R2: -431.8340 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.3367 | Eval Loss: 0.2841 | Eval R2: -356.0157 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2811 | Eval Loss: 0.2339 | Eval R2: -287.4590 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2317 | Eval Loss: 0.1904 | Eval R2: -227.9376 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1895 | Eval Loss: 0.1541 | Eval R2: -178.0724 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1546 | Eval Loss: 0.1246 | Eval R2: -137.6694 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1266 | Eval Loss: 0.1013 | Eval R2: -105.7636 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1047 | Eval Loss: 0.0831 | Eval R2: -81.0401 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0876 | Eval Loss: 0.0691 | Eval R2: -62.1930 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0746 | Eval Loss: 0.0585 | Eval R2: -47.9218 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0647 | Eval Loss: 0.0503 | Eval R2: -37.2229 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0569 | Eval Loss: 0.0441 | Eval R2: -29.1635 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0511 | Eval Loss: 0.0392 | Eval R2: -23.1495 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.037154, test R2 score: -22.710716
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05107304826378822, 'r2_eval_final': -23.149497985839844, 'loss_eval_final': 0.0392075851559639, 'r2_test': -22.710716384304437, 'loss_test': 0.037153519690036774, 'loss_nodes': [[0.025696467608213425, 0.028831860050559044, 0.020902646705508232, 0.06295966356992722, 0.026749033480882645, 0.028263935819268227, 0.02382110431790352, 0.033269356936216354, 0.029641836881637573, 0.03743165358901024, 0.02297825738787651, 0.02518240176141262, 0.02551611140370369, 0.038175154477357864, 0.029250068590044975, 0.03381653502583504, 0.028005244210362434, 0.024492226541042328, 0.06226000189781189, 0.1358269900083542]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05107
wandb: loss_eval 0.03921
wandb: loss_test 0.03715
wandb:   r2_eval -23.1495
wandb:   r2_test -22.71072
wandb: 
wandb: ðŸš€ View run true-plasma-62 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/qqrb7r0y
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_020832-qqrb7r0y/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [18:12:33<12:18:38, 2462.16s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_031528-pcczsw6u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-river-63
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/pcczsw6u

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1809 | Eval Loss: 0.6845 | Eval R2: -889.2283 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6024 | Eval Loss: 0.4362 | Eval R2: -559.7561 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3742 | Eval Loss: 0.2543 | Eval R2: -318.2298 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2168 | Eval Loss: 0.1389 | Eval R2: -157.8815 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1189 | Eval Loss: 0.0713 | Eval R2: -65.1418 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0675 | Eval Loss: 0.0427 | Eval R2: -22.9938 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0470 | Eval Loss: 0.0329 | Eval R2: -10.0606 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0403 | Eval Loss: 0.0300 | Eval R2: -7.4625 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0376 | Eval Loss: 0.0279 | Eval R2: -6.8241 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0352 | Eval Loss: 0.0259 | Eval R2: -6.6199 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0328 | Eval Loss: 0.0242 | Eval R2: -6.1058 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0304 | Eval Loss: 0.0227 | Eval R2: -5.6550 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0282 | Eval Loss: 0.0214 | Eval R2: -5.1907 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0263 | Eval Loss: 0.0204 | Eval R2: -5.1093 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0247 | Eval Loss: 0.0194 | Eval R2: -4.8743 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0233 | Eval Loss: 0.0183 | Eval R2: -4.7128 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0219 | Eval Loss: 0.0172 | Eval R2: -4.3684 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0206 | Eval Loss: 0.0162 | Eval R2: -4.0423 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0194 | Eval Loss: 0.0151 | Eval R2: -3.6187 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0182 | Eval Loss: 0.0141 | Eval R2: -3.2070 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0172 | Eval Loss: 0.0132 | Eval R2: -2.8767 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0162 | Eval Loss: 0.0122 | Eval R2: -2.4997 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0152 | Eval Loss: 0.0114 | Eval R2: -2.2635 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0143 | Eval Loss: 0.0106 | Eval R2: -2.0453 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0134 | Eval Loss: 0.0099 | Eval R2: -1.8230 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0126 | Eval Loss: 0.0091 | Eval R2: -1.6025 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0118 | Eval Loss: 0.0084 | Eval R2: -1.4559 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007460, test R2 score: -2.123678
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.011777092702686787, 'r2_eval_final': -1.455873727798462, 'loss_eval_final': 0.008393367752432823, 'r2_test': -2.123678478777353, 'loss_test': 0.007460397202521563, 'loss_nodes': [[0.0016526860417798162, 0.0072550661861896515, 0.002525540767237544, 0.0031719361431896687, 0.00437835743650794, 0.004677698016166687, 0.01035041268914938, 0.01378068421036005, 0.003694946877658367, 0.00351976091042161, 0.004009096417576075, 0.007270724978297949, 0.005388121120631695, 0.03937581554055214, 0.00513819744810462, 0.004500977229326963, 0.007339851465076208, 0.006774858105927706, 0.007997332140803337, 0.006405897904187441]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01178
wandb: loss_eval 0.00839
wandb: loss_test 0.00746
wandb:   r2_eval -1.45587
wandb:   r2_test -2.12368
wandb: 
wandb: ðŸš€ View run icy-river-63 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/pcczsw6u
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_031528-pcczsw6u/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [19:26:53<14:27:25, 3061.48s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_042948-zpticyi2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-snowflake-64
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/zpticyi2

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.6260 | Eval Loss: 0.9184 | Eval R2: -1193.3700 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8937 | Eval Loss: 0.7492 | Eval R2: -975.5931 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7005 | Eval Loss: 0.5693 | Eval R2: -735.8911 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5378 | Eval Loss: 0.4308 | Eval R2: -546.5880 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4036 | Eval Loss: 0.3122 | Eval R2: -390.7799 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2946 | Eval Loss: 0.2221 | Eval R2: -267.9971 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2110 | Eval Loss: 0.1546 | Eval R2: -176.8956 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1498 | Eval Loss: 0.1064 | Eval R2: -112.7954 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1069 | Eval Loss: 0.0738 | Eval R2: -69.6711 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0781 | Eval Loss: 0.0518 | Eval R2: -42.1352 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0587 | Eval Loss: 0.0366 | Eval R2: -25.0063 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0456 | Eval Loss: 0.0276 | Eval R2: -15.5697 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0361 | Eval Loss: 0.0218 | Eval R2: -10.7827 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0291 | Eval Loss: 0.0183 | Eval R2: -8.6954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0245 | Eval Loss: 0.0168 | Eval R2: -8.4377 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0140 | Eval R2: -5.5006 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0118 | Eval R2: -3.5593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0112 | Eval R2: -3.2574 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0171 | Eval Loss: 0.0105 | Eval R2: -2.7739 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0163 | Eval Loss: 0.0100 | Eval R2: -2.2395 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0157 | Eval Loss: 0.0096 | Eval R2: -1.9573 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0152 | Eval Loss: 0.0093 | Eval R2: -1.7769 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0148 | Eval Loss: 0.0090 | Eval R2: -1.6431 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0144 | Eval Loss: 0.0089 | Eval R2: -1.6965 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0140 | Eval Loss: 0.0087 | Eval R2: -1.6233 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0137 | Eval Loss: 0.0086 | Eval R2: -1.6083 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0133 | Eval Loss: 0.0084 | Eval R2: -1.6082 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007463, test R2 score: -2.204159
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.013302180916070938, 'r2_eval_final': -1.6081876754760742, 'loss_eval_final': 0.008430029265582561, 'r2_test': -2.2041592608908744, 'loss_test': 0.007463482208549976, 'loss_nodes': [[0.009258203208446503, 0.002844726899638772, 0.012989882379770279, 0.002518606139346957, 0.0025782219599932432, 0.009530997835099697, 0.003936316817998886, 0.0069677941501140594, 0.009197692386806011, 0.003009133506566286, 0.006136271636933088, 0.004906109068542719, 0.00696664210408926, 0.0121295265853405, 0.00849416945129633, 0.007642752956598997, 0.0073801446706056595, 0.014945722185075283, 0.008596071042120457, 0.00924064964056015]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0133
wandb: loss_eval 0.00843
wandb: loss_test 0.00746
wandb:   r2_eval -1.60819
wandb:   r2_test -2.20416
wandb: 
wandb: ðŸš€ View run valiant-snowflake-64 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/zpticyi2
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_042948-zpticyi2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [20:37:04<15:08:20, 3406.28s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_053959-3tqo170q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fluent-wood-65
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/3tqo170q

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.1175 | Eval Loss: 1.3449 | Eval R2: -1754.3196 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2933 | Eval Loss: 1.1474 | Eval R2: -1504.5608 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1382 | Eval Loss: 1.0401 | Eval R2: -1366.1879 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0502 | Eval Loss: 0.9654 | Eval R2: -1269.3722 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9783 | Eval Loss: 0.9011 | Eval R2: -1185.9937 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9168 | Eval Loss: 0.8435 | Eval R2: -1110.6115 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8616 | Eval Loss: 0.7932 | Eval R2: -1044.3988 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.8103 | Eval Loss: 0.7449 | Eval R2: -980.2150 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.7593 | Eval Loss: 0.6945 | Eval R2: -913.0714 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.7051 | Eval Loss: 0.6380 | Eval R2: -836.5386 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.6470 | Eval Loss: 0.5811 | Eval R2: -759.8204 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.5867 | Eval Loss: 0.5214 | Eval R2: -679.7445 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.5240 | Eval Loss: 0.4600 | Eval R2: -596.6365 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.4599 | Eval Loss: 0.3988 | Eval R2: -512.8257 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3968 | Eval Loss: 0.3396 | Eval R2: -431.8340 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.3367 | Eval Loss: 0.2841 | Eval R2: -356.0157 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2811 | Eval Loss: 0.2339 | Eval R2: -287.4590 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2317 | Eval Loss: 0.1904 | Eval R2: -227.9376 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1895 | Eval Loss: 0.1541 | Eval R2: -178.0724 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1546 | Eval Loss: 0.1246 | Eval R2: -137.6694 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1266 | Eval Loss: 0.1013 | Eval R2: -105.7636 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1047 | Eval Loss: 0.0831 | Eval R2: -81.0401 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0876 | Eval Loss: 0.0691 | Eval R2: -62.1930 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0746 | Eval Loss: 0.0585 | Eval R2: -47.9218 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0647 | Eval Loss: 0.0503 | Eval R2: -37.2229 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0569 | Eval Loss: 0.0441 | Eval R2: -29.1635 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0511 | Eval Loss: 0.0392 | Eval R2: -23.1495 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.037154, test R2 score: -22.710716
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05107304826378822, 'r2_eval_final': -23.149497985839844, 'loss_eval_final': 0.0392075851559639, 'r2_test': -22.710716384304437, 'loss_test': 0.037153519690036774, 'loss_nodes': [[0.025696467608213425, 0.028831860050559044, 0.020902646705508232, 0.06295966356992722, 0.026749033480882645, 0.028263935819268227, 0.02382110431790352, 0.033269356936216354, 0.029641836881637573, 0.03743165358901024, 0.02297825738787651, 0.02518240176141262, 0.02551611140370369, 0.038175154477357864, 0.029250068590044975, 0.03381653502583504, 0.028005244210362434, 0.024492226541042328, 0.06226000189781189, 0.1358269900083542]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05107
wandb: loss_eval 0.03921
wandb: loss_test 0.03715
wandb:   r2_eval -23.1495
wandb:   r2_test -22.71072
wandb: 
wandb: ðŸš€ View run fluent-wood-65 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/3tqo170q
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_053959-3tqo170q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [21:42:33<14:50:48, 3563.26s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_064528-6p5cvg5n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-oath-66
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/6p5cvg5n

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9125 | Eval Loss: 0.6356 | Eval R2: -828.1833 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5501 | Eval Loss: 0.3538 | Eval R2: -450.4642 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2996 | Eval Loss: 0.1727 | Eval R2: -202.7828 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1489 | Eval Loss: 0.0786 | Eval R2: -74.9680 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0817 | Eval Loss: 0.0437 | Eval R2: -26.6139 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0566 | Eval Loss: 0.0313 | Eval R2: -11.2287 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0477 | Eval Loss: 0.0270 | Eval R2: -7.3468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0448 | Eval Loss: 0.0257 | Eval R2: -6.6191 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0424 | Eval Loss: 0.0241 | Eval R2: -6.0571 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0400 | Eval Loss: 0.0227 | Eval R2: -5.8690 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0373 | Eval Loss: 0.0210 | Eval R2: -5.8803 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0342 | Eval Loss: 0.0196 | Eval R2: -6.9657 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0188 | Eval R2: -9.1237 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0270 | Eval Loss: 0.0187 | Eval R2: -12.1291 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0240 | Eval Loss: 0.0178 | Eval R2: -12.9015 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0149 | Eval R2: -9.8526 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0123 | Eval R2: -6.9338 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0107 | Eval R2: -5.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0168 | Eval Loss: 0.0097 | Eval R2: -4.6913 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0156 | Eval Loss: 0.0087 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0080 | Eval R2: -3.3115 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0137 | Eval Loss: 0.0074 | Eval R2: -2.8324 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0130 | Eval Loss: 0.0069 | Eval R2: -2.5535 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0063 | Eval R2: -2.2374 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0113 | Eval Loss: 0.0059 | Eval R2: -1.9303 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0106 | Eval Loss: 0.0056 | Eval R2: -1.7210 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0100 | Eval Loss: 0.0054 | Eval R2: -1.5654 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.005597, test R2 score: -2.004708
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009975460357964039, 'r2_eval_final': -1.565418004989624, 'loss_eval_final': 0.005364386364817619, 'r2_test': -2.0047076365614602, 'loss_test': 0.005596609320491552, 'loss_nodes': [[0.002452634274959564, 0.0033022717107087374, 0.0026522281114012003, 0.004800139926373959, 0.004224689677357674, 0.003856227034702897, 0.005307621322572231, 0.007563397753983736, 0.005532231647521257, 0.006012852303683758, 0.0037624333053827286, 0.004094624426215887, 0.005834830924868584, 0.008045963943004608, 0.007648222614079714, 0.004923588130623102, 0.0066497838124632835, 0.009574453346431255, 0.006938895210623741, 0.008755107410252094]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00998
wandb: loss_eval 0.00536
wandb: loss_test 0.0056
wandb:   r2_eval -1.56542
wandb:   r2_test -2.00471
wandb: 
wandb: ðŸš€ View run lyric-oath-66 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/6p5cvg5n
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_064528-6p5cvg5n/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [22:02:25<11:05:24, 2851.77s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_070520-4sg9vvzi
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run azure-brook-67
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/4sg9vvzi

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.5714 | Eval Loss: 1.1964 | Eval R2: -1571.2195 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2065 | Eval Loss: 1.1030 | Eval R2: -1450.0873 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1196 | Eval Loss: 1.0303 | Eval R2: -1355.4410 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0491 | Eval Loss: 0.9661 | Eval R2: -1271.2419 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9822 | Eval Loss: 0.9007 | Eval R2: -1184.9846 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9122 | Eval Loss: 0.8322 | Eval R2: -1093.8607 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8388 | Eval Loss: 0.7601 | Eval R2: -997.5918 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.7628 | Eval Loss: 0.6853 | Eval R2: -898.0237 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.6851 | Eval Loss: 0.6099 | Eval R2: -797.5688 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.6069 | Eval Loss: 0.5349 | Eval R2: -697.9406 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.5296 | Eval Loss: 0.4618 | Eval R2: -600.9948 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.4545 | Eval Loss: 0.3919 | Eval R2: -508.5742 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3835 | Eval Loss: 0.3271 | Eval R2: -422.7261 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3189 | Eval Loss: 0.2692 | Eval R2: -345.6285 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2622 | Eval Loss: 0.2190 | Eval R2: -278.6515 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2133 | Eval Loss: 0.1765 | Eval R2: -222.0999 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1724 | Eval Loss: 0.1414 | Eval R2: -175.4744 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1388 | Eval Loss: 0.1130 | Eval R2: -137.7932 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1117 | Eval Loss: 0.0903 | Eval R2: -107.6463 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0900 | Eval Loss: 0.0722 | Eval R2: -83.6785 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0728 | Eval Loss: 0.0578 | Eval R2: -64.7004 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0591 | Eval Loss: 0.0464 | Eval R2: -49.7143 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0482 | Eval Loss: 0.0374 | Eval R2: -37.9432 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0396 | Eval Loss: 0.0304 | Eval R2: -28.7670 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0329 | Eval Loss: 0.0250 | Eval R2: -21.6695 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0278 | Eval Loss: 0.0207 | Eval R2: -16.2065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0237 | Eval Loss: 0.0175 | Eval R2: -12.0824 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.016875, test R2 score: -11.476670
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.023703522980213165, 'r2_eval_final': -12.082376480102539, 'loss_eval_final': 0.017496595159173012, 'r2_test': -11.476669669861437, 'loss_test': 0.016875458881258965, 'loss_nodes': [[0.0072154453955590725, 0.02029954455792904, 0.016651349142193794, 0.018049491569399834, 0.009962285868823528, 0.003366741118952632, 0.008111903443932533, 0.014381170272827148, 0.01449483260512352, 0.007540568709373474, 0.13177204132080078, 0.0073443991132080555, 0.00740716652944684, 0.005997508764266968, 0.009333358146250248, 0.010610408149659634, 0.013398177921772003, 0.009064704179763794, 0.009495525620877743, 0.013012601062655449]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0237
wandb: loss_eval 0.0175
wandb: loss_test 0.01688
wandb:   r2_eval -12.08238
wandb:   r2_test -11.47667
wandb: 
wandb: ðŸš€ View run azure-brook-67 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/4sg9vvzi
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_070520-4sg9vvzi/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 37/50 [22:29:59<9:00:04, 2492.62s/it] Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_073254-auf6a9qn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-shape-68
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/auf6a9qn

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9125 | Eval Loss: 0.6356 | Eval R2: -828.1833 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5501 | Eval Loss: 0.3538 | Eval R2: -450.4642 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2996 | Eval Loss: 0.1727 | Eval R2: -202.7828 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1489 | Eval Loss: 0.0786 | Eval R2: -74.9680 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0817 | Eval Loss: 0.0437 | Eval R2: -26.6139 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0566 | Eval Loss: 0.0313 | Eval R2: -11.2287 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0477 | Eval Loss: 0.0270 | Eval R2: -7.3468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0448 | Eval Loss: 0.0257 | Eval R2: -6.6191 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0424 | Eval Loss: 0.0241 | Eval R2: -6.0571 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0400 | Eval Loss: 0.0227 | Eval R2: -5.8690 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0373 | Eval Loss: 0.0210 | Eval R2: -5.8803 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0342 | Eval Loss: 0.0196 | Eval R2: -6.9657 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0188 | Eval R2: -9.1237 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0270 | Eval Loss: 0.0187 | Eval R2: -12.1291 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0240 | Eval Loss: 0.0178 | Eval R2: -12.9015 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0149 | Eval R2: -9.8526 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0123 | Eval R2: -6.9338 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0107 | Eval R2: -5.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0168 | Eval Loss: 0.0097 | Eval R2: -4.6913 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0156 | Eval Loss: 0.0087 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0080 | Eval R2: -3.3115 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0137 | Eval Loss: 0.0074 | Eval R2: -2.8324 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0130 | Eval Loss: 0.0069 | Eval R2: -2.5535 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0063 | Eval R2: -2.2374 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0113 | Eval Loss: 0.0059 | Eval R2: -1.9303 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0106 | Eval Loss: 0.0056 | Eval R2: -1.7210 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0100 | Eval Loss: 0.0054 | Eval R2: -1.5654 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.005597, test R2 score: -2.004708
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009975460357964039, 'r2_eval_final': -1.565418004989624, 'loss_eval_final': 0.005364386364817619, 'r2_test': -2.0047076365614602, 'loss_test': 0.005596609320491552, 'loss_nodes': [[0.002452634274959564, 0.0033022717107087374, 0.0026522281114012003, 0.004800139926373959, 0.004224689677357674, 0.003856227034702897, 0.005307621322572231, 0.007563397753983736, 0.005532231647521257, 0.006012852303683758, 0.0037624333053827286, 0.004094624426215887, 0.005834830924868584, 0.008045963943004608, 0.007648222614079714, 0.004923588130623102, 0.0066497838124632835, 0.009574453346431255, 0.006938895210623741, 0.008755107410252094]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00998
wandb: loss_eval 0.00536
wandb: loss_test 0.0056
wandb:   r2_eval -1.56542
wandb:   r2_test -2.00471
wandb: 
wandb: ðŸš€ View run rich-shape-68 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/auf6a9qn
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_073254-auf6a9qn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [22:50:46<7:03:46, 2118.86s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_075341-te5oyytn
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run grateful-galaxy-69
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/te5oyytn

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.5714 | Eval Loss: 1.1964 | Eval R2: -1571.2195 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2065 | Eval Loss: 1.1030 | Eval R2: -1450.0873 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1196 | Eval Loss: 1.0303 | Eval R2: -1355.4410 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0491 | Eval Loss: 0.9661 | Eval R2: -1271.2419 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9822 | Eval Loss: 0.9007 | Eval R2: -1184.9846 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9122 | Eval Loss: 0.8322 | Eval R2: -1093.8607 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8388 | Eval Loss: 0.7601 | Eval R2: -997.5918 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.7628 | Eval Loss: 0.6853 | Eval R2: -898.0237 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.6851 | Eval Loss: 0.6099 | Eval R2: -797.5688 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.6069 | Eval Loss: 0.5349 | Eval R2: -697.9406 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.5296 | Eval Loss: 0.4618 | Eval R2: -600.9948 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.4545 | Eval Loss: 0.3919 | Eval R2: -508.5742 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3835 | Eval Loss: 0.3271 | Eval R2: -422.7261 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3189 | Eval Loss: 0.2692 | Eval R2: -345.6285 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2622 | Eval Loss: 0.2190 | Eval R2: -278.6515 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2133 | Eval Loss: 0.1765 | Eval R2: -222.0999 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1724 | Eval Loss: 0.1414 | Eval R2: -175.4744 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1388 | Eval Loss: 0.1130 | Eval R2: -137.7932 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1117 | Eval Loss: 0.0903 | Eval R2: -107.6463 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0900 | Eval Loss: 0.0722 | Eval R2: -83.6785 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0728 | Eval Loss: 0.0578 | Eval R2: -64.7004 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0591 | Eval Loss: 0.0464 | Eval R2: -49.7143 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0482 | Eval Loss: 0.0374 | Eval R2: -37.9432 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0396 | Eval Loss: 0.0304 | Eval R2: -28.7670 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0329 | Eval Loss: 0.0250 | Eval R2: -21.6695 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0278 | Eval Loss: 0.0207 | Eval R2: -16.2065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0237 | Eval Loss: 0.0175 | Eval R2: -12.0824 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.016875, test R2 score: -11.476670
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.023703522980213165, 'r2_eval_final': -12.082376480102539, 'loss_eval_final': 0.017496595159173012, 'r2_test': -11.476669669861437, 'loss_test': 0.016875458881258965, 'loss_nodes': [[0.0072154453955590725, 0.02029954455792904, 0.016651349142193794, 0.018049491569399834, 0.009962285868823528, 0.003366741118952632, 0.008111903443932533, 0.014381170272827148, 0.01449483260512352, 0.007540568709373474, 0.13177204132080078, 0.0073443991132080555, 0.00740716652944684, 0.005997508764266968, 0.009333358146250248, 0.010610408149659634, 0.013398177921772003, 0.009064704179763794, 0.009495525620877743, 0.013012601062655449]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.0237
wandb: loss_eval 0.0175
wandb: loss_test 0.01688
wandb:   r2_eval -12.08238
wandb:   r2_test -11.47667
wandb: 
wandb: ðŸš€ View run grateful-galaxy-69 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/te5oyytn
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_075341-te5oyytn/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [23:17:44<6:00:54, 1968.64s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_082039-ckp2y4z3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run amber-meadow-70
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/ckp2y4z3

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9125 | Eval Loss: 0.6356 | Eval R2: -828.1833 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5501 | Eval Loss: 0.3538 | Eval R2: -450.4642 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2996 | Eval Loss: 0.1727 | Eval R2: -202.7828 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1489 | Eval Loss: 0.0786 | Eval R2: -74.9680 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0817 | Eval Loss: 0.0437 | Eval R2: -26.6139 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0566 | Eval Loss: 0.0313 | Eval R2: -11.2287 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0477 | Eval Loss: 0.0270 | Eval R2: -7.3468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0448 | Eval Loss: 0.0257 | Eval R2: -6.6191 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0424 | Eval Loss: 0.0241 | Eval R2: -6.0571 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0400 | Eval Loss: 0.0227 | Eval R2: -5.8690 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0373 | Eval Loss: 0.0210 | Eval R2: -5.8803 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0342 | Eval Loss: 0.0196 | Eval R2: -6.9657 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0188 | Eval R2: -9.1237 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0270 | Eval Loss: 0.0187 | Eval R2: -12.1291 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0240 | Eval Loss: 0.0178 | Eval R2: -12.9015 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0149 | Eval R2: -9.8526 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0123 | Eval R2: -6.9338 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0107 | Eval R2: -5.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0168 | Eval Loss: 0.0097 | Eval R2: -4.6913 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0156 | Eval Loss: 0.0087 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0080 | Eval R2: -3.3115 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0137 | Eval Loss: 0.0074 | Eval R2: -2.8324 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0130 | Eval Loss: 0.0069 | Eval R2: -2.5535 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0063 | Eval R2: -2.2374 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0113 | Eval Loss: 0.0059 | Eval R2: -1.9303 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0106 | Eval Loss: 0.0056 | Eval R2: -1.7210 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0100 | Eval Loss: 0.0054 | Eval R2: -1.5654 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.005597, test R2 score: -2.004708
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009975460357964039, 'r2_eval_final': -1.565418004989624, 'loss_eval_final': 0.005364386364817619, 'r2_test': -2.0047076365614602, 'loss_test': 0.005596609320491552, 'loss_nodes': [[0.002452634274959564, 0.0033022717107087374, 0.0026522281114012003, 0.004800139926373959, 0.004224689677357674, 0.003856227034702897, 0.005307621322572231, 0.007563397753983736, 0.005532231647521257, 0.006012852303683758, 0.0037624333053827286, 0.004094624426215887, 0.005834830924868584, 0.008045963943004608, 0.007648222614079714, 0.004923588130623102, 0.0066497838124632835, 0.009574453346431255, 0.006938895210623741, 0.008755107410252094]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00998
wandb: loss_eval 0.00536
wandb: loss_test 0.0056
wandb:   r2_eval -1.56542
wandb:   r2_test -2.00471
wandb: 
wandb: ðŸš€ View run amber-meadow-70 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/ckp2y4z3
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_082039-ckp2y4z3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [23:38:06<4:50:45, 1744.57s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_084101-f1whz2ws
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run devoted-plasma-71
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/f1whz2ws

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.3282 | Eval Loss: 1.6572 | Eval R2: -2150.4080 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.5703 | Eval Loss: 1.3522 | Eval R2: -1755.8922 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.3168 | Eval Loss: 1.1583 | Eval R2: -1508.8317 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.1326 | Eval Loss: 0.9951 | Eval R2: -1296.5870 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9874 | Eval Loss: 0.8761 | Eval R2: -1141.4620 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.8677 | Eval Loss: 0.7706 | Eval R2: -1003.1132 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.7659 | Eval Loss: 0.6806 | Eval R2: -884.1174 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.6778 | Eval Loss: 0.6020 | Eval R2: -779.5423 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.6011 | Eval Loss: 0.5333 | Eval R2: -688.2289 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.5342 | Eval Loss: 0.4734 | Eval R2: -608.5543 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.4755 | Eval Loss: 0.4208 | Eval R2: -538.8421 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.4237 | Eval Loss: 0.3742 | Eval R2: -477.2992 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.3777 | Eval Loss: 0.3328 | Eval R2: -422.7291 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.3367 | Eval Loss: 0.2958 | Eval R2: -374.0954 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.2997 | Eval Loss: 0.2625 | Eval R2: -330.5504 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.2665 | Eval Loss: 0.2326 | Eval R2: -291.4294 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2365 | Eval Loss: 0.2054 | Eval R2: -256.1612 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2092 | Eval Loss: 0.1809 | Eval R2: -224.3621 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1845 | Eval Loss: 0.1586 | Eval R2: -195.6070 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1622 | Eval Loss: 0.1385 | Eval R2: -169.6536 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1419 | Eval Loss: 0.1203 | Eval R2: -146.1759 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1236 | Eval Loss: 0.1038 | Eval R2: -125.0420 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.1070 | Eval Loss: 0.0891 | Eval R2: -106.0660 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0923 | Eval Loss: 0.0759 | Eval R2: -89.1881 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0792 | Eval Loss: 0.0644 | Eval R2: -74.3706 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0677 | Eval Loss: 0.0544 | Eval R2: -61.4893 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0577 | Eval Loss: 0.0457 | Eval R2: -50.3745 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.046126, test R2 score: -44.306491
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05773135647177696, 'r2_eval_final': -50.37446594238281, 'loss_eval_final': 0.045736148953437805, 'r2_test': -44.306490943999, 'loss_test': 0.04612583667039871, 'loss_nodes': [[0.2803061306476593, 0.01629689894616604, 0.010288788005709648, 0.004464718047529459, 0.007843839935958385, 0.00891904253512621, 0.015844933688640594, 0.009098434820771217, 0.013574357144534588, 0.00996175967156887, 0.01902969740331173, 0.05328703671693802, 0.010281577706336975, 0.015395130030810833, 0.010568448342382908, 0.022547461092472076, 0.33414772152900696, 0.014339873567223549, 0.014233856461942196, 0.05208705738186836]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05773
wandb: loss_eval 0.04574
wandb: loss_test 0.04613
wandb:   r2_eval -50.37447
wandb:   r2_test -44.30649
wandb: 
wandb: ðŸš€ View run devoted-plasma-71 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/f1whz2ws
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_084101-f1whz2ws/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 41/50 [23:50:48<3:37:28, 1449.86s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_085343-03stp2ee
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-breeze-72
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/03stp2ee

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3182 | Eval Loss: 1.0123 | Eval R2: -1324.4379 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.9830 | Eval Loss: 0.8684 | Eval R2: -1138.7653 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.8501 | Eval Loss: 0.7513 | Eval R2: -986.9944 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7412 | Eval Loss: 0.6547 | Eval R2: -860.9456 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.6499 | Eval Loss: 0.5724 | Eval R2: -752.7508 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.5707 | Eval Loss: 0.5003 | Eval R2: -657.0286 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5003 | Eval Loss: 0.4358 | Eval R2: -570.6354 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.4369 | Eval Loss: 0.3779 | Eval R2: -492.2425 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3798 | Eval Loss: 0.3264 | Eval R2: -421.6185 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.3289 | Eval Loss: 0.2810 | Eval R2: -358.8567 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2843 | Eval Loss: 0.2416 | Eval R2: -303.8835 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.2456 | Eval Loss: 0.2077 | Eval R2: -256.3029 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.2123 | Eval Loss: 0.1788 | Eval R2: -215.4800 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1839 | Eval Loss: 0.1542 | Eval R2: -180.6871 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.1598 | Eval Loss: 0.1334 | Eval R2: -151.1991 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.1395 | Eval Loss: 0.1159 | Eval R2: -126.3311 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.1224 | Eval Loss: 0.1013 | Eval R2: -105.4564 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.1082 | Eval Loss: 0.0892 | Eval R2: -88.0139 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0963 | Eval Loss: 0.0791 | Eval R2: -73.5083 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0865 | Eval Loss: 0.0709 | Eval R2: -61.5028 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0785 | Eval Loss: 0.0641 | Eval R2: -51.6137 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0718 | Eval Loss: 0.0586 | Eval R2: -43.5052 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0665 | Eval Loss: 0.0542 | Eval R2: -36.8851 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0621 | Eval Loss: 0.0506 | Eval R2: -31.5015 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0586 | Eval Loss: 0.0477 | Eval R2: -27.1397 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0557 | Eval Loss: 0.0454 | Eval R2: -23.6179 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0535 | Eval Loss: 0.0435 | Eval R2: -20.7835 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.039728, test R2 score: -21.116098
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05345681682229042, 'r2_eval_final': -20.783491134643555, 'loss_eval_final': 0.04352850094437599, 'r2_test': -21.116098323979795, 'loss_test': 0.0397278368473053, 'loss_nodes': [[0.03219415992498398, 0.0338757149875164, 0.03295576572418213, 0.031400248408317566, 0.04492153972387314, 0.11406455934047699, 0.03303079679608345, 0.033421099185943604, 0.03823458030819893, 0.04304229095578194, 0.03185929358005524, 0.033143021166324615, 0.032705556601285934, 0.036894381046295166, 0.032979100942611694, 0.05868392065167427, 0.032557424157857895, 0.033269815146923065, 0.03256424143910408, 0.03275926783680916]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05346
wandb: loss_eval 0.04353
wandb: loss_test 0.03973
wandb:   r2_eval -20.78349
wandb:   r2_test -21.1161
wandb: 
wandb: ðŸš€ View run dulcet-breeze-72 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/03stp2ee
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_085343-03stp2ee/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [24:08:38<2:58:06, 1335.85s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_091133-dxoit1k6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run exalted-water-73
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/dxoit1k6

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.4771 | Eval Loss: 1.1092 | Eval R2: -1456.4575 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.9603 | Eval Loss: 0.7069 | Eval R2: -926.5992 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6059 | Eval Loss: 0.4293 | Eval R2: -559.7743 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3694 | Eval Loss: 0.2525 | Eval R2: -325.6609 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2157 | Eval Loss: 0.1386 | Eval R2: -173.8532 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1197 | Eval Loss: 0.0724 | Eval R2: -85.0931 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0647 | Eval Loss: 0.0363 | Eval R2: -37.1404 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0356 | Eval Loss: 0.0184 | Eval R2: -13.6414 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0216 | Eval Loss: 0.0111 | Eval R2: -4.2548 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0161 | Eval Loss: 0.0088 | Eval R2: -1.6429 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0143 | Eval Loss: 0.0081 | Eval R2: -1.2037 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0133 | Eval Loss: 0.0077 | Eval R2: -1.0832 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0124 | Eval Loss: 0.0073 | Eval R2: -0.9196 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0117 | Eval Loss: 0.0069 | Eval R2: -0.8183 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0110 | Eval Loss: 0.0066 | Eval R2: -0.7245 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0104 | Eval Loss: 0.0063 | Eval R2: -0.6396 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0099 | Eval Loss: 0.0061 | Eval R2: -0.5889 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0093 | Eval Loss: 0.0058 | Eval R2: -0.5129 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0088 | Eval Loss: 0.0056 | Eval R2: -0.4359 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0083 | Eval Loss: 0.0053 | Eval R2: -0.3789 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0080 | Eval Loss: 0.0052 | Eval R2: -0.3300 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0076 | Eval Loss: 0.0050 | Eval R2: -0.2756 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0072 | Eval Loss: 0.0048 | Eval R2: -0.2275 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0069 | Eval Loss: 0.0046 | Eval R2: -0.1827 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0066 | Eval Loss: 0.0045 | Eval R2: -0.1452 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0064 | Eval Loss: 0.0044 | Eval R2: -0.1173 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0061 | Eval Loss: 0.0042 | Eval R2: -0.0830 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.003772, test R2 score: -0.503625
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.006137287709861994, 'r2_eval_final': -0.08300533145666122, 'loss_eval_final': 0.004229082725942135, 'r2_test': -0.5036245481516022, 'loss_test': 0.003771590068936348, 'loss_nodes': [[0.001237940858118236, 0.003674688283354044, 0.0015434410888701677, 0.003098956076428294, 0.0015254857717081904, 0.0012416751123964787, 0.0019193013431504369, 0.003833367954939604, 0.005477069411426783, 0.0022304262965917587, 0.0022708200849592686, 0.005205524619668722, 0.0064371866174042225, 0.003656538436189294, 0.005485201720148325, 0.003751211566850543, 0.004891593009233475, 0.005332895088940859, 0.005764184053987265, 0.006854300387203693]]}
wandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00614
wandb: loss_eval 0.00423
wandb: loss_test 0.00377
wandb:   r2_eval -0.08301
wandb:   r2_test -0.50362
wandb: 
wandb: ðŸš€ View run exalted-water-73 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/dxoit1k6
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_091133-dxoit1k6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [24:32:35<2:39:22, 1366.11s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_093530-f8iirgdj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smart-smoke-74
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/f8iirgdj

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9125 | Eval Loss: 0.6356 | Eval R2: -828.1833 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5501 | Eval Loss: 0.3538 | Eval R2: -450.4642 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2996 | Eval Loss: 0.1727 | Eval R2: -202.7828 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1489 | Eval Loss: 0.0786 | Eval R2: -74.9680 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0817 | Eval Loss: 0.0437 | Eval R2: -26.6139 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0566 | Eval Loss: 0.0313 | Eval R2: -11.2287 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0477 | Eval Loss: 0.0270 | Eval R2: -7.3468 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0448 | Eval Loss: 0.0257 | Eval R2: -6.6191 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0424 | Eval Loss: 0.0241 | Eval R2: -6.0571 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0400 | Eval Loss: 0.0227 | Eval R2: -5.8690 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0373 | Eval Loss: 0.0210 | Eval R2: -5.8803 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0342 | Eval Loss: 0.0196 | Eval R2: -6.9657 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0188 | Eval R2: -9.1237 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0270 | Eval Loss: 0.0187 | Eval R2: -12.1291 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0240 | Eval Loss: 0.0178 | Eval R2: -12.9015 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0217 | Eval Loss: 0.0149 | Eval R2: -9.8526 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0198 | Eval Loss: 0.0123 | Eval R2: -6.9338 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0107 | Eval R2: -5.5611 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0168 | Eval Loss: 0.0097 | Eval R2: -4.6913 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0156 | Eval Loss: 0.0087 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0080 | Eval R2: -3.3115 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0137 | Eval Loss: 0.0074 | Eval R2: -2.8324 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0130 | Eval Loss: 0.0069 | Eval R2: -2.5535 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0123 | Eval Loss: 0.0063 | Eval R2: -2.2374 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0113 | Eval Loss: 0.0059 | Eval R2: -1.9303 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0106 | Eval Loss: 0.0056 | Eval R2: -1.7210 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0100 | Eval Loss: 0.0054 | Eval R2: -1.5654 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.005597, test R2 score: -2.004708
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009975460357964039, 'r2_eval_final': -1.565418004989624, 'loss_eval_final': 0.005364386364817619, 'r2_test': -2.0047076365614602, 'loss_test': 0.005596609320491552, 'loss_nodes': [[0.002452634274959564, 0.0033022717107087374, 0.0026522281114012003, 0.004800139926373959, 0.004224689677357674, 0.003856227034702897, 0.005307621322572231, 0.007563397753983736, 0.005532231647521257, 0.006012852303683758, 0.0037624333053827286, 0.004094624426215887, 0.005834830924868584, 0.008045963943004608, 0.007648222614079714, 0.004923588130623102, 0.0066497838124632835, 0.009574453346431255, 0.006938895210623741, 0.008755107410252094]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.006 MB uploadedwandb: / 0.002 MB of 0.006 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00998
wandb: loss_eval 0.00536
wandb: loss_test 0.0056
wandb:   r2_eval -1.56542
wandb:   r2_test -2.00471
wandb: 
wandb: ðŸš€ View run smart-smoke-74 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/f8iirgdj
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_093530-f8iirgdj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [24:54:18<2:14:44, 1347.39s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_095714-0un7lslq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vague-sunset-75
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/0un7lslq

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9041 | Eval Loss: 0.7612 | Eval R2: -996.9238 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7647 | Eval Loss: 0.6846 | Eval R2: -899.0084 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6941 | Eval Loss: 0.6216 | Eval R2: -812.9354 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6296 | Eval Loss: 0.5585 | Eval R2: -728.5205 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5629 | Eval Loss: 0.4948 | Eval R2: -642.3735 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4953 | Eval Loss: 0.4306 | Eval R2: -554.8746 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4288 | Eval Loss: 0.3687 | Eval R2: -471.0504 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3659 | Eval Loss: 0.3106 | Eval R2: -392.5980 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3078 | Eval Loss: 0.2579 | Eval R2: -321.1087 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2554 | Eval Loss: 0.2111 | Eval R2: -257.8965 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2094 | Eval Loss: 0.1710 | Eval R2: -203.8057 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1707 | Eval Loss: 0.1376 | Eval R2: -159.1200 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1388 | Eval Loss: 0.1106 | Eval R2: -123.3797 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1132 | Eval Loss: 0.0893 | Eval R2: -95.6407 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0928 | Eval Loss: 0.0726 | Eval R2: -74.2588 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0764 | Eval Loss: 0.0594 | Eval R2: -57.8652 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0631 | Eval Loss: 0.0487 | Eval R2: -45.1380 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0520 | Eval Loss: 0.0403 | Eval R2: -35.2741 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0429 | Eval Loss: 0.0332 | Eval R2: -26.9667 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0275 | Eval R2: -20.1591 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0300 | Eval Loss: 0.0230 | Eval R2: -14.6814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0257 | Eval Loss: 0.0196 | Eval R2: -10.7098 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0225 | Eval Loss: 0.0171 | Eval R2: -7.9165 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0200 | Eval Loss: 0.0151 | Eval R2: -5.9292 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0180 | Eval Loss: 0.0137 | Eval R2: -4.8086 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0165 | Eval Loss: 0.0124 | Eval R2: -3.8270 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0151 | Eval Loss: 0.0114 | Eval R2: -3.1922 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.010108, test R2 score: -3.593988
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.01514763105660677, 'r2_eval_final': -3.192204236984253, 'loss_eval_final': 0.011433062143623829, 'r2_test': -3.593987595411585, 'loss_test': 0.010108032263815403, 'loss_nodes': [[0.0049039022997021675, 0.019939454272389412, 0.007327746134251356, 0.0029728764202445745, 0.0033477996475994587, 0.023724354803562164, 0.003918927162885666, 0.012832381762564182, 0.004782555159181356, 0.003684115596115589, 0.00440628919750452, 0.008504842408001423, 0.013646646402776241, 0.01789482869207859, 0.0067116073332726955, 0.012484489008784294, 0.009863875806331635, 0.015354786068201065, 0.013131584040820599, 0.01272757537662983]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01515
wandb: loss_eval 0.01143
wandb: loss_test 0.01011
wandb:   r2_eval -3.1922
wandb:   r2_test -3.59399
wandb: 
wandb: ðŸš€ View run vague-sunset-75 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/0un7lslq
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_095714-0un7lslq/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 45/50 [25:14:54<1:49:29, 1313.89s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_101749-rrtk6hzd
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run flowing-lake-76
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/rrtk6hzd

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3704 | Eval Loss: 1.1588 | Eval R2: -1522.6636 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.1019 | Eval Loss: 0.9470 | Eval R2: -1245.9148 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.9013 | Eval Loss: 0.7650 | Eval R2: -1006.7076 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7234 | Eval Loss: 0.5988 | Eval R2: -786.5292 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5588 | Eval Loss: 0.4462 | Eval R2: -582.3038 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4103 | Eval Loss: 0.3139 | Eval R2: -403.0302 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2855 | Eval Loss: 0.2090 | Eval R2: -258.7387 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1901 | Eval Loss: 0.1339 | Eval R2: -153.9694 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1248 | Eval Loss: 0.0867 | Eval R2: -86.5291 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0855 | Eval Loss: 0.0607 | Eval R2: -48.0117 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0643 | Eval Loss: 0.0478 | Eval R2: -27.9694 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0539 | Eval Loss: 0.0419 | Eval R2: -18.1874 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0490 | Eval Loss: 0.0391 | Eval R2: -13.5753 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0468 | Eval Loss: 0.0379 | Eval R2: -11.4338 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0457 | Eval Loss: 0.0373 | Eval R2: -10.4576 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0453 | Eval Loss: 0.0371 | Eval R2: -10.0297 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0451 | Eval Loss: 0.0370 | Eval R2: -9.8503 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7739 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7381 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7208 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7130 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7096 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7080 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7071 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7065 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7060 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7055 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032325, test R2 score: -11.425828
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.04493798688054085, 'r2_eval_final': -9.705501556396484, 'loss_eval_final': 0.036991655826568604, 'r2_test': -11.42582807908623, 'loss_test': 0.03232539817690849, 'loss_nodes': [[0.03110206313431263, 0.03264728561043739, 0.03281053900718689, 0.03139190375804901, 0.03299935907125473, 0.03204686939716339, 0.033029913902282715, 0.033427681773900986, 0.032020606100559235, 0.032134972512722015, 0.03186018764972687, 0.032381873577833176, 0.03260040283203125, 0.03131607919931412, 0.03297436982393265, 0.031206253916025162, 0.032495930790901184, 0.03298694267868996, 0.032477378845214844, 0.03259730339050293]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04494
wandb: loss_eval 0.03699
wandb: loss_test 0.03233
wandb:   r2_eval -9.7055
wandb:   r2_test -11.42583
wandb: 
wandb: ðŸš€ View run flowing-lake-76 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/rrtk6hzd
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_101749-rrtk6hzd/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 46/50 [25:26:50<1:15:37, 1134.42s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_102945-83454mz8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-frog-77
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/83454mz8

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.0337 | Eval Loss: 0.7933 | Eval R2: -1042.4657 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7872 | Eval Loss: 0.6690 | Eval R2: -879.5557 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6585 | Eval Loss: 0.5482 | Eval R2: -717.2827 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.5360 | Eval Loss: 0.4317 | Eval R2: -558.2559 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4109 | Eval Loss: 0.3109 | Eval R2: -394.5335 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2873 | Eval Loss: 0.1999 | Eval R2: -243.8269 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1831 | Eval Loss: 0.1172 | Eval R2: -132.0788 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1112 | Eval Loss: 0.0657 | Eval R2: -62.7667 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0687 | Eval Loss: 0.0390 | Eval R2: -26.8953 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0475 | Eval Loss: 0.0273 | Eval R2: -11.9896 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0379 | Eval Loss: 0.0226 | Eval R2: -6.7223 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0335 | Eval Loss: 0.0202 | Eval R2: -5.0845 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0305 | Eval Loss: 0.0185 | Eval R2: -4.4338 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0282 | Eval Loss: 0.0169 | Eval R2: -4.0049 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0258 | Eval Loss: 0.0153 | Eval R2: -3.6145 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0235 | Eval Loss: 0.0137 | Eval R2: -3.2292 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0214 | Eval Loss: 0.0122 | Eval R2: -2.9122 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0193 | Eval Loss: 0.0110 | Eval R2: -2.6363 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0175 | Eval Loss: 0.0099 | Eval R2: -2.3717 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0159 | Eval Loss: 0.0091 | Eval R2: -2.0724 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0146 | Eval Loss: 0.0085 | Eval R2: -1.7670 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0138 | Eval Loss: 0.0081 | Eval R2: -1.5797 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0131 | Eval Loss: 0.0079 | Eval R2: -1.4502 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0126 | Eval Loss: 0.0077 | Eval R2: -1.3556 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0122 | Eval Loss: 0.0075 | Eval R2: -1.3664 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0118 | Eval Loss: 0.0073 | Eval R2: -1.3398 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0115 | Eval Loss: 0.0072 | Eval R2: -1.4419 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.006700, test R2 score: -2.012458
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.011464916169643402, 'r2_eval_final': -1.4419054985046387, 'loss_eval_final': 0.007208006922155619, 'r2_test': -2.0124577484154904, 'loss_test': 0.00669993506744504, 'loss_nodes': [[0.006285675335675478, 0.007602578029036522, 0.004641087260097265, 0.004516795743256807, 0.0038700795266777277, 0.003085093107074499, 0.006708220113068819, 0.005903664510697126, 0.006014675833284855, 0.004023987799882889, 0.006565439980477095, 0.0071082101203501225, 0.006082074251025915, 0.012020086869597435, 0.008868887089192867, 0.009200847707688808, 0.007203719578683376, 0.00757422111928463, 0.008437776938080788, 0.008285571821033955]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01146
wandb: loss_eval 0.00721
wandb: loss_test 0.0067
wandb:   r2_eval -1.44191
wandb:   r2_test -2.01246
wandb: 
wandb: ðŸš€ View run deep-frog-77 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/83454mz8
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_102945-83454mz8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [25:47:03<57:53, 1157.95s/it]  Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_104958-h05s7w2q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run peach-oath-78
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/h05s7w2q

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9041 | Eval Loss: 0.7612 | Eval R2: -996.9238 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7647 | Eval Loss: 0.6846 | Eval R2: -899.0084 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6941 | Eval Loss: 0.6216 | Eval R2: -812.9354 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6296 | Eval Loss: 0.5585 | Eval R2: -728.5205 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.5629 | Eval Loss: 0.4948 | Eval R2: -642.3735 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4953 | Eval Loss: 0.4306 | Eval R2: -554.8746 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.4288 | Eval Loss: 0.3687 | Eval R2: -471.0504 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3659 | Eval Loss: 0.3106 | Eval R2: -392.5980 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.3078 | Eval Loss: 0.2579 | Eval R2: -321.1087 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.2554 | Eval Loss: 0.2111 | Eval R2: -257.8965 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.2094 | Eval Loss: 0.1710 | Eval R2: -203.8057 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1707 | Eval Loss: 0.1376 | Eval R2: -159.1200 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.1388 | Eval Loss: 0.1106 | Eval R2: -123.3797 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.1132 | Eval Loss: 0.0893 | Eval R2: -95.6407 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0928 | Eval Loss: 0.0726 | Eval R2: -74.2588 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0764 | Eval Loss: 0.0594 | Eval R2: -57.8652 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0631 | Eval Loss: 0.0487 | Eval R2: -45.1380 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0520 | Eval Loss: 0.0403 | Eval R2: -35.2741 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0429 | Eval Loss: 0.0332 | Eval R2: -26.9667 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0356 | Eval Loss: 0.0275 | Eval R2: -20.1591 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0300 | Eval Loss: 0.0230 | Eval R2: -14.6814 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0257 | Eval Loss: 0.0196 | Eval R2: -10.7098 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0225 | Eval Loss: 0.0171 | Eval R2: -7.9165 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0200 | Eval Loss: 0.0151 | Eval R2: -5.9292 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0180 | Eval Loss: 0.0137 | Eval R2: -4.8086 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0165 | Eval Loss: 0.0124 | Eval R2: -3.8270 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0151 | Eval Loss: 0.0114 | Eval R2: -3.1922 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.010108, test R2 score: -3.593988
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.01514763105660677, 'r2_eval_final': -3.192204236984253, 'loss_eval_final': 0.011433062143623829, 'r2_test': -3.593987595411585, 'loss_test': 0.010108032263815403, 'loss_nodes': [[0.0049039022997021675, 0.019939454272389412, 0.007327746134251356, 0.0029728764202445745, 0.0033477996475994587, 0.023724354803562164, 0.003918927162885666, 0.012832381762564182, 0.004782555159181356, 0.003684115596115589, 0.00440628919750452, 0.008504842408001423, 0.013646646402776241, 0.01789482869207859, 0.0067116073332726955, 0.012484489008784294, 0.009863875806331635, 0.015354786068201065, 0.013131584040820599, 0.01272757537662983]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01515
wandb: loss_eval 0.01143
wandb: loss_test 0.01011
wandb:   r2_eval -3.1922
wandb:   r2_test -3.59399
wandb: 
wandb: ðŸš€ View run peach-oath-78 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/h05s7w2q
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_104958-h05s7w2q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [26:02:56<36:32, 1096.46s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: | Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_110551-adt2knvf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run sweet-cosmos-79
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/adt2knvf

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 2.1175 | Eval Loss: 1.3449 | Eval R2: -1754.3196 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2933 | Eval Loss: 1.1474 | Eval R2: -1504.5608 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.1382 | Eval Loss: 1.0401 | Eval R2: -1366.1879 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0502 | Eval Loss: 0.9654 | Eval R2: -1269.3722 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9783 | Eval Loss: 0.9011 | Eval R2: -1185.9937 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.9168 | Eval Loss: 0.8435 | Eval R2: -1110.6115 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.8616 | Eval Loss: 0.7932 | Eval R2: -1044.3988 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.8103 | Eval Loss: 0.7449 | Eval R2: -980.2150 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.7593 | Eval Loss: 0.6945 | Eval R2: -913.0714 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.7051 | Eval Loss: 0.6380 | Eval R2: -836.5386 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.6470 | Eval Loss: 0.5811 | Eval R2: -759.8204 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.5867 | Eval Loss: 0.5214 | Eval R2: -679.7445 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.5240 | Eval Loss: 0.4600 | Eval R2: -596.6365 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.4599 | Eval Loss: 0.3988 | Eval R2: -512.8257 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.3968 | Eval Loss: 0.3396 | Eval R2: -431.8340 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.3367 | Eval Loss: 0.2841 | Eval R2: -356.0157 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.2811 | Eval Loss: 0.2339 | Eval R2: -287.4590 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.2317 | Eval Loss: 0.1904 | Eval R2: -227.9376 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.1895 | Eval Loss: 0.1541 | Eval R2: -178.0724 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.1546 | Eval Loss: 0.1246 | Eval R2: -137.6694 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.1266 | Eval Loss: 0.1013 | Eval R2: -105.7636 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.1047 | Eval Loss: 0.0831 | Eval R2: -81.0401 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0876 | Eval Loss: 0.0691 | Eval R2: -62.1930 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0746 | Eval Loss: 0.0585 | Eval R2: -47.9218 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0647 | Eval Loss: 0.0503 | Eval R2: -37.2229 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0569 | Eval Loss: 0.0441 | Eval R2: -29.1635 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0511 | Eval Loss: 0.0392 | Eval R2: -23.1495 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.037154, test R2 score: -22.710716
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.05107304826378822, 'r2_eval_final': -23.149497985839844, 'loss_eval_final': 0.0392075851559639, 'r2_test': -22.710716384304437, 'loss_test': 0.037153519690036774, 'loss_nodes': [[0.025696467608213425, 0.028831860050559044, 0.020902646705508232, 0.06295966356992722, 0.026749033480882645, 0.028263935819268227, 0.02382110431790352, 0.033269356936216354, 0.029641836881637573, 0.03743165358901024, 0.02297825738787651, 0.02518240176141262, 0.02551611140370369, 0.038175154477357864, 0.029250068590044975, 0.03381653502583504, 0.028005244210362434, 0.024492226541042328, 0.06226000189781189, 0.1358269900083542]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.05107
wandb: loss_eval 0.03921
wandb: loss_test 0.03715
wandb:   r2_eval -23.1495
wandb:   r2_test -22.71072
wandb: 
wandb: ðŸš€ View run sweet-cosmos-79 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/adt2knvf
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_110551-adt2knvf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [26:51:34<27:23, 1643.10s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.5
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240727_115429-e67986zl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run robust-valley-80
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/e67986zl

==================== DATASET INFO ===================

Train dataset: 2380
Validation dataset: 510
Test dataset: 510

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3063 | Eval Loss: 1.0793 | Eval R2: -1420.9307 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.0560 | Eval Loss: 0.9341 | Eval R2: -1230.3807 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.9089 | Eval Loss: 0.7920 | Eval R2: -1042.1178 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.7629 | Eval Loss: 0.6486 | Eval R2: -850.6778 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.6140 | Eval Loss: 0.5035 | Eval R2: -655.7615 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.4661 | Eval Loss: 0.3644 | Eval R2: -467.9607 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.3298 | Eval Loss: 0.2439 | Eval R2: -303.8376 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.2179 | Eval Loss: 0.1533 | Eval R2: -178.5770 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1389 | Eval Loss: 0.0957 | Eval R2: -97.0186 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0913 | Eval Loss: 0.0645 | Eval R2: -51.3696 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0664 | Eval Loss: 0.0493 | Eval R2: -28.4781 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0544 | Eval Loss: 0.0423 | Eval R2: -17.8434 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0490 | Eval Loss: 0.0392 | Eval R2: -13.2202 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0467 | Eval Loss: 0.0379 | Eval R2: -11.2576 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0457 | Eval Loss: 0.0374 | Eval R2: -10.3884 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0452 | Eval Loss: 0.0371 | Eval R2: -9.9924 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0451 | Eval Loss: 0.0370 | Eval R2: -9.8209 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7534 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0450 | Eval Loss: 0.0370 | Eval R2: -9.7282 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7178 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7126 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7099 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7086 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7079 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7074 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7069 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0449 | Eval Loss: 0.0370 | Eval R2: -9.7064 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.032326, test R2 score: -11.426744
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.0449376255273819, 'r2_eval_final': -9.706374168395996, 'loss_eval_final': 0.03699089214205742, 'r2_test': -11.426743797802617, 'loss_test': 0.03232550993561745, 'loss_nodes': [[0.031102033331990242, 0.03264738991856575, 0.032808396965265274, 0.03139190003275871, 0.03300034627318382, 0.032048579305410385, 0.033028747886419296, 0.033429570496082306, 0.03202066197991371, 0.03213421627879143, 0.031860820949077606, 0.0323820486664772, 0.03260233625769615, 0.0313163585960865, 0.032973770052194595, 0.03120523691177368, 0.03249577432870865, 0.03298669680953026, 0.032477300614118576, 0.03259793668985367]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.04494
wandb: loss_eval 0.03699
wandb: loss_test 0.03233
wandb:   r2_eval -9.70637
wandb:   r2_test -11.42674
wandb: 
wandb: ðŸš€ View run robust-valley-80 at: https://wandb.ai/maragumar01/mtgnn_branch_trip/runs/e67986zl
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_branch_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240727_115429-e67986zl/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [27:06:38<00:00, 1421.23s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [27:06:38<00:00, 1951.97s/it]

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.003772, test R2 score: -0.503625

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.003296
1   LSTM_NOBATCH  ...   0.004827
2    DyGrEncoder  ...   0.002677
3      MPNN_LSTM  ...   0.003865
4         MSTGCN  ...   0.003641
5      EvolveGCN  ...   0.038322
6         ASTGCN  ...   0.003370
7          AGCRN  ...   0.009095
8          DCRNN  ...   0.003779
9          MTGNN  ...   0.011934
10         MTGNN  ...   0.006137
11         MTGNN  ...   0.006137

[12 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

