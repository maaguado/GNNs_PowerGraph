Processing dataset...
Node:  0  not included, including...
Node:  1  not included, including...
Node:  2  not included, including...
Node:  3  not included, including...
Node:  4  not included, including...
Node:  5  not included, including...
Node:  6  not included, including...
Node:  7  not included, including...
Node:  8  not included, including...
Node:  9  not included, including...
Node:  10  not included, including...
Node:  11  not included, including...
Node:  12  not included, including...
Node:  13  not included, including...
Node:  14  not included, including...
Node:  15  not included, including...
Node:  16  not included, including...
Node:  17  not included, including...
Node:  18  not included, including...
Node:  19  not included, including...
Node:  20  not included, including...
Node:  21  not included, including...
Node:  22  not included, including...
Skipping  row_328
Ajustando modelo para gen_trip...
Number of situations:  549
Number of timestamps:  800
Number of situations of the selected type:  184
  0%|          | 0/50 [00:00<?, ?it/s]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: Currently logged in as: maragumar01. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240718_081951-ackfwpc3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run elated-plant-224
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/ackfwpc3

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.4524 | Eval Loss: 1.3294 | Eval R2: -651.6798 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.2965 | Eval Loss: 1.2473 | Eval R2: -609.2119 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 1.2002 | Eval Loss: 1.1388 | Eval R2: -555.2855 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 1.0745 | Eval Loss: 0.9949 | Eval R2: -483.5457 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.9091 | Eval Loss: 0.8086 | Eval R2: -391.3849 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.7101 | Eval Loss: 0.6038 | Eval R2: -289.9641 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.5115 | Eval Loss: 0.4160 | Eval R2: -197.1376 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.3386 | Eval Loss: 0.2606 | Eval R2: -120.6699 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.2026 | Eval Loss: 0.1473 | Eval R2: -65.1904 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1107 | Eval Loss: 0.0784 | Eval R2: -31.8886 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0590 | Eval Loss: 0.0436 | Eval R2: -15.2971 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0344 | Eval Loss: 0.0285 | Eval R2: -8.4001 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0242 | Eval Loss: 0.0226 | Eval R2: -5.7940 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0202 | Eval Loss: 0.0202 | Eval R2: -4.8827 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0185 | Eval Loss: 0.0192 | Eval R2: -4.5099 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0175 | Eval Loss: 0.0186 | Eval R2: -4.3216 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0168 | Eval Loss: 0.0180 | Eval R2: -4.1258 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0162 | Eval Loss: 0.0177 | Eval R2: -4.0136 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0157 | Eval Loss: 0.0173 | Eval R2: -3.8065 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0152 | Eval Loss: 0.0171 | Eval R2: -3.7829 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0148 | Eval Loss: 0.0168 | Eval R2: -3.6106 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0144 | Eval Loss: 0.0169 | Eval R2: -3.7439 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0141 | Eval Loss: 0.0167 | Eval R2: -3.6046 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0138 | Eval Loss: 0.0167 | Eval R2: -3.6249 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0136 | Eval Loss: 0.0167 | Eval R2: -3.6548 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0133 | Eval Loss: 0.0166 | Eval R2: -3.5606 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0131 | Eval Loss: 0.0163 | Eval R2: -3.4311 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.010974, test R2 score: -2.794078

==================== GUARDANDO RESULTADOS ===================

         Modelo  ... Loss_final
0          LSTM  ...   0.008689
1     MPNN_LSTM  ...   0.059339
2  LSTM_NOBATCH  ...   0.009110
3   DyGrEncoder  ...   0.006838
4         AGCRN  ...   0.009918
5     EvolveGCN  ...   0.082644
6         DCRNN  ...   0.008213
7        MSTGCN  ...   0.009062
8         MTGNN  ...   0.013109

[9 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.013108965009450912, 'r2_eval_final': -3.4310736656188965, 'loss_eval_final': 0.01632133685052395, 'r2_test': -2.794077732303806, 'loss_test': 0.010973984375596046, 'loss_nodes': [[0.00669148750603199, 0.01374356634914875, 0.004632624797523022, 0.015472698956727982, 0.007088857237249613, 0.005746231414377689, 0.005923193879425526, 0.006573155988007784, 0.011466284282505512, 0.00889382790774107, 0.007904422469437122, 0.00784431304782629, 0.010638246312737465, 0.017678137868642807, 0.013464581221342087, 0.010740021243691444, 0.014574715867638588, 0.016139641404151917, 0.02237141691148281, 0.011892221868038177]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.007 MB of 0.007 MB uploadedwandb: / 0.007 MB of 0.007 MB uploadedwandb: - 0.015 MB of 0.015 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01311
wandb: loss_eval 0.01632
wandb: loss_test 0.01097
wandb:   r2_eval -3.43107
wandb:   r2_test -2.79408
wandb: 
wandb: ðŸš€ View run elated-plant-224 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/ackfwpc3
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_081951-ackfwpc3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  2%|â–         | 1/50 [52:02<42:29:55, 3122.36s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240718_091153-jt3gemil
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vocal-plasma-225
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/jt3gemil

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9576 | Eval Loss: 0.6558 | Eval R2: -305.7206 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5341 | Eval Loss: 0.4065 | Eval R2: -183.1376 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3217 | Eval Loss: 0.2332 | Eval R2: -99.3493 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1732 | Eval Loss: 0.1138 | Eval R2: -43.6700 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0822 | Eval Loss: 0.0549 | Eval R2: -17.0165 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0463 | Eval Loss: 0.0393 | Eval R2: -10.9698 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0370 | Eval Loss: 0.0341 | Eval R2: -9.3842 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0328 | Eval Loss: 0.0309 | Eval R2: -8.3425 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0303 | Eval Loss: 0.0290 | Eval R2: -7.6477 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0284 | Eval Loss: 0.0273 | Eval R2: -7.1463 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0267 | Eval Loss: 0.0257 | Eval R2: -6.6412 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0250 | Eval Loss: 0.0242 | Eval R2: -6.3054 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0233 | Eval Loss: 0.0229 | Eval R2: -5.9424 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0219 | Eval Loss: 0.0217 | Eval R2: -5.5869 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0205 | Eval Loss: 0.0206 | Eval R2: -5.2743 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0192 | Eval Loss: 0.0192 | Eval R2: -4.8662 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0176 | Eval Loss: 0.0178 | Eval R2: -4.3373 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0167 | Eval Loss: 0.0171 | Eval R2: -4.1125 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0160 | Eval Loss: 0.0166 | Eval R2: -3.9100 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0153 | Eval Loss: 0.0161 | Eval R2: -3.7218 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0148 | Eval Loss: 0.0157 | Eval R2: -3.5667 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0144 | Eval Loss: 0.0154 | Eval R2: -3.4414 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0140 | Eval Loss: 0.0151 | Eval R2: -3.3155 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0136 | Eval Loss: 0.0148 | Eval R2: -3.1854 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0133 | Eval Loss: 0.0146 | Eval R2: -3.0730 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0130 | Eval Loss: 0.0144 | Eval R2: -3.0073 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0127 | Eval Loss: 0.0142 | Eval R2: -2.9175 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.009371, test R2 score: -2.196727

==================== GUARDANDO RESULTADOS ===================

         Modelo  ... Loss_final
0          LSTM  ...   0.008689
1     MPNN_LSTM  ...   0.059339
2  LSTM_NOBATCH  ...   0.009110
3   DyGrEncoder  ...   0.006838
4         AGCRN  ...   0.009918
5     EvolveGCN  ...   0.082644
6         DCRNN  ...   0.008213
7        MSTGCN  ...   0.009062
8         MTGNN  ...   0.013109
9         MTGNN  ...   0.012725

[10 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.012724850326776505, 'r2_eval_final': -2.9174623489379883, 'loss_eval_final': 0.014203147031366825, 'r2_test': -2.1967269639988554, 'loss_test': 0.009370974265038967, 'loss_nodes': [[0.007497584912925959, 0.006264840252697468, 0.0059365807101130486, 0.006425988860428333, 0.012436261400580406, 0.0032451876904815435, 0.008940329775214195, 0.007670043036341667, 0.006211771164089441, 0.006123312283307314, 0.0071707069873809814, 0.007166113704442978, 0.01993398927152157, 0.011295082978904247, 0.009319211356341839, 0.009871809743344784, 0.01085343211889267, 0.01794670708477497, 0.010805564932525158, 0.012304956093430519]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01272
wandb: loss_eval 0.0142
wandb: loss_test 0.00937
wandb:   r2_eval -2.91746
wandb:   r2_test -2.19673
wandb: 
wandb: ðŸš€ View run vocal-plasma-225 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/jt3gemil
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_091153-jt3gemil/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  4%|â–         | 2/50 [27:00:00<755:00:43, 56625.90s/it]Entrenando modelo con gcn_depth=2, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240719_111951-195w5fun
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fragrant-river-226
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/195w5fun

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.0612 | Eval Loss: 0.7064 | Eval R2: -334.4062 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4594 | Eval Loss: 0.2101 | Eval R2: -89.0342 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.1148 | Eval Loss: 0.0582 | Eval R2: -15.2584 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0538 | Eval Loss: 0.0434 | Eval R2: -11.8803 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0397 | Eval Loss: 0.0315 | Eval R2: -9.1456 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0296 | Eval Loss: 0.0239 | Eval R2: -6.6803 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0235 | Eval Loss: 0.0206 | Eval R2: -5.3971 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0204 | Eval Loss: 0.0186 | Eval R2: -4.7229 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0184 | Eval Loss: 0.0171 | Eval R2: -4.2736 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0167 | Eval Loss: 0.0160 | Eval R2: -3.8675 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0156 | Eval Loss: 0.0152 | Eval R2: -3.6055 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0146 | Eval Loss: 0.0145 | Eval R2: -3.3904 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0139 | Eval Loss: 0.0141 | Eval R2: -3.2371 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0133 | Eval Loss: 0.0137 | Eval R2: -3.0475 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0128 | Eval Loss: 0.0134 | Eval R2: -2.8956 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0125 | Eval Loss: 0.0132 | Eval R2: -2.8056 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0121 | Eval Loss: 0.0129 | Eval R2: -2.6247 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0118 | Eval Loss: 0.0128 | Eval R2: -2.5421 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0115 | Eval Loss: 0.0127 | Eval R2: -2.4721 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0113 | Eval Loss: 0.0125 | Eval R2: -2.3832 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0110 | Eval Loss: 0.0126 | Eval R2: -2.3866 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0108 | Eval Loss: 0.0125 | Eval R2: -2.3431 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0107 | Eval Loss: 0.0125 | Eval R2: -2.3317 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0105 | Eval Loss: 0.0124 | Eval R2: -2.2782 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0104 | Eval Loss: 0.0123 | Eval R2: -2.2441 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0102 | Eval Loss: 0.0124 | Eval R2: -2.2275 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0101 | Eval Loss: 0.0124 | Eval R2: -2.2600 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007361, test R2 score: -1.607198

==================== GUARDANDO RESULTADOS ===================

          Modelo  ... Loss_final
0           LSTM  ...   0.008689
1      MPNN_LSTM  ...   0.059339
2   LSTM_NOBATCH  ...   0.009110
3    DyGrEncoder  ...   0.006838
4          AGCRN  ...   0.009918
5      EvolveGCN  ...   0.082644
6          DCRNN  ...   0.008213
7         MSTGCN  ...   0.009062
8          MTGNN  ...   0.013109
9          MTGNN  ...   0.012725
10         MTGNN  ...   0.010076

[11 rows x 9 columns]

==================== RESULTADOS GUARDADOS ===================

Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.01007605530321598, 'r2_eval_final': -2.2600436210632324, 'loss_eval_final': 0.012398923747241497, 'r2_test': -1.6071976850283323, 'loss_test': 0.0073609668761491776, 'loss_nodes': [[0.003069500671699643, 0.005607121624052525, 0.0035393000580370426, 0.0055349403992295265, 0.005572017747908831, 0.00344365113414824, 0.00870905164629221, 0.005974257364869118, 0.006441014818847179, 0.006325206719338894, 0.0053663672879338264, 0.006855480372905731, 0.009955612011253834, 0.010336615145206451, 0.00873962789773941, 0.007420430425554514, 0.010455566458404064, 0.011817733757197857, 0.009736908599734306, 0.012318911030888557]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01008
wandb: loss_eval 0.0124
wandb: loss_test 0.00736
wandb:   r2_eval -2.26004
wandb:   r2_test -1.6072
wandb: 
wandb: ðŸš€ View run fragrant-river-226 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/195w5fun
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_111951-195w5fun/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  6%|â–Œ         | 3/50 [52:28:51<948:33:19, 72655.30s/it]Entrenando modelo con gcn_depth=2, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_124841-ja2wgb86
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-serenity-227
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/ja2wgb86

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.0176 | Eval Loss: 0.6068 | Eval R2: -285.1144 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3517 | Eval Loss: 0.1304 | Eval R2: -50.4648 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0808 | Eval Loss: 0.0494 | Eval R2: -14.2003 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0496 | Eval Loss: 0.0387 | Eval R2: -10.3076 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0416 | Eval Loss: 0.0334 | Eval R2: -8.7889 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0365 | Eval Loss: 0.0292 | Eval R2: -7.7001 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0322 | Eval Loss: 0.0254 | Eval R2: -6.5960 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0283 | Eval Loss: 0.0226 | Eval R2: -5.7637 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0258 | Eval Loss: 0.0212 | Eval R2: -5.3357 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0238 | Eval Loss: 0.0201 | Eval R2: -4.9959 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0219 | Eval Loss: 0.0190 | Eval R2: -4.7072 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0206 | Eval Loss: 0.0180 | Eval R2: -4.4216 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0192 | Eval Loss: 0.0173 | Eval R2: -4.2233 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0181 | Eval Loss: 0.0166 | Eval R2: -3.9779 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0172 | Eval Loss: 0.0161 | Eval R2: -3.8407 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0163 | Eval Loss: 0.0157 | Eval R2: -3.7371 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0153 | Eval Loss: 0.0150 | Eval R2: -3.4489 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0145 | Eval Loss: 0.0146 | Eval R2: -3.2935 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0138 | Eval Loss: 0.0144 | Eval R2: -3.2261 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0130 | Eval Loss: 0.0142 | Eval R2: -3.0826 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0126 | Eval Loss: 0.0140 | Eval R2: -3.0325 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0121 | Eval Loss: 0.0138 | Eval R2: -2.9164 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0118 | Eval Loss: 0.0138 | Eval R2: -2.8834 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0114 | Eval Loss: 0.0136 | Eval R2: -2.7725 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0111 | Eval Loss: 0.0135 | Eval R2: -2.7125 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0108 | Eval Loss: 0.0135 | Eval R2: -2.6987 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0105 | Eval Loss: 0.0133 | Eval R2: -2.6268 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008180, test R2 score: -2.021204
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010525071062147617, 'r2_eval_final': -2.626753330230713, 'loss_eval_final': 0.013346265070140362, 'r2_test': -2.0212043188670683, 'loss_test': 0.008179514668881893, 'loss_nodes': [[0.0027522889431566, 0.006458092946559191, 0.004273825325071812, 0.005436053965240717, 0.006105293519794941, 0.0035414935555309057, 0.007351716049015522, 0.008111339993774891, 0.008540729992091656, 0.006490243133157492, 0.005913040600717068, 0.007375409360975027, 0.011630704626441002, 0.012972538359463215, 0.009826569817960262, 0.009777584113180637, 0.011541439220309258, 0.01210852898657322, 0.011164695955812931, 0.01221870444715023]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01053
wandb: loss_eval 0.01335
wandb: loss_test 0.00818
wandb:   r2_eval -2.62675
wandb:   r2_test -2.0212
wandb: 
wandb: ðŸš€ View run ancient-serenity-227 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/ja2wgb86
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_124841-ja2wgb86/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
  8%|â–Š         | 4/50 [53:05:22<572:55:07, 44837.12s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_132513-mfm1irc2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dulcet-yogurt-228
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/mfm1irc2

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8350 | Eval Loss: 0.5227 | Eval R2: -241.5250 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3613 | Eval Loss: 0.1938 | Eval R2: -80.3234 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.1183 | Eval Loss: 0.0599 | Eval R2: -18.7929 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0501 | Eval Loss: 0.0374 | Eval R2: -10.7518 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0368 | Eval Loss: 0.0303 | Eval R2: -8.0450 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0315 | Eval Loss: 0.0269 | Eval R2: -7.0632 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0281 | Eval Loss: 0.0240 | Eval R2: -6.1744 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0251 | Eval Loss: 0.0216 | Eval R2: -5.4089 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0226 | Eval Loss: 0.0196 | Eval R2: -4.7582 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0204 | Eval Loss: 0.0180 | Eval R2: -4.3183 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0186 | Eval Loss: 0.0169 | Eval R2: -4.0137 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0172 | Eval Loss: 0.0161 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0160 | Eval Loss: 0.0154 | Eval R2: -3.6045 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0151 | Eval Loss: 0.0149 | Eval R2: -3.4714 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0144 | Eval Loss: 0.0147 | Eval R2: -3.4171 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0138 | Eval Loss: 0.0144 | Eval R2: -3.3152 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0132 | Eval Loss: 0.0143 | Eval R2: -3.2511 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0128 | Eval Loss: 0.0140 | Eval R2: -3.1241 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0124 | Eval Loss: 0.0140 | Eval R2: -3.1432 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0121 | Eval Loss: 0.0138 | Eval R2: -3.0493 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0117 | Eval Loss: 0.0136 | Eval R2: -2.9750 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0115 | Eval Loss: 0.0136 | Eval R2: -2.9381 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0113 | Eval Loss: 0.0136 | Eval R2: -2.9415 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0111 | Eval Loss: 0.0134 | Eval R2: -2.8623 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0110 | Eval Loss: 0.0134 | Eval R2: -2.8343 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0107 | Eval Loss: 0.0134 | Eval R2: -2.8390 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0106 | Eval Loss: 0.0134 | Eval R2: -2.8392 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008181, test R2 score: -2.080456
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010556205175817013, 'r2_eval_final': -2.8392465114593506, 'loss_eval_final': 0.013399410992860794, 'r2_test': -2.0804556782008263, 'loss_test': 0.008181365206837654, 'loss_nodes': [[0.005279895383864641, 0.006477587856352329, 0.005530509632080793, 0.00612683268263936, 0.007255478296428919, 0.004017912317067385, 0.0063543422147631645, 0.007113167084753513, 0.006777789909392595, 0.0062931859865784645, 0.005634566303342581, 0.007324103266000748, 0.012140071950852871, 0.012498297728598118, 0.010620460845530033, 0.008985335938632488, 0.010516461916267872, 0.011718152090907097, 0.010445611551404, 0.012517549097537994]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01056
wandb: loss_eval 0.0134
wandb: loss_test 0.00818
wandb:   r2_eval -2.83925
wandb:   r2_test -2.08046
wandb: 
wandb: ðŸš€ View run dulcet-yogurt-228 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/mfm1irc2
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_132513-mfm1irc2/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 10%|â–ˆ         | 5/50 [53:50:52<370:39:33, 29652.74s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_141043-6clmd2x3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run deep-paper-229
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6clmd2x3

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.7717 | Eval Loss: 1.2269 | Eval R2: -598.9273 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.0830 | Eval Loss: 0.9457 | Eval R2: -454.2734 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.8282 | Eval Loss: 0.7118 | Eval R2: -334.7273 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6108 | Eval Loss: 0.5109 | Eval R2: -232.8490 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4292 | Eval Loss: 0.3504 | Eval R2: -152.0409 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2922 | Eval Loss: 0.2380 | Eval R2: -95.8941 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2018 | Eval Loss: 0.1684 | Eval R2: -61.4633 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1478 | Eval Loss: 0.1283 | Eval R2: -41.7370 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1173 | Eval Loss: 0.1062 | Eval R2: -30.8901 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1008 | Eval Loss: 0.0945 | Eval R2: -25.2079 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0922 | Eval Loss: 0.0886 | Eval R2: -22.3995 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0881 | Eval Loss: 0.0858 | Eval R2: -21.0933 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0861 | Eval Loss: 0.0845 | Eval R2: -20.5230 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0852 | Eval Loss: 0.0840 | Eval R2: -20.2895 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0848 | Eval Loss: 0.0837 | Eval R2: -20.2003 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1690 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1567 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1560 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1555 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1547 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1538 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1527 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1515 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1502 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1490 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1478 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.078585, test R2 score: -19.374039
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.08461064845323563, 'r2_eval_final': -20.14777374267578, 'loss_eval_final': 0.08359131962060928, 'r2_test': -19.37403871744218, 'loss_test': 0.07858463376760483, 'loss_nodes': [[0.07591517269611359, 0.07774806767702103, 0.07972442358732224, 0.07712581753730774, 0.08007319271564484, 0.07588586956262589, 0.07660738378763199, 0.0799608901143074, 0.07667720317840576, 0.0808873102068901, 0.07683776319026947, 0.07735767215490341, 0.08243271708488464, 0.077853262424469, 0.08088311553001404, 0.07786070555448532, 0.07664046436548233, 0.08135152608156204, 0.0774589255452156, 0.08241135627031326]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.002 MB of 0.002 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.08461
wandb: loss_eval 0.08359
wandb: loss_test 0.07858
wandb:   r2_eval -20.14777
wandb:   r2_test -19.37404
wandb: 
wandb: ðŸš€ View run deep-paper-229 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6clmd2x3
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_141043-6clmd2x3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 12%|â–ˆâ–        | 6/50 [54:17:22<245:48:18, 20111.34s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_143712-hu01ad2d
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run fresh-deluge-230
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/hu01ad2d

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7649 | Eval Loss: 0.4238 | Eval R2: -194.5570 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2605 | Eval Loss: 0.1203 | Eval R2: -44.8158 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0857 | Eval Loss: 0.0470 | Eval R2: -11.8465 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0480 | Eval Loss: 0.0348 | Eval R2: -8.9346 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0361 | Eval Loss: 0.0270 | Eval R2: -7.2971 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0286 | Eval Loss: 0.0216 | Eval R2: -5.6918 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0238 | Eval Loss: 0.0191 | Eval R2: -4.9246 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0207 | Eval Loss: 0.0175 | Eval R2: -4.3599 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0184 | Eval Loss: 0.0165 | Eval R2: -3.9941 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0168 | Eval Loss: 0.0158 | Eval R2: -3.7329 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0155 | Eval Loss: 0.0150 | Eval R2: -3.4255 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0146 | Eval Loss: 0.0145 | Eval R2: -3.1997 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0139 | Eval Loss: 0.0141 | Eval R2: -3.0198 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0134 | Eval Loss: 0.0137 | Eval R2: -2.8655 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0128 | Eval Loss: 0.0135 | Eval R2: -2.7686 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0124 | Eval Loss: 0.0132 | Eval R2: -2.6558 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0120 | Eval Loss: 0.0131 | Eval R2: -2.6119 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0117 | Eval Loss: 0.0131 | Eval R2: -2.6070 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0114 | Eval Loss: 0.0129 | Eval R2: -2.5201 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0112 | Eval Loss: 0.0129 | Eval R2: -2.5036 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0109 | Eval Loss: 0.0128 | Eval R2: -2.4298 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0108 | Eval Loss: 0.0127 | Eval R2: -2.4138 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0105 | Eval Loss: 0.0126 | Eval R2: -2.3638 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0104 | Eval Loss: 0.0126 | Eval R2: -2.3343 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0103 | Eval Loss: 0.0126 | Eval R2: -2.3662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0101 | Eval Loss: 0.0125 | Eval R2: -2.3065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0099 | Eval Loss: 0.0124 | Eval R2: -2.2481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007395, test R2 score: -1.603510
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009914983995258808, 'r2_eval_final': -2.248145580291748, 'loss_eval_final': 0.012391580268740654, 'r2_test': -1.6035095022572465, 'loss_test': 0.007394607178866863, 'loss_nodes': [[0.0032892676535993814, 0.005834546871483326, 0.003906643949449062, 0.005043874029070139, 0.006223712582141161, 0.003227456007152796, 0.005535629577934742, 0.006417362950742245, 0.005933465901762247, 0.006218669470399618, 0.005112646613270044, 0.006893838755786419, 0.010543531738221645, 0.011527103371918201, 0.00935370847582817, 0.00777405546978116, 0.010092121548950672, 0.011839871294796467, 0.011341477744281292, 0.01178316306322813]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00991
wandb: loss_eval 0.01239
wandb: loss_test 0.00739
wandb:   r2_eval -2.24815
wandb:   r2_test -1.60351
wandb: 
wandb: ðŸš€ View run fresh-deluge-230 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/hu01ad2d
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_143712-hu01ad2d/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 14%|â–ˆâ–        | 7/50 [55:00:17<171:44:30, 14378.38s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_152008-sh2ap705
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run true-meadow-231
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/sh2ap705

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.7717 | Eval Loss: 1.2269 | Eval R2: -598.9273 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.0830 | Eval Loss: 0.9457 | Eval R2: -454.2734 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.8282 | Eval Loss: 0.7118 | Eval R2: -334.7273 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6108 | Eval Loss: 0.5109 | Eval R2: -232.8490 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4292 | Eval Loss: 0.3504 | Eval R2: -152.0409 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2922 | Eval Loss: 0.2380 | Eval R2: -95.8941 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2018 | Eval Loss: 0.1684 | Eval R2: -61.4633 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1478 | Eval Loss: 0.1283 | Eval R2: -41.7370 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1173 | Eval Loss: 0.1062 | Eval R2: -30.8901 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1008 | Eval Loss: 0.0945 | Eval R2: -25.2079 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0922 | Eval Loss: 0.0886 | Eval R2: -22.3995 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0881 | Eval Loss: 0.0858 | Eval R2: -21.0933 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0861 | Eval Loss: 0.0845 | Eval R2: -20.5230 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0852 | Eval Loss: 0.0840 | Eval R2: -20.2895 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0848 | Eval Loss: 0.0837 | Eval R2: -20.2003 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1690 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1567 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1560 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1555 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1547 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1538 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1527 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1515 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1502 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1490 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1478 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.078585, test R2 score: -19.374039
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.08461064845323563, 'r2_eval_final': -20.14777374267578, 'loss_eval_final': 0.08359131962060928, 'r2_test': -19.37403871744218, 'loss_test': 0.07858463376760483, 'loss_nodes': [[0.07591517269611359, 0.07774806767702103, 0.07972442358732224, 0.07712581753730774, 0.08007319271564484, 0.07588586956262589, 0.07660738378763199, 0.0799608901143074, 0.07667720317840576, 0.0808873102068901, 0.07683776319026947, 0.07735767215490341, 0.08243271708488464, 0.077853262424469, 0.08088311553001404, 0.07786070555448532, 0.07664046436548233, 0.08135152608156204, 0.0774589255452156, 0.08241135627031326]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.08461
wandb: loss_eval 0.08359
wandb: loss_test 0.07858
wandb:   r2_eval -20.14777
wandb:   r2_test -19.37404
wandb: 
wandb: ðŸš€ View run true-meadow-231 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/sh2ap705
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_152008-sh2ap705/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 16%|â–ˆâ–Œ        | 8/50 [55:27:06<120:19:13, 10313.19s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_154657-viwg7bcu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ancient-thunder-232
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/viwg7bcu

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7649 | Eval Loss: 0.4238 | Eval R2: -194.5570 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2605 | Eval Loss: 0.1203 | Eval R2: -44.8158 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0857 | Eval Loss: 0.0470 | Eval R2: -11.8465 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0480 | Eval Loss: 0.0348 | Eval R2: -8.9346 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0361 | Eval Loss: 0.0270 | Eval R2: -7.2971 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0286 | Eval Loss: 0.0216 | Eval R2: -5.6918 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0238 | Eval Loss: 0.0191 | Eval R2: -4.9246 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0207 | Eval Loss: 0.0175 | Eval R2: -4.3599 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0184 | Eval Loss: 0.0165 | Eval R2: -3.9941 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0168 | Eval Loss: 0.0158 | Eval R2: -3.7329 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0155 | Eval Loss: 0.0150 | Eval R2: -3.4255 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0146 | Eval Loss: 0.0145 | Eval R2: -3.1997 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0139 | Eval Loss: 0.0141 | Eval R2: -3.0198 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0134 | Eval Loss: 0.0137 | Eval R2: -2.8655 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0128 | Eval Loss: 0.0135 | Eval R2: -2.7686 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0124 | Eval Loss: 0.0132 | Eval R2: -2.6558 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0120 | Eval Loss: 0.0131 | Eval R2: -2.6119 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0117 | Eval Loss: 0.0131 | Eval R2: -2.6070 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0114 | Eval Loss: 0.0129 | Eval R2: -2.5201 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0112 | Eval Loss: 0.0129 | Eval R2: -2.5036 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0109 | Eval Loss: 0.0128 | Eval R2: -2.4298 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0108 | Eval Loss: 0.0127 | Eval R2: -2.4138 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0105 | Eval Loss: 0.0126 | Eval R2: -2.3638 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0104 | Eval Loss: 0.0126 | Eval R2: -2.3343 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0103 | Eval Loss: 0.0126 | Eval R2: -2.3662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0101 | Eval Loss: 0.0125 | Eval R2: -2.3065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0099 | Eval Loss: 0.0124 | Eval R2: -2.2481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007395, test R2 score: -1.603510
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009914983995258808, 'r2_eval_final': -2.248145580291748, 'loss_eval_final': 0.012391580268740654, 'r2_test': -1.6035095022572465, 'loss_test': 0.007394607178866863, 'loss_nodes': [[0.0032892676535993814, 0.005834546871483326, 0.003906643949449062, 0.005043874029070139, 0.006223712582141161, 0.003227456007152796, 0.005535629577934742, 0.006417362950742245, 0.005933465901762247, 0.006218669470399618, 0.005112646613270044, 0.006893838755786419, 0.010543531738221645, 0.011527103371918201, 0.00935370847582817, 0.00777405546978116, 0.010092121548950672, 0.011839871294796467, 0.011341477744281292, 0.01178316306322813]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00991
wandb: loss_eval 0.01239
wandb: loss_test 0.00739
wandb:   r2_eval -2.24815
wandb:   r2_test -1.60351
wandb: 
wandb: ðŸš€ View run ancient-thunder-232 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/viwg7bcu
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_154657-viwg7bcu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 18%|â–ˆâ–Š        | 9/50 [56:09:26<89:46:52, 7883.24s/it]  Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_162917-ied52ug6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run silver-violet-233
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/ied52ug6

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7649 | Eval Loss: 0.4238 | Eval R2: -194.5570 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2605 | Eval Loss: 0.1203 | Eval R2: -44.8158 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0857 | Eval Loss: 0.0470 | Eval R2: -11.8465 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0480 | Eval Loss: 0.0348 | Eval R2: -8.9346 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0361 | Eval Loss: 0.0270 | Eval R2: -7.2971 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0286 | Eval Loss: 0.0216 | Eval R2: -5.6918 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0238 | Eval Loss: 0.0191 | Eval R2: -4.9246 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0207 | Eval Loss: 0.0175 | Eval R2: -4.3599 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0184 | Eval Loss: 0.0165 | Eval R2: -3.9941 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0168 | Eval Loss: 0.0158 | Eval R2: -3.7329 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0155 | Eval Loss: 0.0150 | Eval R2: -3.4255 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0146 | Eval Loss: 0.0145 | Eval R2: -3.1997 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0139 | Eval Loss: 0.0141 | Eval R2: -3.0198 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0134 | Eval Loss: 0.0137 | Eval R2: -2.8655 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0128 | Eval Loss: 0.0135 | Eval R2: -2.7686 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0124 | Eval Loss: 0.0132 | Eval R2: -2.6558 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0120 | Eval Loss: 0.0131 | Eval R2: -2.6119 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0117 | Eval Loss: 0.0131 | Eval R2: -2.6070 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0114 | Eval Loss: 0.0129 | Eval R2: -2.5201 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0112 | Eval Loss: 0.0129 | Eval R2: -2.5036 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0109 | Eval Loss: 0.0128 | Eval R2: -2.4298 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0108 | Eval Loss: 0.0127 | Eval R2: -2.4138 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0105 | Eval Loss: 0.0126 | Eval R2: -2.3638 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0104 | Eval Loss: 0.0126 | Eval R2: -2.3343 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0103 | Eval Loss: 0.0126 | Eval R2: -2.3662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0101 | Eval Loss: 0.0125 | Eval R2: -2.3065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0099 | Eval Loss: 0.0124 | Eval R2: -2.2481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007395, test R2 score: -1.603510
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009914983995258808, 'r2_eval_final': -2.248145580291748, 'loss_eval_final': 0.012391580268740654, 'r2_test': -1.6035095022572465, 'loss_test': 0.007394607178866863, 'loss_nodes': [[0.0032892676535993814, 0.005834546871483326, 0.003906643949449062, 0.005043874029070139, 0.006223712582141161, 0.003227456007152796, 0.005535629577934742, 0.006417362950742245, 0.005933465901762247, 0.006218669470399618, 0.005112646613270044, 0.006893838755786419, 0.010543531738221645, 0.011527103371918201, 0.00935370847582817, 0.00777405546978116, 0.010092121548950672, 0.011839871294796467, 0.011341477744281292, 0.01178316306322813]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00991
wandb: loss_eval 0.01239
wandb: loss_test 0.00739
wandb:   r2_eval -2.24815
wandb:   r2_test -1.60351
wandb: 
wandb: ðŸš€ View run silver-violet-233 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/ied52ug6
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_162917-ied52ug6/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 20%|â–ˆâ–ˆ        | 10/50 [56:51:00<69:06:18, 6219.45s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_171051-s8fp01p9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run trim-frost-234
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/s8fp01p9

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.6308 | Eval Loss: 0.2145 | Eval R2: -89.0338 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.1160 | Eval Loss: 0.0701 | Eval R2: -18.9044 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0655 | Eval Loss: 0.0505 | Eval R2: -12.4713 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0500 | Eval Loss: 0.0399 | Eval R2: -10.6666 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0399 | Eval Loss: 0.0323 | Eval R2: -9.1167 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0333 | Eval Loss: 0.0272 | Eval R2: -7.8927 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0276 | Eval Loss: 0.0226 | Eval R2: -6.3738 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0228 | Eval Loss: 0.0200 | Eval R2: -6.0544 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0194 | Eval Loss: 0.0184 | Eval R2: -5.2520 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0178 | Eval Loss: 0.0176 | Eval R2: -4.7981 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0165 | Eval Loss: 0.0167 | Eval R2: -4.3817 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0156 | Eval Loss: 0.0161 | Eval R2: -4.1906 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0149 | Eval Loss: 0.0157 | Eval R2: -4.0086 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0142 | Eval Loss: 0.0155 | Eval R2: -3.8955 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0138 | Eval Loss: 0.0153 | Eval R2: -3.8367 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0134 | Eval Loss: 0.0150 | Eval R2: -3.6535 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0130 | Eval Loss: 0.0146 | Eval R2: -3.4749 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0126 | Eval Loss: 0.0143 | Eval R2: -3.3153 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0123 | Eval Loss: 0.0142 | Eval R2: -3.2157 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0119 | Eval Loss: 0.0139 | Eval R2: -3.0810 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0117 | Eval Loss: 0.0138 | Eval R2: -3.0182 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0114 | Eval Loss: 0.0136 | Eval R2: -2.8869 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0112 | Eval Loss: 0.0135 | Eval R2: -2.7956 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0109 | Eval Loss: 0.0133 | Eval R2: -2.6663 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0107 | Eval Loss: 0.0132 | Eval R2: -2.6152 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0104 | Eval Loss: 0.0130 | Eval R2: -2.4921 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0102 | Eval Loss: 0.0129 | Eval R2: -2.4489 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007773, test R2 score: -1.765729
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010246354155242443, 'r2_eval_final': -2.4488985538482666, 'loss_eval_final': 0.012925892136991024, 'r2_test': -1.7657294410627193, 'loss_test': 0.007773292250931263, 'loss_nodes': [[0.003589923959225416, 0.0075398595072329044, 0.004796853754669428, 0.005766149144619703, 0.006876473315060139, 0.0037880747113376856, 0.005734752397984266, 0.006346958689391613, 0.00640458008274436, 0.006567814853042364, 0.005453299265354872, 0.006907680071890354, 0.011046354658901691, 0.011265811510384083, 0.009223281405866146, 0.008369595743715763, 0.010506211780011654, 0.012089943513274193, 0.01152133196592331, 0.011670912615954876]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01025
wandb: loss_eval 0.01293
wandb: loss_test 0.00777
wandb:   r2_eval -2.4489
wandb:   r2_test -1.76573
wandb: 
wandb: ðŸš€ View run trim-frost-234 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/s8fp01p9
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_171051-s8fp01p9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 22%|â–ˆâ–ˆâ–       | 11/50 [57:42:25<56:59:10, 5260.26s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_180216-8j4fw9eu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run rich-firefly-235
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/8j4fw9eu

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9220 | Eval Loss: 0.6942 | Eval R2: -325.6749 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5815 | Eval Loss: 0.4716 | Eval R2: -212.9295 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3834 | Eval Loss: 0.2998 | Eval R2: -126.8085 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2396 | Eval Loss: 0.1855 | Eval R2: -69.8607 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1531 | Eval Loss: 0.1250 | Eval R2: -39.9774 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1108 | Eval Loss: 0.0982 | Eval R2: -26.9770 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0934 | Eval Loss: 0.0882 | Eval R2: -22.2153 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0873 | Eval Loss: 0.0849 | Eval R2: -20.7211 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0854 | Eval Loss: 0.0840 | Eval R2: -20.2938 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0848 | Eval Loss: 0.0837 | Eval R2: -20.1807 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1543 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1487 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1466 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1446 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1424 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1401 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1377 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1354 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1331 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1309 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1287 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1266 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1245 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1225 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1205 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1185 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1166 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.078675, test R2 score: -19.379246
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.0846618264913559, 'r2_eval_final': -20.116565704345703, 'loss_eval_final': 0.08362316340208054, 'r2_test': -19.379245624934395, 'loss_test': 0.07867515832185745, 'loss_nodes': [[0.0761028602719307, 0.0778169184923172, 0.08001460880041122, 0.07711181789636612, 0.08031108975410461, 0.076168954372406, 0.07632443308830261, 0.07992075383663177, 0.07693438231945038, 0.08098054677248001, 0.07703667134046555, 0.07755929231643677, 0.08260826766490936, 0.07771733403205872, 0.08118420094251633, 0.07801114767789841, 0.07654562592506409, 0.0813874751329422, 0.07730834186077118, 0.08245844393968582]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.08466
wandb: loss_eval 0.08362
wandb: loss_test 0.07868
wandb:   r2_eval -20.11657
wandb:   r2_test -19.37925
wandb: 
wandb: ðŸš€ View run rich-firefly-235 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/8j4fw9eu
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_180216-8j4fw9eu/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 24%|â–ˆâ–ˆâ–       | 12/50 [58:21:30<46:09:42, 4373.22s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_184120-qceqayd4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run golden-deluge-236
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/qceqayd4

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.4065 | Eval Loss: 1.1637 | Eval R2: -565.6339 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.0012 | Eval Loss: 0.8303 | Eval R2: -395.7083 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.6860 | Eval Loss: 0.5465 | Eval R2: -253.5774 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4610 | Eval Loss: 0.3903 | Eval R2: -176.6908 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3377 | Eval Loss: 0.2877 | Eval R2: -127.6742 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2498 | Eval Loss: 0.2139 | Eval R2: -93.6881 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1857 | Eval Loss: 0.1590 | Eval R2: -68.1120 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1371 | Eval Loss: 0.1162 | Eval R2: -48.3804 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0991 | Eval Loss: 0.0829 | Eval R2: -33.1191 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0703 | Eval Loss: 0.0596 | Eval R2: -22.5267 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0504 | Eval Loss: 0.0442 | Eval R2: -15.5329 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0375 | Eval Loss: 0.0343 | Eval R2: -10.9873 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0297 | Eval Loss: 0.0289 | Eval R2: -8.7325 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0251 | Eval Loss: 0.0261 | Eval R2: -8.0643 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0222 | Eval Loss: 0.0247 | Eval R2: -7.5022 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0206 | Eval Loss: 0.0228 | Eval R2: -6.6275 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0192 | Eval Loss: 0.0205 | Eval R2: -5.5858 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0182 | Eval Loss: 0.0195 | Eval R2: -5.2164 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0176 | Eval Loss: 0.0192 | Eval R2: -5.1136 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0171 | Eval Loss: 0.0186 | Eval R2: -4.7982 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0166 | Eval Loss: 0.0185 | Eval R2: -4.7830 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0162 | Eval Loss: 0.0183 | Eval R2: -4.6591 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0158 | Eval Loss: 0.0182 | Eval R2: -4.5872 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0154 | Eval Loss: 0.0178 | Eval R2: -4.4217 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0151 | Eval Loss: 0.0177 | Eval R2: -4.3644 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0148 | Eval Loss: 0.0174 | Eval R2: -4.1962 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0145 | Eval Loss: 0.0174 | Eval R2: -4.1499 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.012110, test R2 score: -3.368352
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.014493081718683243, 'r2_eval_final': -4.149903297424316, 'loss_eval_final': 0.01736096478998661, 'r2_test': -3.3683521356696344, 'loss_test': 0.012109759263694286, 'loss_nodes': [[0.005930372979491949, 0.007305111736059189, 0.019840093329548836, 0.007029859349131584, 0.007938220165669918, 0.004565889481455088, 0.007227170746773481, 0.009348908439278603, 0.023704860359430313, 0.007429981138557196, 0.010671406984329224, 0.008247052319347858, 0.01750965230166912, 0.014967391267418861, 0.010086762718856335, 0.014960137195885181, 0.011028417386114597, 0.023349441587924957, 0.017463354393839836, 0.01359109953045845]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01449
wandb: loss_eval 0.01736
wandb: loss_test 0.01211
wandb:   r2_eval -4.1499
wandb:   r2_test -3.36835
wandb: 
wandb: ðŸš€ View run golden-deluge-236 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/qceqayd4
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_184120-qceqayd4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 26%|â–ˆâ–ˆâ–Œ       | 13/50 [59:03:59<39:16:06, 3820.71s/it]Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_192350-3ghoc18i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run daily-cloud-237
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/3ghoc18i

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7556 | Eval Loss: 0.5308 | Eval R2: -253.9391 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4295 | Eval Loss: 0.3254 | Eval R2: -152.0872 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2528 | Eval Loss: 0.1743 | Eval R2: -77.3441 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1161 | Eval Loss: 0.0609 | Eval R2: -22.4870 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0369 | Eval Loss: 0.0255 | Eval R2: -6.6145 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0241 | Eval Loss: 0.0234 | Eval R2: -5.8051 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0222 | Eval Loss: 0.0223 | Eval R2: -5.3851 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0209 | Eval Loss: 0.0213 | Eval R2: -5.1436 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0197 | Eval Loss: 0.0202 | Eval R2: -4.8323 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0186 | Eval Loss: 0.0193 | Eval R2: -4.5633 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0176 | Eval Loss: 0.0185 | Eval R2: -4.3394 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0166 | Eval Loss: 0.0178 | Eval R2: -4.1652 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0158 | Eval Loss: 0.0172 | Eval R2: -3.9915 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0151 | Eval Loss: 0.0166 | Eval R2: -3.8279 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0145 | Eval Loss: 0.0162 | Eval R2: -3.6741 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0140 | Eval Loss: 0.0158 | Eval R2: -3.5468 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0136 | Eval Loss: 0.0154 | Eval R2: -3.3937 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0132 | Eval Loss: 0.0151 | Eval R2: -3.2837 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0129 | Eval Loss: 0.0148 | Eval R2: -3.1856 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0126 | Eval Loss: 0.0145 | Eval R2: -3.0976 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0123 | Eval Loss: 0.0143 | Eval R2: -3.0258 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0121 | Eval Loss: 0.0142 | Eval R2: -2.9503 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0119 | Eval Loss: 0.0140 | Eval R2: -2.9056 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0117 | Eval Loss: 0.0139 | Eval R2: -2.8555 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0115 | Eval Loss: 0.0138 | Eval R2: -2.8288 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0113 | Eval Loss: 0.0138 | Eval R2: -2.8009 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0112 | Eval Loss: 0.0138 | Eval R2: -2.8158 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008982, test R2 score: -2.166096
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.011213120073080063, 'r2_eval_final': -2.8157618045806885, 'loss_eval_final': 0.013755099847912788, 'r2_test': -2.1660955917145945, 'loss_test': 0.008982171304523945, 'loss_nodes': [[0.004790971055626869, 0.011955931782722473, 0.004197715315967798, 0.010639150626957417, 0.0067386026494205, 0.0033293701708316803, 0.0068959081545472145, 0.006321327295154333, 0.006698594894260168, 0.006555489730089903, 0.007239785976707935, 0.010292614810168743, 0.011863131076097488, 0.012352446094155312, 0.011036327108740807, 0.008390402421355247, 0.012088966555893421, 0.012845165096223354, 0.011763731017708778, 0.013647832907736301]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01121
wandb: loss_eval 0.01376
wandb: loss_test 0.00898
wandb:   r2_eval -2.81576
wandb:   r2_test -2.1661
wandb: 
wandb: ðŸš€ View run daily-cloud-237 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/3ghoc18i
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_192350-3ghoc18i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 28%|â–ˆâ–ˆâ–Š       | 14/50 [59:53:26<35:37:46, 3562.95s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240720_201317-6ts6g4ap
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run easy-hill-238
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6ts6g4ap

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8973 | Eval Loss: 0.7054 | Eval R2: -332.3318 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.6457 | Eval Loss: 0.5911 | Eval R2: -274.7303 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5418 | Eval Loss: 0.4953 | Eval R2: -226.4248 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4517 | Eval Loss: 0.4097 | Eval R2: -183.1662 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.3715 | Eval Loss: 0.3345 | Eval R2: -145.1687 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3022 | Eval Loss: 0.2707 | Eval R2: -112.9802 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2443 | Eval Loss: 0.2185 | Eval R2: -86.7329 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1979 | Eval Loss: 0.1775 | Eval R2: -66.2160 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1623 | Eval Loss: 0.1470 | Eval R2: -51.0033 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1364 | Eval Loss: 0.1255 | Eval R2: -40.3493 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.1186 | Eval Loss: 0.1110 | Eval R2: -33.2335 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.1067 | Eval Loss: 0.1015 | Eval R2: -28.6006 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0990 | Eval Loss: 0.0952 | Eval R2: -25.5925 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0939 | Eval Loss: 0.0911 | Eval R2: -23.6247 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0906 | Eval Loss: 0.0884 | Eval R2: -22.3336 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0884 | Eval Loss: 0.0866 | Eval R2: -21.4929 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0869 | Eval Loss: 0.0855 | Eval R2: -20.9547 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0860 | Eval Loss: 0.0847 | Eval R2: -20.6180 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0855 | Eval Loss: 0.0843 | Eval R2: -20.4129 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0851 | Eval Loss: 0.0840 | Eval R2: -20.2917 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0849 | Eval Loss: 0.0838 | Eval R2: -20.2225 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0848 | Eval Loss: 0.0837 | Eval R2: -20.1843 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0847 | Eval Loss: 0.0837 | Eval R2: -20.1642 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1542 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1495 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1474 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1465 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.078587, test R2 score: -19.373830
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.084615558385849, 'r2_eval_final': -20.14647102355957, 'loss_eval_final': 0.08359811455011368, 'r2_test': -19.373830056273444, 'loss_test': 0.07858708500862122, 'loss_nodes': [[0.07589973509311676, 0.07795162498950958, 0.07970824837684631, 0.07713501900434494, 0.08012327551841736, 0.07631231099367142, 0.07655633240938187, 0.07993720471858978, 0.07649342715740204, 0.08083833754062653, 0.0769091472029686, 0.07730040699243546, 0.0824306458234787, 0.07769037783145905, 0.0808691531419754, 0.07784664630889893, 0.07652068138122559, 0.08145063370466232, 0.07729903608560562, 0.08246921002864838]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.08462
wandb: loss_eval 0.0836
wandb: loss_test 0.07859
wandb:   r2_eval -20.14647
wandb:   r2_test -19.37383
wandb: 
wandb: ðŸš€ View run easy-hill-238 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6ts6g4ap
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240720_201317-6ts6g4ap/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 30%|â–ˆâ–ˆâ–ˆ       | 15/50 [87:20:49<313:51:56, 32283.32s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240721_234040-6rl31pvw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run leafy-jazz-239
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6rl31pvw

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.2513 | Eval Loss: 0.7919 | Eval R2: -375.4108 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.7003 | Eval Loss: 0.5722 | Eval R2: -268.5297 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.4999 | Eval Loss: 0.4077 | Eval R2: -187.6916 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.3422 | Eval Loss: 0.2616 | Eval R2: -117.0773 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2121 | Eval Loss: 0.1521 | Eval R2: -65.1305 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1211 | Eval Loss: 0.0817 | Eval R2: -32.0221 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0665 | Eval Loss: 0.0442 | Eval R2: -14.7830 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0398 | Eval Loss: 0.0286 | Eval R2: -7.9721 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0292 | Eval Loss: 0.0230 | Eval R2: -5.8278 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0251 | Eval Loss: 0.0208 | Eval R2: -5.1732 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0230 | Eval Loss: 0.0195 | Eval R2: -4.7865 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0215 | Eval Loss: 0.0186 | Eval R2: -4.5382 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0203 | Eval Loss: 0.0179 | Eval R2: -4.3079 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0193 | Eval Loss: 0.0173 | Eval R2: -4.1243 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0186 | Eval Loss: 0.0168 | Eval R2: -4.0098 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0180 | Eval Loss: 0.0165 | Eval R2: -3.8854 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0174 | Eval Loss: 0.0162 | Eval R2: -3.7990 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0169 | Eval Loss: 0.0158 | Eval R2: -3.6702 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0165 | Eval Loss: 0.0156 | Eval R2: -3.6244 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0161 | Eval Loss: 0.0154 | Eval R2: -3.5445 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0157 | Eval Loss: 0.0153 | Eval R2: -3.5218 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0153 | Eval Loss: 0.0152 | Eval R2: -3.5178 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0150 | Eval Loss: 0.0151 | Eval R2: -3.4863 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0147 | Eval Loss: 0.0152 | Eval R2: -3.5359 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0143 | Eval Loss: 0.0151 | Eval R2: -3.4810 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0140 | Eval Loss: 0.0151 | Eval R2: -3.4208 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0138 | Eval Loss: 0.0151 | Eval R2: -3.4162 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.010188, test R2 score: -2.738062
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.013773742131888866, 'r2_eval_final': -3.4161548614501953, 'loss_eval_final': 0.015143376775085926, 'r2_test': -2.7380617165322354, 'loss_test': 0.010187813080847263, 'loss_nodes': [[0.004164711572229862, 0.005966777913272381, 0.006341991480439901, 0.005604973062872887, 0.013158228248357773, 0.009044435806572437, 0.007158982567489147, 0.006760847754776478, 0.008663298562169075, 0.007609844673424959, 0.012184805236756802, 0.009155703708529472, 0.015309443697333336, 0.01388639211654663, 0.014659524895250797, 0.009353676810860634, 0.012832061387598515, 0.017323382198810577, 0.011163393035531044, 0.013413779437541962]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01377
wandb: loss_eval 0.01514
wandb: loss_test 0.01019
wandb:   r2_eval -3.41615
wandb:   r2_test -2.73806
wandb: 
wandb: ðŸš€ View run leafy-jazz-239 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6rl31pvw
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240721_234040-6rl31pvw/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [111:40:14<462:03:04, 48923.08s/it]Entrenando modelo con gcn_depth=3, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240723_000005-rliaq7qc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run comfy-flower-240
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/rliaq7qc

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.0428 | Eval Loss: 0.6493 | Eval R2: -307.1273 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3992 | Eval Loss: 0.1718 | Eval R2: -68.6618 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0976 | Eval Loss: 0.0520 | Eval R2: -13.8112 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0486 | Eval Loss: 0.0380 | Eval R2: -10.4942 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0362 | Eval Loss: 0.0294 | Eval R2: -8.2804 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0295 | Eval Loss: 0.0256 | Eval R2: -7.1486 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0252 | Eval Loss: 0.0224 | Eval R2: -6.1044 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0221 | Eval Loss: 0.0202 | Eval R2: -5.2423 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0202 | Eval Loss: 0.0189 | Eval R2: -4.7447 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0189 | Eval Loss: 0.0180 | Eval R2: -4.3796 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0180 | Eval Loss: 0.0173 | Eval R2: -4.0674 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0172 | Eval Loss: 0.0167 | Eval R2: -3.8734 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0164 | Eval Loss: 0.0162 | Eval R2: -3.6587 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0158 | Eval Loss: 0.0157 | Eval R2: -3.4834 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0152 | Eval Loss: 0.0153 | Eval R2: -3.2950 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0146 | Eval Loss: 0.0149 | Eval R2: -3.1381 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0140 | Eval Loss: 0.0145 | Eval R2: -3.0363 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0135 | Eval Loss: 0.0142 | Eval R2: -2.8989 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0130 | Eval Loss: 0.0141 | Eval R2: -2.8802 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0126 | Eval Loss: 0.0139 | Eval R2: -2.8059 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0122 | Eval Loss: 0.0138 | Eval R2: -2.7759 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0118 | Eval Loss: 0.0135 | Eval R2: -2.5856 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0115 | Eval Loss: 0.0134 | Eval R2: -2.5864 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0112 | Eval Loss: 0.0134 | Eval R2: -2.6032 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0109 | Eval Loss: 0.0135 | Eval R2: -2.6806 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0107 | Eval Loss: 0.0133 | Eval R2: -2.6102 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0104 | Eval Loss: 0.0132 | Eval R2: -2.5280 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008094, test R2 score: -2.012236
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010444528423249722, 'r2_eval_final': -2.5280120372772217, 'loss_eval_final': 0.013177324086427689, 'r2_test': -2.012235522333325, 'loss_test': 0.008093702606856823, 'loss_nodes': [[0.004160269163548946, 0.00743311271071434, 0.004246936179697514, 0.007705026306211948, 0.006901599001139402, 0.0037214148323982954, 0.005998218432068825, 0.006755693815648556, 0.006435602903366089, 0.006307594478130341, 0.005736404098570347, 0.007727403659373522, 0.010703518986701965, 0.011212925426661968, 0.011271256022155285, 0.008504664525389671, 0.011761698871850967, 0.0117381876334548, 0.01261444017291069, 0.010938102379441261]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01044
wandb: loss_eval 0.01318
wandb: loss_test 0.00809
wandb:   r2_eval -2.52801
wandb:   r2_test -2.01224
wandb: 
wandb: ðŸš€ View run comfy-flower-240 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/rliaq7qc
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240723_000005-rliaq7qc/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 34%|â–ˆâ–ˆâ–ˆâ–      | 17/50 [136:42:46<562:06:14, 61320.45s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_010237-6hxt4rg8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run warm-brook-241
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6hxt4rg8

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.1912 | Eval Loss: 0.9564 | Eval R2: -461.2007 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8040 | Eval Loss: 0.6605 | Eval R2: -312.9685 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.5686 | Eval Loss: 0.4756 | Eval R2: -221.7232 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.4079 | Eval Loss: 0.3357 | Eval R2: -153.2284 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.2844 | Eval Loss: 0.2292 | Eval R2: -102.2456 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1913 | Eval Loss: 0.1500 | Eval R2: -64.9132 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.1240 | Eval Loss: 0.0952 | Eval R2: -39.9181 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0787 | Eval Loss: 0.0617 | Eval R2: -24.9031 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0509 | Eval Loss: 0.0424 | Eval R2: -16.1378 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0354 | Eval Loss: 0.0311 | Eval R2: -10.5635 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0275 | Eval Loss: 0.0250 | Eval R2: -7.3696 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0237 | Eval Loss: 0.0225 | Eval R2: -6.2008 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0218 | Eval Loss: 0.0210 | Eval R2: -5.4833 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0207 | Eval Loss: 0.0201 | Eval R2: -5.0539 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0199 | Eval Loss: 0.0195 | Eval R2: -4.8460 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0191 | Eval Loss: 0.0190 | Eval R2: -4.6453 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0184 | Eval Loss: 0.0186 | Eval R2: -4.4572 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0179 | Eval Loss: 0.0183 | Eval R2: -4.3604 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0172 | Eval Loss: 0.0181 | Eval R2: -4.2817 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0168 | Eval Loss: 0.0179 | Eval R2: -4.1708 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0163 | Eval Loss: 0.0176 | Eval R2: -4.0582 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0159 | Eval Loss: 0.0175 | Eval R2: -3.9594 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0155 | Eval Loss: 0.0173 | Eval R2: -3.8607 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0151 | Eval Loss: 0.0171 | Eval R2: -3.7912 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0148 | Eval Loss: 0.0170 | Eval R2: -3.6761 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0144 | Eval Loss: 0.0168 | Eval R2: -3.5921 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0142 | Eval Loss: 0.0167 | Eval R2: -3.5874 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.011124, test R2 score: -2.818955
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.014219686388969421, 'r2_eval_final': -3.5874006748199463, 'loss_eval_final': 0.01674579083919525, 'r2_test': -2.8189551329965417, 'loss_test': 0.011123713105916977, 'loss_nodes': [[0.016771426424384117, 0.006884814240038395, 0.006899693980813026, 0.009242786094546318, 0.011472271755337715, 0.00921299122273922, 0.008030707947909832, 0.0076781208626925945, 0.007414684630930424, 0.01423554215580225, 0.005675069056451321, 0.021081021055579185, 0.011661974713206291, 0.012912494130432606, 0.011940987780690193, 0.009834425523877144, 0.011029099114239216, 0.012088743038475513, 0.016727151349186897, 0.01168027799576521]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.006 MB of 0.009 MB uploadedwandb: | 0.006 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01422
wandb: loss_eval 0.01675
wandb: loss_test 0.01112
wandb:   r2_eval -3.5874
wandb:   r2_test -2.81896
wandb: 
wandb: ðŸš€ View run warm-brook-241 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/6hxt4rg8
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_010237-6hxt4rg8/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 18/50 [137:22:20<387:37:24, 43607.65s/it]Entrenando modelo con gcn_depth=1, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_014210-rryinc97
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run valiant-totem-242
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/rryinc97

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.7717 | Eval Loss: 1.2269 | Eval R2: -598.9273 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 1.0830 | Eval Loss: 0.9457 | Eval R2: -454.2734 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.8282 | Eval Loss: 0.7118 | Eval R2: -334.7273 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6108 | Eval Loss: 0.5109 | Eval R2: -232.8490 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4292 | Eval Loss: 0.3504 | Eval R2: -152.0409 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.2922 | Eval Loss: 0.2380 | Eval R2: -95.8941 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2018 | Eval Loss: 0.1684 | Eval R2: -61.4633 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1478 | Eval Loss: 0.1283 | Eval R2: -41.7370 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1173 | Eval Loss: 0.1062 | Eval R2: -30.8901 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.1008 | Eval Loss: 0.0945 | Eval R2: -25.2079 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0922 | Eval Loss: 0.0886 | Eval R2: -22.3995 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0881 | Eval Loss: 0.0858 | Eval R2: -21.0933 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0861 | Eval Loss: 0.0845 | Eval R2: -20.5230 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0852 | Eval Loss: 0.0840 | Eval R2: -20.2895 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0848 | Eval Loss: 0.0837 | Eval R2: -20.2003 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1690 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1593 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1567 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1560 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1555 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1547 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1538 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1527 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1515 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1502 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1490 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1478 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.078585, test R2 score: -19.374039
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.08461064845323563, 'r2_eval_final': -20.14777374267578, 'loss_eval_final': 0.08359131962060928, 'r2_test': -19.37403871744218, 'loss_test': 0.07858463376760483, 'loss_nodes': [[0.07591517269611359, 0.07774806767702103, 0.07972442358732224, 0.07712581753730774, 0.08007319271564484, 0.07588586956262589, 0.07660738378763199, 0.0799608901143074, 0.07667720317840576, 0.0808873102068901, 0.07683776319026947, 0.07735767215490341, 0.08243271708488464, 0.077853262424469, 0.08088311553001404, 0.07786070555448532, 0.07664046436548233, 0.08135152608156204, 0.0774589255452156, 0.08241135627031326]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.009 MB uploadedwandb: / 0.002 MB of 0.009 MB uploadedwandb: - 0.002 MB of 0.009 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.08461
wandb: loss_eval 0.08359
wandb: loss_test 0.07858
wandb:   r2_eval -20.14777
wandb:   r2_test -19.37404
wandb: 
wandb: ðŸš€ View run valiant-totem-242 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/rryinc97
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_014210-rryinc97/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [137:47:13<266:35:24, 30958.84s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_020703-q1k2x0j9
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wobbly-fog-243
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/q1k2x0j9

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9220 | Eval Loss: 0.6942 | Eval R2: -325.6749 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5815 | Eval Loss: 0.4716 | Eval R2: -212.9295 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3834 | Eval Loss: 0.2998 | Eval R2: -126.8085 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2396 | Eval Loss: 0.1855 | Eval R2: -69.8607 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1531 | Eval Loss: 0.1250 | Eval R2: -39.9774 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1108 | Eval Loss: 0.0982 | Eval R2: -26.9770 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0934 | Eval Loss: 0.0882 | Eval R2: -22.2153 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0873 | Eval Loss: 0.0849 | Eval R2: -20.7211 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0854 | Eval Loss: 0.0840 | Eval R2: -20.2938 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0848 | Eval Loss: 0.0837 | Eval R2: -20.1807 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1543 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1487 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1466 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1446 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1424 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1401 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1377 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1354 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1331 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1309 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1287 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1266 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0846 | Eval Loss: 0.0836 | Eval R2: -20.1245 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1225 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1205 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1185 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0847 | Eval Loss: 0.0836 | Eval R2: -20.1166 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.078675, test R2 score: -19.379246
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.0846618264913559, 'r2_eval_final': -20.116565704345703, 'loss_eval_final': 0.08362316340208054, 'r2_test': -19.379245624934395, 'loss_test': 0.07867515832185745, 'loss_nodes': [[0.0761028602719307, 0.0778169184923172, 0.08001460880041122, 0.07711181789636612, 0.08031108975410461, 0.076168954372406, 0.07632443308830261, 0.07992075383663177, 0.07693438231945038, 0.08098054677248001, 0.07703667134046555, 0.07755929231643677, 0.08260826766490936, 0.07771733403205872, 0.08118420094251633, 0.07801114767789841, 0.07654562592506409, 0.0813874751329422, 0.07730834186077118, 0.08245844393968582]]}
wandb: - 0.002 MB of 0.003 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.08466
wandb: loss_eval 0.08362
wandb: loss_test 0.07868
wandb:   r2_eval -20.11657
wandb:   r2_test -19.37925
wandb: 
wandb: ðŸš€ View run wobbly-fog-243 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/q1k2x0j9
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_020703-q1k2x0j9/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [138:26:26<186:25:08, 22370.29s/it]Entrenando modelo con gcn_depth=1, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_024617-v5h3sob1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run volcanic-bee-244
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/v5h3sob1

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.8350 | Eval Loss: 0.5227 | Eval R2: -241.5250 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.3613 | Eval Loss: 0.1938 | Eval R2: -80.3234 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.1183 | Eval Loss: 0.0599 | Eval R2: -18.7929 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0501 | Eval Loss: 0.0374 | Eval R2: -10.7518 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0368 | Eval Loss: 0.0303 | Eval R2: -8.0450 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0315 | Eval Loss: 0.0269 | Eval R2: -7.0632 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0281 | Eval Loss: 0.0240 | Eval R2: -6.1744 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0251 | Eval Loss: 0.0216 | Eval R2: -5.4089 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0226 | Eval Loss: 0.0196 | Eval R2: -4.7582 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0204 | Eval Loss: 0.0180 | Eval R2: -4.3183 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0186 | Eval Loss: 0.0169 | Eval R2: -4.0137 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0172 | Eval Loss: 0.0161 | Eval R2: -3.8477 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0160 | Eval Loss: 0.0154 | Eval R2: -3.6045 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0151 | Eval Loss: 0.0149 | Eval R2: -3.4714 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0144 | Eval Loss: 0.0147 | Eval R2: -3.4171 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0138 | Eval Loss: 0.0144 | Eval R2: -3.3152 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0132 | Eval Loss: 0.0143 | Eval R2: -3.2511 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0128 | Eval Loss: 0.0140 | Eval R2: -3.1241 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0124 | Eval Loss: 0.0140 | Eval R2: -3.1432 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0121 | Eval Loss: 0.0138 | Eval R2: -3.0493 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0117 | Eval Loss: 0.0136 | Eval R2: -2.9750 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0115 | Eval Loss: 0.0136 | Eval R2: -2.9381 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0113 | Eval Loss: 0.0136 | Eval R2: -2.9415 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0111 | Eval Loss: 0.0134 | Eval R2: -2.8623 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0110 | Eval Loss: 0.0134 | Eval R2: -2.8343 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0107 | Eval Loss: 0.0134 | Eval R2: -2.8390 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0106 | Eval Loss: 0.0134 | Eval R2: -2.8392 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008181, test R2 score: -2.080456
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010556205175817013, 'r2_eval_final': -2.8392465114593506, 'loss_eval_final': 0.013399410992860794, 'r2_test': -2.0804556782008263, 'loss_test': 0.008181365206837654, 'loss_nodes': [[0.005279895383864641, 0.006477587856352329, 0.005530509632080793, 0.00612683268263936, 0.007255478296428919, 0.004017912317067385, 0.0063543422147631645, 0.007113167084753513, 0.006777789909392595, 0.0062931859865784645, 0.005634566303342581, 0.007324103266000748, 0.012140071950852871, 0.012498297728598118, 0.010620460845530033, 0.008985335938632488, 0.010516461916267872, 0.011718152090907097, 0.010445611551404, 0.012517549097537994]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.002 MB of 0.003 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01056
wandb: loss_eval 0.0134
wandb: loss_test 0.00818
wandb:   r2_eval -2.83925
wandb:   r2_test -2.08046
wandb: 
wandb: ðŸš€ View run volcanic-bee-244 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/v5h3sob1
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_024617-v5h3sob1/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 21/50 [139:08:53<132:16:21, 16420.04s/it]Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_032844-b9mg4h0q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run neat-disco-245
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/b9mg4h0q

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.5936 | Eval Loss: 0.1716 | Eval R2: -69.6190 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.1063 | Eval Loss: 0.0502 | Eval R2: -14.2342 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0561 | Eval Loss: 0.0351 | Eval R2: -9.4000 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0432 | Eval Loss: 0.0276 | Eval R2: -7.5251 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0351 | Eval Loss: 0.0226 | Eval R2: -5.9189 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0293 | Eval Loss: 0.0205 | Eval R2: -5.3658 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0254 | Eval Loss: 0.0195 | Eval R2: -5.0237 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0222 | Eval Loss: 0.0187 | Eval R2: -4.7900 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0199 | Eval Loss: 0.0175 | Eval R2: -4.6137 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0180 | Eval Loss: 0.0165 | Eval R2: -4.3503 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0166 | Eval Loss: 0.0156 | Eval R2: -4.0814 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0156 | Eval Loss: 0.0151 | Eval R2: -3.9252 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0148 | Eval Loss: 0.0146 | Eval R2: -3.6920 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0142 | Eval Loss: 0.0141 | Eval R2: -3.4767 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0138 | Eval Loss: 0.0138 | Eval R2: -3.2991 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0133 | Eval Loss: 0.0135 | Eval R2: -3.1414 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0129 | Eval Loss: 0.0133 | Eval R2: -3.0091 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0126 | Eval Loss: 0.0130 | Eval R2: -2.8476 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0123 | Eval Loss: 0.0129 | Eval R2: -2.7659 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0119 | Eval Loss: 0.0127 | Eval R2: -2.6926 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0117 | Eval Loss: 0.0127 | Eval R2: -2.6484 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0114 | Eval Loss: 0.0126 | Eval R2: -2.5915 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0112 | Eval Loss: 0.0124 | Eval R2: -2.5015 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0109 | Eval Loss: 0.0124 | Eval R2: -2.4757 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0107 | Eval Loss: 0.0123 | Eval R2: -2.4392 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0104 | Eval Loss: 0.0122 | Eval R2: -2.3309 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0102 | Eval Loss: 0.0122 | Eval R2: -2.3809 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007463, test R2 score: -1.741614
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.010233997367322445, 'r2_eval_final': -2.380887508392334, 'loss_eval_final': 0.01222878135740757, 'r2_test': -1.7416137287914841, 'loss_test': 0.007462970446795225, 'loss_nodes': [[0.003158936044201255, 0.005537206307053566, 0.004085393622517586, 0.005417147651314735, 0.006170003209263086, 0.003417974803596735, 0.005866777617484331, 0.0062617151997983456, 0.006129303947091103, 0.007021792232990265, 0.005318245850503445, 0.007183005567640066, 0.010563168674707413, 0.011526748538017273, 0.00929169449955225, 0.00803842768073082, 0.010841287672519684, 0.01172688975930214, 0.010580655187368393, 0.011123052798211575]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.002 MB of 0.002 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01023
wandb: loss_eval 0.01223
wandb: loss_test 0.00746
wandb:   r2_eval -2.38089
wandb:   r2_test -1.74161
wandb: 
wandb: ðŸš€ View run neat-disco-245 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/b9mg4h0q
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_032844-b9mg4h0q/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [140:03:45<97:03:58, 12479.93s/it] Entrenando modelo con gcn_depth=2, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_042335-2njb0aqj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dandy-pyramid-246
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/2njb0aqj

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7556 | Eval Loss: 0.5308 | Eval R2: -253.9391 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4295 | Eval Loss: 0.3254 | Eval R2: -152.0872 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2528 | Eval Loss: 0.1743 | Eval R2: -77.3441 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1161 | Eval Loss: 0.0609 | Eval R2: -22.4870 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0369 | Eval Loss: 0.0255 | Eval R2: -6.6145 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0241 | Eval Loss: 0.0234 | Eval R2: -5.8051 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0222 | Eval Loss: 0.0223 | Eval R2: -5.3851 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0209 | Eval Loss: 0.0213 | Eval R2: -5.1436 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0197 | Eval Loss: 0.0202 | Eval R2: -4.8323 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0186 | Eval Loss: 0.0193 | Eval R2: -4.5633 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0176 | Eval Loss: 0.0185 | Eval R2: -4.3394 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0166 | Eval Loss: 0.0178 | Eval R2: -4.1652 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0158 | Eval Loss: 0.0172 | Eval R2: -3.9915 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0151 | Eval Loss: 0.0166 | Eval R2: -3.8279 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0145 | Eval Loss: 0.0162 | Eval R2: -3.6741 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0140 | Eval Loss: 0.0158 | Eval R2: -3.5468 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0136 | Eval Loss: 0.0154 | Eval R2: -3.3937 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0132 | Eval Loss: 0.0151 | Eval R2: -3.2837 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0129 | Eval Loss: 0.0148 | Eval R2: -3.1856 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0126 | Eval Loss: 0.0145 | Eval R2: -3.0976 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0123 | Eval Loss: 0.0143 | Eval R2: -3.0258 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0121 | Eval Loss: 0.0142 | Eval R2: -2.9503 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0119 | Eval Loss: 0.0140 | Eval R2: -2.9056 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0117 | Eval Loss: 0.0139 | Eval R2: -2.8555 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0115 | Eval Loss: 0.0138 | Eval R2: -2.8288 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0113 | Eval Loss: 0.0138 | Eval R2: -2.8009 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0112 | Eval Loss: 0.0138 | Eval R2: -2.8158 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.008982, test R2 score: -2.166096
Resultados intermedios:  {'gcn_depth': 2, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.011213120073080063, 'r2_eval_final': -2.8157618045806885, 'loss_eval_final': 0.013755099847912788, 'r2_test': -2.1660955917145945, 'loss_test': 0.008982171304523945, 'loss_nodes': [[0.004790971055626869, 0.011955931782722473, 0.004197715315967798, 0.010639150626957417, 0.0067386026494205, 0.0033293701708316803, 0.0068959081545472145, 0.006321327295154333, 0.006698594894260168, 0.006555489730089903, 0.007239785976707935, 0.010292614810168743, 0.011863131076097488, 0.012352446094155312, 0.011036327108740807, 0.008390402421355247, 0.012088966555893421, 0.012845165096223354, 0.011763731017708778, 0.013647832907736301]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.009 MB of 0.009 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01121
wandb: loss_eval 0.01376
wandb: loss_test 0.00898
wandb:   r2_eval -2.81576
wandb:   r2_test -2.1661
wandb: 
wandb: ðŸš€ View run dandy-pyramid-246 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/2njb0aqj
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_042335-2njb0aqj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 23/50 [140:53:27<72:13:27, 9629.92s/it] Entrenando modelo con gcn_depth=3, conv_channels=8, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_051318-sdklp11h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glad-dust-247
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/sdklp11h

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9686 | Eval Loss: 0.6546 | Eval R2: -308.8636 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.4792 | Eval Loss: 0.3148 | Eval R2: -141.0679 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.2328 | Eval Loss: 0.1508 | Eval R2: -61.3513 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.1152 | Eval Loss: 0.0786 | Eval R2: -28.0823 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0655 | Eval Loss: 0.0486 | Eval R2: -15.0091 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0444 | Eval Loss: 0.0361 | Eval R2: -10.0451 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0352 | Eval Loss: 0.0307 | Eval R2: -8.0688 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0307 | Eval Loss: 0.0276 | Eval R2: -7.0796 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0271 | Eval Loss: 0.0247 | Eval R2: -6.2341 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0239 | Eval Loss: 0.0221 | Eval R2: -5.5239 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0213 | Eval Loss: 0.0202 | Eval R2: -4.9404 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0194 | Eval Loss: 0.0187 | Eval R2: -4.5215 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0178 | Eval Loss: 0.0175 | Eval R2: -4.1909 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0167 | Eval Loss: 0.0167 | Eval R2: -3.9750 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0158 | Eval Loss: 0.0161 | Eval R2: -3.7956 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0150 | Eval Loss: 0.0156 | Eval R2: -3.6159 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0143 | Eval Loss: 0.0152 | Eval R2: -3.4845 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0139 | Eval Loss: 0.0149 | Eval R2: -3.3739 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0133 | Eval Loss: 0.0146 | Eval R2: -3.2544 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0128 | Eval Loss: 0.0142 | Eval R2: -3.0581 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0124 | Eval Loss: 0.0138 | Eval R2: -2.9245 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0120 | Eval Loss: 0.0135 | Eval R2: -2.7891 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0116 | Eval Loss: 0.0133 | Eval R2: -2.6804 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0113 | Eval Loss: 0.0133 | Eval R2: -2.6742 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0111 | Eval Loss: 0.0131 | Eval R2: -2.6001 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0108 | Eval Loss: 0.0130 | Eval R2: -2.5137 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0106 | Eval Loss: 0.0129 | Eval R2: -2.4851 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007934, test R2 score: -1.849686
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 8, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 8, 'loss_final': 0.010555542074143887, 'r2_eval_final': -2.4850716590881348, 'loss_eval_final': 0.012901444919407368, 'r2_test': -1.8496856520555305, 'loss_test': 0.007933610118925571, 'loss_nodes': [[0.003226259257644415, 0.006074479315429926, 0.0040047671645879745, 0.006435906048864126, 0.006380070932209492, 0.0038863790687173605, 0.006608142517507076, 0.0064100888557732105, 0.006702203769236803, 0.005903687793761492, 0.005929005332291126, 0.008642415516078472, 0.011280614882707596, 0.011494663543999195, 0.009522635489702225, 0.008072266355156898, 0.012537067756056786, 0.01243045087903738, 0.010958612896502018, 0.012172464281320572]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.003 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb: / 0.009 MB of 0.009 MB uploadedwandb: - 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01056
wandb: loss_eval 0.0129
wandb: loss_test 0.00793
wandb:   r2_eval -2.48507
wandb:   r2_test -1.84969
wandb: 
wandb: ðŸš€ View run glad-dust-247 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/sdklp11h
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_051318-sdklp11h/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [141:48:24<55:49:28, 7729.56s/it]Entrenando modelo con gcn_depth=1, conv_channels=16, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=4
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240724_060814-lfikj00i
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lemon-breeze-248
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/lfikj00i

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 1.3213 | Eval Loss: 0.9616 | Eval R2: -462.2696 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.8955 | Eval Loss: 0.8147 | Eval R2: -389.1203 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.7477 | Eval Loss: 0.6719 | Eval R2: -319.4011 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.6077 | Eval Loss: 0.5364 | Eval R2: -253.7837 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.4784 | Eval Loss: 0.4145 | Eval R2: -194.8072 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.3647 | Eval Loss: 0.3094 | Eval R2: -143.9940 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.2677 | Eval Loss: 0.2218 | Eval R2: -101.4869 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.1885 | Eval Loss: 0.1523 | Eval R2: -67.7180 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.1277 | Eval Loss: 0.1012 | Eval R2: -42.9242 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0847 | Eval Loss: 0.0672 | Eval R2: -26.4947 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0572 | Eval Loss: 0.0465 | Eval R2: -16.5822 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0410 | Eval Loss: 0.0349 | Eval R2: -11.0868 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0320 | Eval Loss: 0.0286 | Eval R2: -8.2397 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0272 | Eval Loss: 0.0252 | Eval R2: -6.7755 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0242 | Eval Loss: 0.0229 | Eval R2: -5.9182 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0222 | Eval Loss: 0.0214 | Eval R2: -5.4780 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0208 | Eval Loss: 0.0204 | Eval R2: -5.2047 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0199 | Eval Loss: 0.0197 | Eval R2: -4.9591 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0192 | Eval Loss: 0.0192 | Eval R2: -4.7967 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0186 | Eval Loss: 0.0187 | Eval R2: -4.6473 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0182 | Eval Loss: 0.0184 | Eval R2: -4.5177 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0178 | Eval Loss: 0.0180 | Eval R2: -4.3285 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0174 | Eval Loss: 0.0177 | Eval R2: -4.1960 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0171 | Eval Loss: 0.0174 | Eval R2: -4.0586 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0168 | Eval Loss: 0.0172 | Eval R2: -3.9197 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0164 | Eval Loss: 0.0170 | Eval R2: -3.8490 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0162 | Eval Loss: 0.0167 | Eval R2: -3.7486 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.012064, test R2 score: -3.013604
Resultados intermedios:  {'gcn_depth': 1, 'conv_channels': 16, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 4, 'loss_final': 0.01616859994828701, 'r2_eval_final': -3.7486329078674316, 'loss_eval_final': 0.01673010364174843, 'r2_test': -3.01360423402525, 'loss_test': 0.01206374354660511, 'loss_nodes': [[0.0029139923863112926, 0.00690585607662797, 0.004627708345651627, 0.011960146017372608, 0.0075036389753222466, 0.01487962156534195, 0.028996186330914497, 0.008459271863102913, 0.006715990602970123, 0.007613657973706722, 0.00618708273395896, 0.012701364234089851, 0.011720797047019005, 0.02062833495438099, 0.011918971315026283, 0.014876222237944603, 0.01667613536119461, 0.013241513632237911, 0.01730894297361374, 0.015439389273524284]]}
wandb: - 0.002 MB of 0.002 MB uploadedwandb: \ 0.002 MB of 0.002 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.01617
wandb: loss_eval 0.01673
wandb: loss_test 0.01206
wandb:   r2_eval -3.74863
wandb:   r2_test -3.0136
wandb: 
wandb: ðŸš€ View run lemon-breeze-248 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/lfikj00i
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240724_060814-lfikj00i/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [169:02:46<241:53:43, 34832.92s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=16
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240725_092236-749sum81
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run smooth-lion-249
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/749sum81

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.7649 | Eval Loss: 0.4238 | Eval R2: -194.5570 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.2605 | Eval Loss: 0.1203 | Eval R2: -44.8158 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.0857 | Eval Loss: 0.0470 | Eval R2: -11.8465 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.0480 | Eval Loss: 0.0348 | Eval R2: -8.9346 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.0361 | Eval Loss: 0.0270 | Eval R2: -7.2971 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.0286 | Eval Loss: 0.0216 | Eval R2: -5.6918 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0238 | Eval Loss: 0.0191 | Eval R2: -4.9246 | LR: 0.0010 | 
Epoch 8/50 | Train Loss: 0.0207 | Eval Loss: 0.0175 | Eval R2: -4.3599 | LR: 0.0010 | 
Epoch 9/50 | Train Loss: 0.0184 | Eval Loss: 0.0165 | Eval R2: -3.9941 | LR: 0.0010 | 
Epoch 10/50 | Train Loss: 0.0168 | Eval Loss: 0.0158 | Eval R2: -3.7329 | LR: 0.0010 | 
Epoch 11/50 | Train Loss: 0.0155 | Eval Loss: 0.0150 | Eval R2: -3.4255 | LR: 0.0010 | 
Epoch 12/50 | Train Loss: 0.0146 | Eval Loss: 0.0145 | Eval R2: -3.1997 | LR: 0.0010 | 
Epoch 13/50 | Train Loss: 0.0139 | Eval Loss: 0.0141 | Eval R2: -3.0198 | LR: 0.0010 | 
Epoch 14/50 | Train Loss: 0.0134 | Eval Loss: 0.0137 | Eval R2: -2.8655 | LR: 0.0010 | 
Epoch 15/50 | Train Loss: 0.0128 | Eval Loss: 0.0135 | Eval R2: -2.7686 | LR: 0.0010 | 
Epoch 16/50 | Train Loss: 0.0124 | Eval Loss: 0.0132 | Eval R2: -2.6558 | LR: 0.0010 | 
Epoch 17/50 | Train Loss: 0.0120 | Eval Loss: 0.0131 | Eval R2: -2.6119 | LR: 0.0010 | 
Epoch 18/50 | Train Loss: 0.0117 | Eval Loss: 0.0131 | Eval R2: -2.6070 | LR: 0.0010 | 
Epoch 19/50 | Train Loss: 0.0114 | Eval Loss: 0.0129 | Eval R2: -2.5201 | LR: 0.0010 | 
Epoch 20/50 | Train Loss: 0.0112 | Eval Loss: 0.0129 | Eval R2: -2.5036 | LR: 0.0010 | 
Epoch 21/50 | Train Loss: 0.0109 | Eval Loss: 0.0128 | Eval R2: -2.4298 | LR: 0.0010 | 
Epoch 22/50 | Train Loss: 0.0108 | Eval Loss: 0.0127 | Eval R2: -2.4138 | LR: 0.0010 | 
Epoch 23/50 | Train Loss: 0.0105 | Eval Loss: 0.0126 | Eval R2: -2.3638 | LR: 0.0010 | 
Epoch 24/50 | Train Loss: 0.0104 | Eval Loss: 0.0126 | Eval R2: -2.3343 | LR: 0.0010 | 
Epoch 25/50 | Train Loss: 0.0103 | Eval Loss: 0.0126 | Eval R2: -2.3662 | LR: 0.0010 | 
Epoch 26/50 | Train Loss: 0.0101 | Eval Loss: 0.0125 | Eval R2: -2.3065 | LR: 0.0010 | 
Epoch 27/50 | Train Loss: 0.0099 | Eval Loss: 0.0124 | Eval R2: -2.2481 | LR: 0.0010 | 

==================== TEST INFO ===================

preds:  (1, 23, 20)
test loss: 0.007395, test R2 score: -1.603510
Resultados intermedios:  {'gcn_depth': 3, 'conv_channels': 4, 'kernel_size': 3, 'dropout': 0.25, 'gcn_true': True, 'build_adj': True, 'propalpha': 0.15, 'out_channels': 16, 'loss_final': 0.009914983995258808, 'r2_eval_final': -2.248145580291748, 'loss_eval_final': 0.012391580268740654, 'r2_test': -1.6035095022572465, 'loss_test': 0.007394607178866863, 'loss_nodes': [[0.0032892676535993814, 0.005834546871483326, 0.003906643949449062, 0.005043874029070139, 0.006223712582141161, 0.003227456007152796, 0.005535629577934742, 0.006417362950742245, 0.005933465901762247, 0.006218669470399618, 0.005112646613270044, 0.006893838755786419, 0.010543531738221645, 0.011527103371918201, 0.00935370847582817, 0.00777405546978116, 0.010092121548950672, 0.011839871294796467, 0.011341477744281292, 0.01178316306322813]]}
wandb: - 0.002 MB of 0.006 MB uploadedwandb: \ 0.002 MB of 0.006 MB uploadedwandb: | 0.009 MB of 0.009 MB uploadedwandb:                                                                                
wandb: 
wandb: Run history:
wandb:      loss â–
wandb: loss_eval â–
wandb: loss_test â–
wandb:   r2_eval â–
wandb:   r2_test â–
wandb: 
wandb: Run summary:
wandb:      loss 0.00991
wandb: loss_eval 0.01239
wandb: loss_test 0.00739
wandb:   r2_eval -2.24815
wandb:   r2_test -1.60351
wandb: 
wandb: ðŸš€ View run smooth-lion-249 at: https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/749sum81
wandb: â­ï¸ View project at: https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240725_092236-749sum81/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [169:43:53<167:28:54, 25122.29s/it]Entrenando modelo con gcn_depth=3, conv_channels=4, kernel_size=3, dropout=0.25, gcn_true=True, build_adj=True, propalpha=0.15, out_channels=8
wandb: - Waiting for wandb.init()...wandb: \ Waiting for wandb.init()...wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /usr/src/app/GNNs_PowerGraph/TFM/experimentos_split/scripts/wandb/run-20240725_100344-pfxx6f6c
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lilac-leaf-250
wandb: â­ï¸ View project at https://wandb.ai/maragumar01/mtgnn_gen_trip
wandb: ðŸš€ View run at https://wandb.ai/maragumar01/mtgnn_gen_trip/runs/pfxx6f6c

==================== DATASET INFO ===================

Train dataset: 4350
Validation dataset: 930
Test dataset: 942

==================== TRAIN INFO ===================

Epoch 1/50 | Train Loss: 0.9220 | Eval Loss: 0.6942 | Eval R2: -325.6749 | LR: 0.0010 | 
Epoch 2/50 | Train Loss: 0.5815 | Eval Loss: 0.4716 | Eval R2: -212.9295 | LR: 0.0010 | 
Epoch 3/50 | Train Loss: 0.3834 | Eval Loss: 0.2998 | Eval R2: -126.8085 | LR: 0.0010 | 
Epoch 4/50 | Train Loss: 0.2396 | Eval Loss: 0.1855 | Eval R2: -69.8607 | LR: 0.0010 | 
Epoch 5/50 | Train Loss: 0.1531 | Eval Loss: 0.1250 | Eval R2: -39.9774 | LR: 0.0010 | 
Epoch 6/50 | Train Loss: 0.1108 | Eval Loss: 0.0982 | Eval R2: -26.9770 | LR: 0.0010 | 
Epoch 7/50 | Train Loss: 0.0934 | Eval Loss: 0.0882 | Eval R2: -22.2153 | LR: 0.0010 | 
