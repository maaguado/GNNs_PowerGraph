{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentación - LSTM \n",
    "\n",
    "\n",
    "\n",
    "## 1. Obtención de datos\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "sns.set_palette(\"coolwarm_r\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import os, sys\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "sys.path.insert(1, \"/\".join(path.split(\"/\")[0:-1]))\n",
    "\n",
    "from utils import powergrid\n",
    "\n",
    "\n",
    "from utils import pygt_loader\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    def tqdm(iterable):\n",
    "        return iterable\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from utils.utils_graph import format_plot, plot_training_and_eval_losses, plot_predictions\n",
    "\n",
    "from importlib import reload  # Python 3.4+\n",
    "utils.trainer = reload(utils.trainer)\n",
    "utils.models = reload(utils.models)\n",
    "from utils.models import LSTMModel\n",
    "from utils.trainer import  TrainerLSTMModel\n",
    "\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = \"/home/TFM/code_dataset2/datos/Natural Oscillation\"\n",
    "folder_path = \"/Users/maguado/Documents/UGR/Master/TFM/datos_2/Natural Oscillation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  row_307\n",
      "Node:  0  not included, including...\n",
      "Node:  1  not included, including...\n",
      "Node:  2  not included, including...\n",
      "Node:  3  not included, including...\n",
      "Node:  4  not included, including...\n",
      "Node:  5  not included, including...\n",
      "Node:  6  not included, including...\n",
      "Node:  7  not included, including...\n",
      "Node:  8  not included, including...\n",
      "Node:  9  not included, including...\n",
      "Node:  10  not included, including...\n",
      "Node:  11  not included, including...\n",
      "Node:  12  not included, including...\n",
      "Node:  13  not included, including...\n",
      "Node:  14  not included, including...\n",
      "Node:  15  not included, including...\n",
      "Node:  16  not included, including...\n",
      "Node:  17  not included, including...\n",
      "Node:  18  not included, including...\n",
      "Node:  19  not included, including...\n",
      "Node:  20  not included, including...\n",
      "Node:  21  not included, including...\n",
      "Node:  22  not included, including...\n",
      "Processing:  row_135\n",
      "Processing:  row_75\n",
      "Processing:  row_338\n",
      "Processing:  row_81\n",
      "Processing:  row_132\n",
      "Processing:  row_300\n",
      "Processing:  row_86\n",
      "Processing:  row_72\n",
      "Processing:  row_104\n",
      "Processing:  row_336\n",
      "Processing:  row_309\n",
      "Processing:  row_44\n",
      "Processing:  row_331\n",
      "Processing:  row_88\n",
      "Processing:  row_103\n",
      "Processing:  row_43\n",
      "Processing:  row_17\n",
      "Processing:  row_168\n",
      "Processing:  row_28\n",
      "Processing:  row_391\n",
      "Processing:  row_157\n",
      "Processing:  row_365\n",
      "Processing:  row_533\n",
      "Processing:  row_10\n",
      "Processing:  row_534\n",
      "Processing:  row_362\n",
      "Processing:  row_150\n",
      "Processing:  row_396\n",
      "Processing:  row_159\n",
      "Processing:  row_26\n",
      "Processing:  row_354\n",
      "Processing:  row_166\n",
      "Processing:  row_502\n",
      "Processing:  row_19\n",
      "Processing:  row_192\n",
      "Processing:  row_21\n",
      "Processing:  row_398\n",
      "Processing:  row_195\n",
      "Processing:  row_505\n",
      "Processing:  row_161\n",
      "Processing:  row_353\n",
      "Processing:  row_102\n",
      "Processing:  row_330\n",
      "Processing:  row_89\n",
      "Processing:  row_42\n",
      "Processing:  row_337\n",
      "Processing:  row_105\n",
      "Processing:  row_45\n",
      "Processing:  row_308\n",
      "Processing:  row_301\n",
      "Processing:  row_133\n",
      "Processing:  row_73\n",
      "Processing:  row_87\n",
      "Processing:  row_550\n",
      "Processing:  row_134\n",
      "Processing:  row_306\n",
      "Processing:  row_339\n",
      "Processing:  row_80\n",
      "Processing:  row_74\n",
      "Processing:  row_20\n",
      "Processing:  row_399\n",
      "Processing:  row_504\n",
      "Processing:  row_352\n",
      "Processing:  row_160\n",
      "Processing:  row_194\n",
      "Processing:  row_27\n",
      "Processing:  row_158\n",
      "Processing:  row_193\n",
      "Processing:  row_18\n",
      "Processing:  row_167\n",
      "Processing:  row_355\n",
      "Processing:  row_503\n",
      "Processing:  row_11\n",
      "Processing:  row_397\n",
      "Processing:  row_535\n",
      "Processing:  row_151\n",
      "Processing:  row_363\n",
      "Processing:  row_169\n",
      "Processing:  row_16\n",
      "Processing:  row_364\n",
      "Processing:  row_156\n",
      "Processing:  row_532\n",
      "Processing:  row_29\n",
      "Processing:  row_390\n",
      "Processing:  row_440\n",
      "Processing:  row_216\n",
      "Processing:  row_229\n",
      "Processing:  row_211\n",
      "Processing:  row_447\n",
      "Processing:  row_478\n",
      "Processing:  row_471\n",
      "Processing:  row_227\n",
      "Processing:  row_485\n",
      "Processing:  row_218\n",
      "Processing:  row_482\n",
      "Processing:  row_220\n",
      "Processing:  row_476\n",
      "Processing:  row_449\n",
      "Processing:  row_280\n",
      "Processing:  row_274\n",
      "Processing:  row_422\n",
      "Processing:  row_425\n",
      "Processing:  row_273\n",
      "Processing:  row_287\n",
      "Processing:  row_245\n",
      "Processing:  row_413\n",
      "Processing:  row_289\n",
      "Processing:  row_414\n",
      "Processing:  row_242\n",
      "Processing:  row_221\n",
      "Processing:  row_477\n",
      "Processing:  row_483\n",
      "Processing:  row_448\n",
      "Processing:  row_484\n",
      "Processing:  row_470\n",
      "Processing:  row_226\n",
      "Processing:  row_219\n",
      "Processing:  row_210\n",
      "Processing:  row_446\n",
      "Processing:  row_479\n",
      "Processing:  row_441\n",
      "Processing:  row_217\n",
      "Processing:  row_228\n",
      "Processing:  row_288\n",
      "Processing:  row_415\n",
      "Processing:  row_243\n",
      "Processing:  row_244\n",
      "Processing:  row_412\n",
      "Processing:  row_286\n",
      "Processing:  row_424\n",
      "Processing:  row_272\n",
      "Processing:  row_275\n",
      "Processing:  row_423\n",
      "Processing:  row_281\n",
      "Processing:  row_401\n",
      "Processing:  row_257\n",
      "Processing:  row_268\n",
      "Processing:  row_3\n",
      "Processing:  row_250\n",
      "Processing:  row_406\n",
      "Processing:  row_439\n",
      "Processing:  row_4\n",
      "Processing:  row_292\n",
      "Processing:  row_430\n",
      "Processing:  row_266\n",
      "Processing:  row_259\n",
      "Processing:  row_261\n",
      "Processing:  row_437\n",
      "Processing:  row_295\n",
      "Processing:  row_408\n",
      "Processing:  row_235\n",
      "Processing:  row_463\n",
      "Processing:  row_497\n",
      "Processing:  row_490\n",
      "Processing:  row_464\n",
      "Processing:  row_232\n",
      "Processing:  row_499\n",
      "Processing:  row_204\n",
      "Processing:  row_452\n",
      "Processing:  row_455\n",
      "Processing:  row_203\n",
      "Processing:  row_294\n",
      "Processing:  row_260\n",
      "Processing:  row_436\n",
      "Processing:  row_409\n",
      "Processing:  row_431\n",
      "Processing:  row_267\n",
      "Processing:  row_293\n",
      "Processing:  row_258\n",
      "Processing:  row_251\n",
      "Processing:  row_407\n",
      "Processing:  row_438\n",
      "Processing:  row_5\n",
      "Processing:  row_400\n",
      "Processing:  row_256\n",
      "Processing:  row_2\n",
      "Processing:  row_269\n",
      "Processing:  row_454\n",
      "Processing:  row_202\n",
      "Processing:  row_498\n",
      "Processing:  row_205\n",
      "Processing:  row_453\n",
      "Processing:  row_465\n",
      "Processing:  row_233\n",
      "Processing:  row_491\n",
      "Processing:  row_496\n",
      "Processing:  row_234\n",
      "Processing:  row_462\n",
      "Processing:  row_510\n",
      "Processing:  row_174\n",
      "Processing:  row_346\n",
      "Processing:  row_180\n",
      "Processing:  row_379\n",
      "Processing:  row_34\n",
      "Processing:  row_187\n",
      "Processing:  row_341\n",
      "Processing:  row_173\n",
      "Processing:  row_517\n",
      "Processing:  row_33\n",
      "Processing:  row_528\n",
      "Processing:  row_383\n",
      "Processing:  row_521\n",
      "Processing:  row_377\n",
      "Processing:  row_145\n",
      "Processing:  row_348\n",
      "Processing:  row_142\n",
      "Processing:  row_370\n",
      "Processing:  row_526\n",
      "Processing:  row_384\n",
      "Processing:  row_519\n",
      "Processing:  row_189\n",
      "Processing:  row_129\n",
      "Processing:  row_56\n",
      "Processing:  row_324\n",
      "Processing:  row_116\n",
      "Processing:  row_69\n",
      "Processing:  row_51\n",
      "Processing:  row_111\n",
      "Processing:  row_323\n",
      "Processing:  row_67\n",
      "Processing:  row_93\n",
      "Processing:  row_118\n",
      "Processing:  row_58\n",
      "Processing:  row_127\n",
      "Processing:  row_315\n",
      "Processing:  row_543\n",
      "Processing:  row_94\n",
      "Processing:  row_60\n",
      "Processing:  row_544\n",
      "Processing:  row_312\n",
      "Processing:  row_120\n",
      "Processing:  row_385\n",
      "Processing:  row_371\n",
      "Processing:  row_143\n",
      "Processing:  row_527\n",
      "Processing:  row_188\n",
      "Processing:  row_518\n",
      "Processing:  row_520\n",
      "Processing:  row_144\n",
      "Processing:  row_376\n",
      "Processing:  row_382\n",
      "Processing:  row_349\n",
      "Processing:  row_172\n",
      "Processing:  row_340\n",
      "Processing:  row_516\n",
      "Processing:  row_186\n",
      "Processing:  row_529\n",
      "Processing:  row_32\n",
      "Processing:  row_181\n",
      "Processing:  row_511\n",
      "Processing:  row_347\n",
      "Processing:  row_175\n",
      "Processing:  row_35\n",
      "Processing:  row_378\n",
      "Processing:  row_61\n",
      "Processing:  row_95\n",
      "Processing:  row_545\n",
      "Processing:  row_121\n",
      "Processing:  row_313\n",
      "Processing:  row_119\n",
      "Processing:  row_92\n",
      "Processing:  row_66\n",
      "Processing:  row_314\n",
      "Processing:  row_126\n",
      "Processing:  row_542\n",
      "Processing:  row_59\n",
      "Processing:  row_50\n",
      "Processing:  row_322\n",
      "Processing:  row_110\n",
      "Processing:  row_57\n",
      "Processing:  row_128\n",
      "Processing:  row_68\n",
      "Processing:  row_117\n",
      "Processing:  row_325\n",
      "Processing:  row_508\n",
      "Processing:  row_13\n",
      "Processing:  row_198\n",
      "Processing:  row_537\n",
      "Processing:  row_153\n",
      "Processing:  row_361\n",
      "Processing:  row_395\n",
      "Processing:  row_14\n",
      "Processing:  row_359\n",
      "Processing:  row_392\n",
      "Processing:  row_366\n",
      "Processing:  row_154\n",
      "Processing:  row_530\n",
      "Processing:  row_22\n",
      "Processing:  row_539\n",
      "Processing:  row_196\n",
      "Processing:  row_506\n",
      "Processing:  row_350\n",
      "Processing:  row_162\n",
      "Processing:  row_368\n",
      "Processing:  row_25\n",
      "Processing:  row_165\n",
      "Processing:  row_357\n",
      "Processing:  row_501\n",
      "Processing:  row_191\n",
      "Processing:  row_303\n",
      "Processing:  row_131\n",
      "Processing:  row_85\n",
      "Processing:  row_71\n",
      "Processing:  row_49\n",
      "Processing:  row_136\n",
      "Processing:  row_304\n",
      "Processing:  row_76\n",
      "Processing:  row_82\n",
      "Processing:  row_109\n",
      "Processing:  row_100\n",
      "Processing:  row_332\n",
      "Processing:  row_40\n",
      "Processing:  row_335\n",
      "Processing:  row_107\n",
      "Processing:  row_78\n",
      "Processing:  row_138\n",
      "Processing:  row_47\n",
      "Processing:  row_24\n",
      "Processing:  row_369\n",
      "Processing:  row_190\n",
      "Processing:  row_356\n",
      "Processing:  row_164\n",
      "Processing:  row_500\n",
      "Processing:  row_538\n",
      "Processing:  row_23\n",
      "Processing:  row_507\n",
      "Processing:  row_163\n",
      "Processing:  row_351\n",
      "Processing:  row_197\n",
      "Processing:  row_358\n",
      "Processing:  row_15\n",
      "Processing:  row_155\n",
      "Processing:  row_367\n",
      "Processing:  row_531\n",
      "Processing:  row_393\n",
      "Processing:  row_199\n",
      "Processing:  row_12\n",
      "Processing:  row_509\n",
      "Processing:  row_394\n",
      "Processing:  row_536\n",
      "Processing:  row_360\n",
      "Processing:  row_152\n",
      "Processing:  row_79\n",
      "Processing:  row_106\n",
      "Processing:  row_334\n",
      "Processing:  row_46\n",
      "Processing:  row_139\n",
      "Processing:  row_333\n",
      "Processing:  row_101\n",
      "Processing:  row_41\n",
      "Processing:  row_305\n",
      "Processing:  row_137\n",
      "Processing:  row_48\n",
      "Processing:  row_108\n",
      "Processing:  row_83\n",
      "Processing:  row_77\n",
      "Processing:  row_130\n",
      "Processing:  row_302\n",
      "Processing:  row_70\n",
      "Processing:  row_84\n",
      "Processing:  row_419\n",
      "Processing:  row_426\n",
      "Processing:  row_270\n",
      "Processing:  row_284\n",
      "Processing:  row_248\n",
      "Processing:  row_283\n",
      "Processing:  row_277\n",
      "Processing:  row_421\n",
      "Processing:  row_428\n",
      "Processing:  row_417\n",
      "Processing:  row_241\n",
      "Processing:  row_279\n",
      "Processing:  row_246\n",
      "Processing:  row_410\n",
      "Processing:  row_212\n",
      "Processing:  row_444\n",
      "Processing:  row_443\n",
      "Processing:  row_215\n",
      "Processing:  row_488\n",
      "Processing:  row_481\n",
      "Processing:  row_223\n",
      "Processing:  row_475\n",
      "Processing:  row_472\n",
      "Processing:  row_224\n",
      "Processing:  row_486\n",
      "Processing:  row_278\n",
      "Processing:  row_247\n",
      "Processing:  row_411\n",
      "Processing:  row_429\n",
      "Processing:  row_416\n",
      "Processing:  row_240\n",
      "Processing:  row_249\n",
      "Processing:  row_276\n",
      "Processing:  row_420\n",
      "Processing:  row_282\n",
      "Processing:  row_418\n",
      "Processing:  row_285\n",
      "Processing:  row_427\n",
      "Processing:  row_271\n",
      "Processing:  row_487\n",
      "Processing:  row_473\n",
      "Processing:  row_225\n",
      "Processing:  row_222\n",
      "Processing:  row_474\n",
      "Processing:  row_480\n",
      "Processing:  row_442\n",
      "Processing:  row_214\n",
      "Processing:  row_489\n",
      "Processing:  row_213\n",
      "Processing:  row_445\n",
      "Processing:  row_458\n",
      "Processing:  row_493\n",
      "Processing:  row_467\n",
      "Processing:  row_231\n",
      "Processing:  row_209\n",
      "Processing:  row_236\n",
      "Processing:  row_460\n",
      "Processing:  row_494\n",
      "Processing:  row_469\n",
      "Processing:  row_456\n",
      "Processing:  row_200\n",
      "Processing:  row_238\n",
      "Processing:  row_207\n",
      "Processing:  row_451\n",
      "Processing:  row_253\n",
      "Processing:  row_405\n",
      "Processing:  row_298\n",
      "Processing:  row_7\n",
      "Processing:  row_402\n",
      "Processing:  row_254\n",
      "Processing:  row_262\n",
      "Processing:  row_9\n",
      "Processing:  row_434\n",
      "Processing:  row_296\n",
      "Processing:  row_291\n",
      "Processing:  row_433\n",
      "Processing:  row_265\n",
      "Processing:  row_239\n",
      "Processing:  row_206\n",
      "Processing:  row_450\n",
      "Processing:  row_468\n",
      "Processing:  row_457\n",
      "Processing:  row_201\n",
      "Processing:  row_208\n",
      "Processing:  row_495\n",
      "Processing:  row_237\n",
      "Processing:  row_461\n",
      "Processing:  row_459\n",
      "Processing:  row_466\n",
      "Processing:  row_230\n",
      "Processing:  row_492\n",
      "Processing:  row_432\n",
      "Processing:  row_264\n",
      "Processing:  row_290\n",
      "Processing:  row_297\n",
      "Processing:  row_8\n",
      "Processing:  row_263\n",
      "Processing:  row_435\n",
      "Processing:  row_403\n",
      "Processing:  row_255\n",
      "Processing:  row_1\n",
      "Processing:  row_252\n",
      "Processing:  row_404\n",
      "Processing:  row_6\n",
      "Processing:  row_299\n",
      "Processing:  row_52\n",
      "Processing:  row_549\n",
      "Processing:  row_99\n",
      "Processing:  row_320\n",
      "Processing:  row_112\n",
      "Processing:  row_318\n",
      "Processing:  row_55\n",
      "Processing:  row_115\n",
      "Processing:  row_327\n",
      "Processing:  row_97\n",
      "Processing:  row_63\n",
      "Processing:  row_547\n",
      "Processing:  row_123\n",
      "Processing:  row_311\n",
      "Processing:  row_64\n",
      "Processing:  row_90\n",
      "Processing:  row_329\n",
      "Processing:  row_316\n",
      "Processing:  row_124\n",
      "Processing:  row_540\n",
      "Processing:  row_184\n",
      "Processing:  row_170\n",
      "Processing:  row_342\n",
      "Processing:  row_514\n",
      "Processing:  row_389\n",
      "Processing:  row_30\n",
      "Processing:  row_513\n",
      "Processing:  row_345\n",
      "Processing:  row_177\n",
      "Processing:  row_183\n",
      "Processing:  row_148\n",
      "Processing:  row_37\n",
      "Processing:  row_373\n",
      "Processing:  row_141\n",
      "Processing:  row_525\n",
      "Processing:  row_387\n",
      "Processing:  row_380\n",
      "Processing:  row_39\n",
      "Processing:  row_522\n",
      "Processing:  row_146\n",
      "Processing:  row_374\n",
      "Processing:  row_179\n",
      "Processing:  row_91\n",
      "Processing:  row_328\n",
      "Skipping  row_328\n",
      "Processing:  row_65\n",
      "Processing:  row_125\n",
      "Processing:  row_317\n",
      "Processing:  row_541\n",
      "Processing:  row_62\n",
      "Processing:  row_96\n",
      "Processing:  row_546\n",
      "Processing:  row_310\n",
      "Processing:  row_122\n",
      "Processing:  row_54\n",
      "Processing:  row_319\n",
      "Processing:  row_326\n",
      "Processing:  row_114\n",
      "Processing:  row_548\n",
      "Processing:  row_53\n",
      "Processing:  row_113\n",
      "Processing:  row_98\n",
      "Processing:  row_321\n",
      "Processing:  row_523\n",
      "Processing:  row_375\n",
      "Processing:  row_147\n",
      "Processing:  row_381\n",
      "Processing:  row_38\n",
      "Processing:  row_178\n",
      "Processing:  row_386\n",
      "Processing:  row_140\n",
      "Processing:  row_372\n",
      "Processing:  row_524\n",
      "Processing:  row_182\n",
      "Processing:  row_512\n",
      "Processing:  row_176\n",
      "Processing:  row_344\n",
      "Processing:  row_36\n",
      "Processing:  row_149\n",
      "Processing:  row_343\n",
      "Processing:  row_171\n",
      "Processing:  row_515\n",
      "Processing:  row_185\n",
      "Processing:  row_388\n",
      "Processing:  row_31\n",
      "Number of situations:  549\n",
      "Number of timestamps:  800\n",
      "Number of situations of the selected type:  549\n"
     ]
    }
   ],
   "source": [
    "loader = powergrid.PowerGridDatasetLoader(folder_path, problem=\"classification_type\")\n",
    "_,_,_ =loader.process()\n",
    "limit = 300\n",
    "dataset_full, situations_each = loader.get_dataset()\n",
    "target_names = loader.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones auxiliares - entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba - predicción por tipos - usando batch \n",
    "\n",
    "Algunos modelos de los que veremos luego admiten entrenamiento por batch, y otros no, por lo que guardaremos los resultados de LSTM en ambos casos, para poder compararlos.\n",
    "\n",
    "En GNNs el entrenamiento por batch muchas veces carece de sentido, porque estaríamos modelando todos los grafos del mismo batch como el mismo grafo, perdiendo información que puede ser valiosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucle rápido para guardar resultados con batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== DATASET INFO ===================\n",
      "\n",
      "Train dataset: 390\n",
      "Validation dataset: 75\n",
      "Test dataset: 84\n",
      "\n",
      "==================== DATALOADER INFO ===================\n",
      "\n",
      "DataLoader for train set:\n",
      "Number of batches: 195\n",
      "DataLoader for val set:\n",
      "Number of batches: 37\n",
      "DataLoader for test set:\n",
      "Number of batches: 42\n",
      "\n",
      "==================== TRAIN INFO ===================\n",
      "\n",
      "Epoch 1/100 | Train Loss: 1.5145 | Eval Loss: 1.3496 | Accuracy: 0.5676 | Precision: 0.2297 | Recall: 0.3857 | F1-Score: 0.2877 | LR: 0.0010 | \n",
      "Epoch 2/100 | Train Loss: 1.4278 | Eval Loss: 1.3014 | Accuracy: 0.6351 | Precision: 0.4428 | Recall: 0.5000 | F1-Score: 0.4354 | LR: 0.0010 | \n",
      "Epoch 3/100 | Train Loss: 1.3357 | Eval Loss: 1.2248 | Accuracy: 0.6892 | Precision: 0.4566 | Recall: 0.6000 | F1-Score: 0.5125 | LR: 0.0010 | \n",
      "Epoch 4/100 | Train Loss: 1.2983 | Eval Loss: 1.2369 | Accuracy: 0.6757 | Precision: 0.4500 | Recall: 0.5857 | F1-Score: 0.5033 | LR: 0.0010 | \n",
      "Epoch 5/100 | Train Loss: 1.2794 | Eval Loss: 1.2198 | Accuracy: 0.6892 | Precision: 0.4376 | Recall: 0.6000 | F1-Score: 0.5030 | LR: 0.0010 | \n",
      "Epoch 6/100 | Train Loss: 1.2748 | Eval Loss: 1.2158 | Accuracy: 0.6892 | Precision: 0.4376 | Recall: 0.6000 | F1-Score: 0.5030 | LR: 0.0010 | \n",
      "Epoch 7/100 | Train Loss: 1.2910 | Eval Loss: 1.2279 | Accuracy: 0.6757 | Precision: 0.4163 | Recall: 0.5857 | F1-Score: 0.4856 | LR: 0.0010 | \n",
      "Epoch 8/100 | Train Loss: 1.2701 | Eval Loss: 1.2206 | Accuracy: 0.6892 | Precision: 0.4566 | Recall: 0.6000 | F1-Score: 0.5125 | LR: 0.0010 | \n",
      "Epoch 9/100 | Train Loss: 1.2821 | Eval Loss: 1.2089 | Accuracy: 0.6892 | Precision: 0.4535 | Recall: 0.6000 | F1-Score: 0.5093 | LR: 0.0010 | \n",
      "Epoch 10/100 | Train Loss: 1.2745 | Eval Loss: 1.2110 | Accuracy: 0.6892 | Precision: 0.4359 | Recall: 0.6000 | F1-Score: 0.5013 | LR: 0.0010 | \n",
      "Epoch 11/100 | Train Loss: 1.2663 | Eval Loss: 1.2088 | Accuracy: 0.6892 | Precision: 0.4535 | Recall: 0.6000 | F1-Score: 0.5093 | LR: 0.0010 | \n",
      "Epoch 12/100 | Train Loss: 1.2942 | Eval Loss: 1.2197 | Accuracy: 0.6892 | Precision: 0.4376 | Recall: 0.6000 | F1-Score: 0.5030 | LR: 0.0010 | \n",
      "Epoch 13/100 | Train Loss: 1.2693 | Eval Loss: 1.2159 | Accuracy: 0.6892 | Precision: 0.4376 | Recall: 0.6000 | F1-Score: 0.5030 | LR: 0.0010 | \n",
      "Epoch 14/100 | Train Loss: 1.2947 | Eval Loss: 1.3005 | Accuracy: 0.5946 | Precision: 0.4137 | Recall: 0.5000 | F1-Score: 0.4450 | LR: 0.0010 | \n",
      "Epoch 15/100 | Train Loss: 1.2805 | Eval Loss: 1.2208 | Accuracy: 0.6892 | Precision: 0.4376 | Recall: 0.6000 | F1-Score: 0.5030 | LR: 0.0010 | \n",
      "Epoch 16/100 | Train Loss: 1.2759 | Eval Loss: 1.2181 | Accuracy: 0.6892 | Precision: 0.4376 | Recall: 0.6000 | F1-Score: 0.5030 | LR: 0.0010 | \n",
      "Epoch 17/100 | Train Loss: 1.2804 | Eval Loss: 1.2570 | Accuracy: 0.6351 | Precision: 0.3942 | Recall: 0.5429 | F1-Score: 0.4557 | LR: 0.0010 | \n",
      "Epoch 18/100 | Train Loss: 1.3640 | Eval Loss: 1.6569 | Accuracy: 0.1892 | Precision: 0.0378 | Recall: 0.2000 | F1-Score: 0.0636 | LR: 0.0010 | \n",
      "Epoch 19/100 | Train Loss: 1.6719 | Eval Loss: 1.5641 | Accuracy: 0.2838 | Precision: 0.1888 | Recall: 0.3761 | F1-Score: 0.2122 | LR: 0.0010 | \n",
      "Epoch 20/100 | Train Loss: 1.3211 | Eval Loss: 1.2512 | Accuracy: 0.6486 | Precision: 0.4361 | Recall: 0.5571 | F1-Score: 0.4841 | LR: 0.0010 | \n",
      "Epoch 21/100 | Train Loss: 1.5454 | Eval Loss: 1.5239 | Accuracy: 0.3243 | Precision: 0.2561 | Recall: 0.4308 | F1-Score: 0.3196 | LR: 0.0010 | \n",
      "Epoch 22/100 | Train Loss: 1.3072 | Eval Loss: 1.2529 | Accuracy: 0.6486 | Precision: 0.4047 | Recall: 0.5571 | F1-Score: 0.4679 | LR: 0.0010 | \n",
      "Epoch 23/100 | Train Loss: 1.3417 | Eval Loss: 1.3273 | Accuracy: 0.5811 | Precision: 0.4319 | Recall: 0.4107 | F1-Score: 0.3339 | LR: 0.0010 | \n",
      "Epoch 24/100 | Train Loss: 1.3176 | Eval Loss: 1.2852 | Accuracy: 0.6216 | Precision: 0.4319 | Recall: 0.5293 | F1-Score: 0.4532 | LR: 0.0010 | \n",
      "Epoch 25/100 | Train Loss: 1.2808 | Eval Loss: 1.2223 | Accuracy: 0.6892 | Precision: 0.4566 | Recall: 0.6000 | F1-Score: 0.5125 | LR: 0.0010 | \n",
      "Epoch 26/100 | Train Loss: 1.2812 | Eval Loss: 1.2159 | Accuracy: 0.6892 | Precision: 0.4566 | Recall: 0.6000 | F1-Score: 0.5125 | LR: 0.0010 | \n",
      "Epoch 27/100 | Train Loss: 1.2771 | Eval Loss: 1.2432 | Accuracy: 0.6622 | Precision: 0.4239 | Recall: 0.5714 | F1-Score: 0.4843 | LR: 0.0010 | \n",
      "Epoch 28/100 | Train Loss: 1.2795 | Eval Loss: 1.2369 | Accuracy: 0.6622 | Precision: 0.3978 | Recall: 0.5714 | F1-Score: 0.4689 | LR: 0.0010 | \n",
      "Epoch 29/100 | Train Loss: 1.2749 | Eval Loss: 1.2294 | Accuracy: 0.6757 | Precision: 0.4500 | Recall: 0.5857 | F1-Score: 0.5033 | LR: 0.0010 | \n",
      "Epoch 30/100 | Train Loss: 1.2614 | Eval Loss: 1.2097 | Accuracy: 0.6892 | Precision: 0.4535 | Recall: 0.6000 | F1-Score: 0.5093 | LR: 0.0010 | \n",
      "Epoch 31/100 | Train Loss: 1.2586 | Eval Loss: 1.2280 | Accuracy: 0.6757 | Precision: 0.5527 | Recall: 0.5654 | F1-Score: 0.5062 | LR: 0.0010 | \n",
      "Epoch 32/100 | Train Loss: 1.2551 | Eval Loss: 1.2676 | Accuracy: 0.6216 | Precision: 0.4421 | Recall: 0.5112 | F1-Score: 0.4484 | LR: 0.0010 | \n",
      "Epoch 33/100 | Train Loss: 1.2705 | Eval Loss: 1.2181 | Accuracy: 0.6757 | Precision: 0.4511 | Recall: 0.5857 | F1-Score: 0.5039 | LR: 0.0010 | \n",
      "Epoch 34/100 | Train Loss: 1.4319 | Eval Loss: 1.3116 | Accuracy: 0.5811 | Precision: 0.2326 | Recall: 0.4000 | F1-Score: 0.2933 | LR: 0.0010 | \n",
      "Epoch 35/100 | Train Loss: 1.3897 | Eval Loss: 1.3079 | Accuracy: 0.5811 | Precision: 0.2535 | Recall: 0.4000 | F1-Score: 0.3093 | LR: 0.0010 | \n",
      "Epoch 36/100 | Train Loss: 1.2954 | Eval Loss: 1.2145 | Accuracy: 0.6757 | Precision: 0.4498 | Recall: 0.5857 | F1-Score: 0.5026 | LR: 0.0010 | \n",
      "Epoch 37/100 | Train Loss: 1.2876 | Eval Loss: 1.2185 | Accuracy: 0.6757 | Precision: 0.4341 | Recall: 0.5857 | F1-Score: 0.4961 | LR: 0.0010 | \n",
      "Epoch 38/100 | Train Loss: 1.2635 | Eval Loss: 1.2076 | Accuracy: 0.6892 | Precision: 0.5545 | Recall: 0.6011 | F1-Score: 0.5329 | LR: 0.0010 | \n",
      "Epoch 39/100 | Train Loss: 1.2607 | Eval Loss: 1.2066 | Accuracy: 0.6892 | Precision: 0.4535 | Recall: 0.6000 | F1-Score: 0.5093 | LR: 0.0010 | \n",
      "Epoch 40/100 | Train Loss: 1.2677 | Eval Loss: 1.2079 | Accuracy: 0.7027 | Precision: 0.6598 | Recall: 0.6154 | F1-Score: 0.5433 | LR: 0.0010 | \n",
      "Epoch 41/100 | Train Loss: 1.2592 | Eval Loss: 1.2032 | Accuracy: 0.7027 | Precision: 0.5751 | Recall: 0.6187 | F1-Score: 0.5814 | LR: 0.0010 | \n",
      "Epoch 42/100 | Train Loss: 1.2493 | Eval Loss: 1.2061 | Accuracy: 0.7162 | Precision: 0.5959 | Recall: 0.6341 | F1-Score: 0.6029 | LR: 0.0010 | \n",
      "Epoch 43/100 | Train Loss: 1.2573 | Eval Loss: 1.2306 | Accuracy: 0.6892 | Precision: 0.5713 | Recall: 0.5734 | F1-Score: 0.5544 | LR: 0.0010 | \n",
      "Epoch 44/100 | Train Loss: 1.2465 | Eval Loss: 1.1871 | Accuracy: 0.7297 | Precision: 0.6177 | Recall: 0.6484 | F1-Score: 0.6157 | LR: 0.0010 | \n",
      "Epoch 45/100 | Train Loss: 1.2312 | Eval Loss: 1.1981 | Accuracy: 0.7297 | Precision: 0.6123 | Recall: 0.6495 | F1-Score: 0.6215 | LR: 0.0010 | \n",
      "Epoch 46/100 | Train Loss: 1.2293 | Eval Loss: 1.1862 | Accuracy: 0.7162 | Precision: 0.5900 | Recall: 0.6352 | F1-Score: 0.6058 | LR: 0.0010 | \n",
      "Epoch 47/100 | Train Loss: 1.2420 | Eval Loss: 1.1869 | Accuracy: 0.7162 | Precision: 0.5983 | Recall: 0.6330 | F1-Score: 0.5935 | LR: 0.0010 | \n",
      "Epoch 48/100 | Train Loss: 1.2871 | Eval Loss: 1.2792 | Accuracy: 0.5946 | Precision: 0.4176 | Recall: 0.5066 | F1-Score: 0.4331 | LR: 0.0010 | \n",
      "Epoch 49/100 | Train Loss: 1.2556 | Eval Loss: 1.1917 | Accuracy: 0.7162 | Precision: 0.5948 | Recall: 0.6352 | F1-Score: 0.6080 | LR: 0.0010 | \n",
      "Epoch 50/100 | Train Loss: 1.2321 | Eval Loss: 1.1824 | Accuracy: 0.7027 | Precision: 0.5751 | Recall: 0.6187 | F1-Score: 0.5814 | LR: 0.0010 | \n",
      "Epoch 51/100 | Train Loss: 1.2408 | Eval Loss: 1.2063 | Accuracy: 0.7027 | Precision: 0.5930 | Recall: 0.6165 | F1-Score: 0.5602 | LR: 0.0010 | \n",
      "Epoch 52/100 | Train Loss: 1.2497 | Eval Loss: 1.2156 | Accuracy: 0.6892 | Precision: 0.5557 | Recall: 0.6022 | F1-Score: 0.5499 | LR: 0.0010 | \n",
      "\n",
      "==================== TEST INFO ===================\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "branch_fault       0.67      0.83      0.74        12\n",
      " branch_trip       0.00      0.00      0.00        15\n",
      "   bus_fault       0.50      0.11      0.18         9\n",
      "    bus_trip       1.00      1.00      1.00        19\n",
      "    gen_trip       0.60      1.00      0.75        29\n",
      "\n",
      "    accuracy                           0.70        84\n",
      "   macro avg       0.55      0.59      0.54        84\n",
      "weighted avg       0.58      0.70      0.61        84\n",
      "\n",
      "preds:  ()\n",
      "test loss: 1.204880, test accuracy: 0.7024, test precision: 0.5542, test recall: 0.5889, test F1-score: 0.5352\n"
     ]
    }
   ],
   "source": [
    "dataloader_params2 = {\n",
    "    \"batch_size\": 2,\n",
    "    \"data_split_ratio\": [0.7, 0.15, 0.15],\n",
    "    \"seed\": 42,\n",
    "    \"keep_same\": True,\n",
    "    \"use_batch\": True\n",
    "}\n",
    "params_model = {\n",
    "    \"hidden_size\": 50,\n",
    "    \"n_layers\": 2\n",
    "}\n",
    "problem =\"Classification\"\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "n_div_prob = loader.div\n",
    "n_nodes =dataset_full.features[0].shape[0]\n",
    "n_target = dataset_full.targets[0].shape[0]\n",
    "n_features = dataset_full[0].x.shape[1]\n",
    "\n",
    "n_layers = params_model['n_layers']\n",
    "hidden_size = params_model['hidden_size']\n",
    "\n",
    "\n",
    "model_prob = LSTMModel(name=\"LSTM\", node_features=n_features, node_count=n_nodes, n_target=n_target, hidden_size=hidden_size, num_layers=n_layers, is_classification=True)\n",
    "\n",
    "trainer_lstm = TrainerLSTMModel(model_prob, dataset_full,device, f\"./results/{problem}\", dataloader_params2, batch=True, is_classification=True)\n",
    "losses, eval_losses, accs, precisions, recalls = trainer_lstm.train(num_epochs=100, steps=100, num_early_stop=15)\n",
    "test_acc, test_precision, test_recall, test_f1, test_loss, preds, real = trainer_lstm.test(target_names=target_names)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
